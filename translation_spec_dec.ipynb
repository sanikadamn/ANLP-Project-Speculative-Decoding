{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "if not os.path.exists(\"/scratch/sarthak\"):\n",
    "    os.makedirs(\"/scratch/sarthak\")\n",
    "\n",
    "os.environ[\"HF_HOME\"] = \"/scratch/sarthak/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "EdqllqRvz8YX"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import T5ForConditionalGeneration, T5Tokenizer, AutoModelForSeq2SeqLM\n",
    "from typing import List, Tuple, Optional\n",
    "import time\n",
    "import numpy as np\n",
    "# %pip install tqdm\n",
    "# %pip install ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "import torch\n",
    "import datasets\n",
    "from transformers import T5Tokenizer\n",
    "\n",
    "# # WMT16 EN-DE dataset\n",
    "# def preprocess_function_ende(examples, tokenizer, args, train_args, prefix=\"translate English to German: \"):\n",
    "#     if train_args.debug:\n",
    "#         all_text = examples['translation'][:2]\n",
    "#     else:\n",
    "#         all_text = examples['translation']\n",
    "        \n",
    "#     inputs = []\n",
    "#     targets = []\n",
    "#     for excerpt in all_text:\n",
    "#         en_text = prefix + excerpt['en']\n",
    "#         de_text = excerpt['de']\n",
    "\n",
    "#         inputs.append(en_text)\n",
    "#         targets.append(de_text)\n",
    "            \n",
    "#     padding = 'max_length'\n",
    "#     model_inputs = tokenizer(\n",
    "#         inputs,\n",
    "#         max_length=args.source_max_length,\n",
    "#         padding=padding,\n",
    "#         truncation=True,\n",
    "#         return_tensors=\"pt\",\n",
    "#     )\n",
    "#     # Tokenize targets with the `text_target` keyword argument\n",
    "#     labels = tokenizer(\n",
    "#         text_target=targets,\n",
    "#         max_length=args.train_target_max_length,\n",
    "#         padding=padding,\n",
    "#         truncation=True,\n",
    "    #     return_tensors=\"pt\",\n",
    "    # )\n",
    "\n",
    "    # if padding == \"max_length\":\n",
    "    #             labels[\"input_ids\"] = [\n",
    "    #                 [(l if l != tokenizer.pad_token_id else -100) for l in label] for label in labels[\"input_ids\"]\n",
    "    #             ]\n",
    "\n",
    "    # model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    # model_inputs[\"decoder_attention_mask\"] = labels[\"attention_mask\"]\n",
    "    # return model_inputs\n",
    "\n",
    "en_gr_dataset = datasets.load_dataset('wmt16', 'de-en')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Base Speculative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "v3kozZQNyBBQ"
   },
   "outputs": [],
   "source": [
    "class SpeculativeDecoder:\n",
    "    def __init__(\n",
    "        self,\n",
    "        target_model_name = \"google-t5/t5-3b\",\n",
    "        draft_model_name = \"google-t5/t5-small\",\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "        gamma = 4,\n",
    "        temperature = 0.5\n",
    "    ):\n",
    "        self.device = device\n",
    "        self.gamma = gamma\n",
    "\n",
    "        self.temperature = temperature\n",
    "\n",
    "        self.tokenizer = T5Tokenizer.from_pretrained(target_model_name)\n",
    "        \n",
    "        # self.target_model = T5ForConditionalGeneration.from_pretrained(target_model_name).to(device)\n",
    "        self.target_model = AutoModelForSeq2SeqLM.from_pretrained(target_model_name, device_map='auto')\n",
    "\n",
    "        self.draft_model = T5ForConditionalGeneration.from_pretrained(draft_model_name).to(device)\n",
    "\n",
    "        self.target_model.eval()\n",
    "        self.draft_model.eval()\n",
    "\n",
    "    def get_draft_logits(\n",
    "        self,\n",
    "        input_ids: torch.Tensor,\n",
    "        attention_mask: torch.Tensor,\n",
    "        decoder_input_ids: torch.Tensor,\n",
    "        gamma: int\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"Get draft logits for gamma tokens\"\"\"\n",
    "        draft_tokens = []\n",
    "        draft_probs = []\n",
    "        current_decoder_ids = decoder_input_ids\n",
    "\n",
    "        # Generate gamma tokens from the draft model\n",
    "        for _ in range(gamma):\n",
    "            with torch.no_grad():\n",
    "                outputs = self.draft_model(\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    decoder_input_ids=current_decoder_ids,\n",
    "                    return_dict=True\n",
    "                )\n",
    "                logits = outputs.logits[:, -1, :]  # Get logits for last position\n",
    "                probs = F.softmax(logits, dim=-1)\n",
    "\n",
    "                # Sample token\n",
    "                token_id = torch.multinomial(probs, num_samples=1).squeeze(-1)\n",
    "                prob = probs.gather(-1, token_id.unsqueeze(-1)).squeeze(-1)\n",
    "\n",
    "                draft_tokens.append(token_id.item())\n",
    "                draft_probs.append(prob.item())\n",
    "\n",
    "                # Update decoder inputs for next iteration\n",
    "                current_decoder_ids = torch.cat(\n",
    "                    [current_decoder_ids, token_id.view(1, 1)],\n",
    "                    dim=1\n",
    "                )\n",
    "\n",
    "                if token_id.item() == self.tokenizer.eos_token_id:\n",
    "                    break\n",
    "\n",
    "        return draft_tokens, draft_probs#, current_decoder_ids, outputs.logits\n",
    "\n",
    "    # def get_target_probs(\n",
    "    #     self,\n",
    "    #     input_ids: torch.Tensor,\n",
    "    #     attention_mask: torch.Tensor,\n",
    "    #     decoder_input_ids: torch.Tensor,\n",
    "    #     draft_tokens: torch.Tensor\n",
    "    # ) -> torch.Tensor:\n",
    "    #     \"\"\"Get target probabilities for the draft tokens in parallel.\"\"\"\n",
    "    #     with torch.no_grad():\n",
    "    #         # Add draft tokens to decoder input\n",
    "    #         # full_decoder_ids = torch.cat([decoder_input_ids, draft_tokens.unsqueeze(0)], dim=1)\n",
    "\n",
    "    #         full_decoder_ids = [decoder_input_ids]\n",
    "    #         for i in range(len(draft_tokens)):\n",
    "    #             x = torch.cat([decoder_input_ids, draft_tokens.unsqueeze(0)[:, :i+1]], dim=1)\n",
    "    #             full_decoder_ids.append(x)\n",
    "\n",
    "    #         maxlen = max([x.shape[1] for x in full_decoder_ids])\n",
    "\n",
    "    #         padded_decoder_ids = torch.stack([torch.tensor(\n",
    "    #             F.pad(x, (0, maxlen - x.shape[1]), value=self.tokenizer.pad_token_id)[0]\n",
    "    #         , device=self.device) for x in full_decoder_ids])\n",
    "\n",
    "    #         batch_size = padded_decoder_ids.shape[0]\n",
    "    #         input_ids_batched = input_ids.repeat(batch_size, 1)\n",
    "    #         attention_mask_batched = attention_mask.repeat(batch_size, 1)\n",
    "\n",
    "    #         # make it a triangular attention mask\n",
    "\n",
    "\n",
    "    #         # print(input_ids_batched.shape, attention_mask_batched.shape, padded_decoder_ids.shape)\n",
    "\n",
    "    #         # outputs = self.target_model(\n",
    "    #         #     input_ids=input_ids,\n",
    "    #         #     attention_mask=attention_mask,\n",
    "    #         #     decoder_input_ids=full_decoder_ids,\n",
    "    #         #     return_dict=True\n",
    "    #         # )\n",
    "    #         outputs = self.target_model(\n",
    "    #             input_ids=input_ids_batched,\n",
    "    #             attention_mask=attention_mask_batched,\n",
    "    #             decoder_input_ids=padded_decoder_ids,\n",
    "    #             # decoder_attention_mask=torch.triu(\n",
    "    #             #     torch.zeros((padded_decoder_ids.shape[0], padded_decoder_ids.shape[1]), device=self.device)\n",
    "    #             # ),\n",
    "    #             return_dict=True\n",
    "    #         )\n",
    "    #         # print(\"passes target model\")\n",
    "\n",
    "    #         batched_logits = outputs.logits\n",
    "    #         # print(batched_logits.shape)\n",
    "    #         # 5, 5, 32128\n",
    "    #         logits = torch.zeros((batched_logits.shape[1], batched_logits.shape[2]), device=self.device)\n",
    "    #         for i in range(len(draft_tokens)):\n",
    "    #             logits[i] = batched_logits[i, i, :]\n",
    "\n",
    "    #         # print(logits.shape, batched_logits[-1].shape)\n",
    "\n",
    "    #         target_probs = F.softmax(logits, dim=-1)\n",
    "\n",
    "    #         # # Get probabilities for positions before each draft token\n",
    "    #         # logits = outputs.logits[:, -(len(draft_tokens) + 1):-1, :]\n",
    "    #         # target_probs = F.softmax(logits, dim=-1)\n",
    "\n",
    "    #         # print(batched_logits[-1].unsqueeze(0).shape)\n",
    "\n",
    "    #         return target_probs.squeeze(0), logits.unsqueeze(0)\n",
    "\n",
    "    def get_target_probs(\n",
    "        self,\n",
    "        input_ids: torch.Tensor,\n",
    "        attention_mask: torch.Tensor,\n",
    "        decoder_input_ids: torch.Tensor,\n",
    "        draft_tokens: torch.Tensor\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"Get target probabilities for the draft tokens in parallel.\"\"\"\n",
    "        with torch.no_grad():\n",
    "            # Add draft tokens to decoder input\n",
    "            full_decoder_ids = torch.cat([decoder_input_ids, draft_tokens.unsqueeze(0)], dim=1)\n",
    "\n",
    "            # print(input_ids_batched.shape, attention_mask_batched.shape, padded_decoder_ids.shape)\n",
    "\n",
    "            outputs = self.target_model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                decoder_input_ids=full_decoder_ids,\n",
    "                # decoder_attention_mask=torch.triu(\n",
    "                #     torch.zeros((full_decoder_ids.shape[0], full_decoder_ids.shape[1]), device=self.device)\n",
    "                # ),\n",
    "                return_dict=True\n",
    "            )\n",
    "\n",
    "            # Get probabilities for positions before each draft token\n",
    "            logits = outputs.logits[:, -(len(draft_tokens) + 1):-1, :]\n",
    "            target_probs = F.softmax(logits, dim=-1)\n",
    "\n",
    "            return target_probs.squeeze(0), outputs.logits\n",
    "\n",
    "    # def verify_tokens(\n",
    "    #     self,\n",
    "    #     target_probs: torch.Tensor,\n",
    "    #     draft_tokens: torch.Tensor,\n",
    "    #     draft_probs: torch.Tensor,\n",
    "    # ) -> int:\n",
    "    #     \"\"\"Determine number of accepted tokens\"\"\"\n",
    "    #     # Get target probabilities for the draft tokens\n",
    "    #     target_probs_draft_tokens = target_probs.gather(\n",
    "    #         -1,\n",
    "    #         draft_tokens.unsqueeze(-1)\n",
    "    #     ).squeeze(-1)\n",
    "\n",
    "    #     # Calculate acceptance ratios\n",
    "    #     acceptance_ratios = target_probs_draft_tokens / draft_probs\n",
    "\n",
    "    #     # Sample uniform random numbers\n",
    "    #     random_nums = torch.rand_like(acceptance_ratios)\n",
    "\n",
    "    #     # Find number of accepted tokens\n",
    "    #     # Accept if random number < min(1, target_prob / draft_prob)\n",
    "    #     accepts = random_nums < torch.minimum(\n",
    "    #         torch.ones_like(acceptance_ratios),\n",
    "    #         acceptance_ratios\n",
    "    #     )\n",
    "\n",
    "    #     # Find first rejection\n",
    "    #     try:\n",
    "    #         n_accepted = torch.where(~accepts)[0][0].item()\n",
    "    #     except:\n",
    "    #         n_accepted = len(accepts)\n",
    "\n",
    "    #     return n_accepted\n",
    "\n",
    "    def verify_tokens(\n",
    "        self,\n",
    "        target_probs: torch.Tensor,\n",
    "        draft_tokens: torch.Tensor,\n",
    "        draft_probs: torch.Tensor,\n",
    "        temperature: float = 1.0\n",
    "    ) -> int:\n",
    "        \"\"\"Determine number of accepted tokens\"\"\"\n",
    "        # Get target probabilities for the draft tokens\n",
    "        target_probs_draft_tokens = target_probs.gather(\n",
    "            -1,\n",
    "            draft_tokens.unsqueeze(-1)\n",
    "        ).squeeze(-1)\n",
    "\n",
    "        target_probs_draft_tokens = target_probs_draft_tokens / max(temperature, 1e-10)\n",
    "        draft_probs = draft_probs / max(temperature, 1e-10)\n",
    "\n",
    "        # Calculate acceptance ratios\n",
    "        acceptance_ratios = target_probs_draft_tokens.float() / draft_probs\n",
    "\n",
    "        # Sample uniform random numbers \n",
    "        random_nums = torch.zeros_like(target_probs_draft_tokens).float().uniform_()\n",
    "\n",
    "        mask = random_nums > acceptance_ratios\n",
    "        num_accepted = (mask.cumsum(dim = -1) == 0).sum(dim = -1)\n",
    "\n",
    "        return num_accepted.int().item()\n",
    "\n",
    "    def translate(\n",
    "        self,\n",
    "        source_text: str,\n",
    "        max_length: int = 128\n",
    "    ) -> str:\n",
    "        \"\"\"Translate source text using speculative decoding.\"\"\"\n",
    "        # Encode source text\n",
    "        encoder_inputs = self.tokenizer(\n",
    "            f\"translate English to German: {source_text}\",\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True\n",
    "        ).to(self.device)\n",
    "\n",
    "        # Initialize with start token\n",
    "        decoder_input_ids = torch.tensor([[self.tokenizer.pad_token_id]], device=self.device)\n",
    "\n",
    "        output = self.target_model(\n",
    "            input_ids=encoder_inputs.input_ids,\n",
    "            attention_mask=encoder_inputs.attention_mask,\n",
    "            decoder_input_ids=decoder_input_ids,\n",
    "            return_dict=True\n",
    "        )\n",
    "\n",
    "        probs = output.logits[:, -1, :]\n",
    "                    \n",
    "        probs = F.softmax(probs, dim=-1)\n",
    "        token_id = torch.multinomial(probs, num_samples=1)\n",
    "\n",
    "        decoder_input_ids = torch.tensor([[self.tokenizer.pad_token_id, token_id.item()]], device=self.device)\n",
    "\n",
    "\n",
    "        total_tokens = 0\n",
    "        accepted_tokens = 0\n",
    "\n",
    "        while decoder_input_ids.shape[1] < max_length:\n",
    "            # Get draft tokens autoregressively\n",
    "            draft_tokens, draft_probs = self.get_draft_logits(\n",
    "                encoder_inputs.input_ids,\n",
    "                encoder_inputs.attention_mask,\n",
    "                decoder_input_ids,\n",
    "                self.gamma\n",
    "            )\n",
    "\n",
    "            draft_tokens = torch.tensor(draft_tokens, device=self.device)\n",
    "            draft_probs = torch.tensor(draft_probs, device=self.device)\n",
    "\n",
    "            if len(draft_tokens) == 0:\n",
    "                raise ValueError(\"Draft tokens not generated.\")\n",
    "\n",
    "            # Get target probabilities in parallel\n",
    "            # start = time.time()\n",
    "            target_probs, target_logits = self.get_target_probs(\n",
    "                encoder_inputs.input_ids,\n",
    "                encoder_inputs.attention_mask,\n",
    "                decoder_input_ids,\n",
    "                draft_tokens\n",
    "            )\n",
    "            # print(\"Time taken for target probs: \", time.time() - start)\n",
    "\n",
    "            # Verify tokens\n",
    "            n_accepted = self.verify_tokens(target_probs, draft_tokens, draft_probs, self.temperature)\n",
    "            # print(n_accepted)\n",
    "\n",
    "            # Accept verified tokens\n",
    "            if n_accepted > 0:\n",
    "                decoder_input_ids = torch.cat([\n",
    "                    decoder_input_ids,\n",
    "                    draft_tokens[:n_accepted].unsqueeze(0)\n",
    "                ], dim=1)\n",
    "                \n",
    "            with torch.no_grad():\n",
    "                # n_rejected = self.gamma - n_accepted\n",
    "                n_rejected = len(draft_tokens) - n_accepted # CHECK IF THIS IS CORRECT\n",
    "                total_tokens += len(draft_tokens) #self.gamma\n",
    "                accepted_tokens += n_accepted\n",
    "                if n_rejected > 0:\n",
    "                    probs = target_logits[:, -n_rejected, :] #- draft_logits[:, 1-n_rejected, :]\n",
    "                else:\n",
    "                    probs = target_logits[:, -1, :]\n",
    "                    \n",
    "                probs = F.softmax(probs, dim=-1)\n",
    "                token_id = torch.multinomial(probs, num_samples=1)\n",
    "\n",
    "                decoder_input_ids = torch.cat([decoder_input_ids, token_id], dim=1)\n",
    "\n",
    "            # Check for end of sequence\n",
    "            if decoder_input_ids[0][-1].item() == self.tokenizer.eos_token_id:\n",
    "                break\n",
    "\n",
    "        # Decode translation\n",
    "        translation = self.tokenizer.decode(\n",
    "            decoder_input_ids[0],\n",
    "            skip_special_tokens=True,\n",
    "            clean_up_tokenization_spaces=True\n",
    "        )\n",
    "        perc_accepted = accepted_tokens / total_tokens * 100\n",
    "        return translation, perc_accepted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# speculative_decoder = SpeculativeDecoder(gamma=2)\n",
    "\n",
    "# source_text = \"Obama receives Netanyahu\"\n",
    "\n",
    "# speculative_decoder.translate(source_text)\n",
    "# start = time.time()\n",
    "# speculative_translation, pc = speculative_decoder.translate(source_text)\n",
    "# end = time.time()\n",
    "# print(f\"Speculative translation: {speculative_translation}\")\n",
    "# print(f\"Percentage tokens accepted: {pc:.2f}%\")\n",
    "# print(f\"Time taken: {end - start:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "CopfVI-mjdb4"
   },
   "outputs": [],
   "source": [
    "class NormalDecoder:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name: str = \"google-t5/t5-3b\",\n",
    "        device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "    ):\n",
    "        self.device = device\n",
    "\n",
    "        # Initialize tokenizer and model\n",
    "        self.tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "        # self.model = T5ForConditionalGeneration.from_pretrained(model_name).to(device)\n",
    "        self.model = AutoModelForSeq2SeqLM.from_pretrained(model_name, device_map='auto')\n",
    "        self.model.eval()\n",
    "\n",
    "    def get_logits(\n",
    "        self,\n",
    "        input_ids: torch.Tensor,\n",
    "        attention_mask: torch.Tensor,\n",
    "        decoder_input_ids: torch.Tensor\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"Get logits from model for the last token.\"\"\"\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                decoder_input_ids=decoder_input_ids,\n",
    "                return_dict=True\n",
    "            )\n",
    "            return outputs.logits[:, -1, :]\n",
    "\n",
    "    def sample_token(self, logits: torch.Tensor, temperature: float = 1.0) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"Sample a token from logits using temperature sampling.\"\"\"\n",
    "        if temperature == 0:\n",
    "            # Greedy sampling\n",
    "            token_id = torch.argmax(logits, dim=-1)\n",
    "            prob = torch.ones_like(token_id, dtype=torch.float)\n",
    "        else:\n",
    "            # Temperature sampling\n",
    "            probs = F.softmax(logits / temperature, dim=-1)\n",
    "            token_id = torch.multinomial(probs, num_samples=1).squeeze(-1)\n",
    "            prob = probs.gather(-1, token_id.unsqueeze(-1)).squeeze(-1)\n",
    "        return token_id, prob\n",
    "\n",
    "    def translate(\n",
    "        self,\n",
    "        source_text: str,\n",
    "        max_length: int = 128,\n",
    "        temperature: float = 0.7\n",
    "    ) -> str:\n",
    "        \"\"\"Translate source text using the normal T5 model.\"\"\"\n",
    "        # Encode source text\n",
    "        encoder_inputs = self.tokenizer(\n",
    "            f\"translate English to German: {source_text}\",\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True\n",
    "        ).to(self.device)\n",
    "\n",
    "        # Initialize decoder input with start token\n",
    "        decoder_input_ids = torch.tensor([[self.tokenizer.pad_token_id]], device=self.device)\n",
    "\n",
    "        while decoder_input_ids.shape[1] < max_length:\n",
    "            # Generate logits for the next token\n",
    "            logits = self.get_logits(\n",
    "                encoder_inputs.input_ids,\n",
    "                encoder_inputs.attention_mask,\n",
    "                decoder_input_ids\n",
    "            )\n",
    "\n",
    "            # Sample a token\n",
    "            token_id, _ = self.sample_token(logits, temperature)\n",
    "\n",
    "            # Add token to the decoder input\n",
    "            decoder_input_ids = torch.cat(\n",
    "                [decoder_input_ids, token_id.view(1, 1)],\n",
    "                dim=1\n",
    "            )\n",
    "\n",
    "            # Break if end token is generated\n",
    "            if token_id.item() == self.tokenizer.eos_token_id:\n",
    "                break\n",
    "\n",
    "        # Decode and return translation\n",
    "        translation = self.tokenizer.decode(\n",
    "            decoder_input_ids[0],\n",
    "            skip_special_tokens=True,\n",
    "            clean_up_tokenization_spaces=True\n",
    "        )\n",
    "        return translation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# speculative_decoder = SpeculativeDecoder()\n",
    "# normal_decoder = NormalDecoder()\n",
    "\n",
    "# source_text = \"He said Lamb made the fateful 911 call sometime after that.\" # spec does not work\n",
    "\n",
    "# speculative_decoder.translate(source_text)\n",
    "# start = time.time()\n",
    "# speculative_translation, pc = speculative_decoder.translate(source_text)\n",
    "# end = time.time()\n",
    "# print(f\"Speculative translation: {speculative_translation}\")\n",
    "# print(f\"Percentage tokens accepted: {pc:.2f}%\")\n",
    "# print(f\"Time taken: {end - start:.4f} seconds\")\n",
    "\n",
    "# start = time.time()\n",
    "# normal_translation = normal_decoder.translate(source_text)\n",
    "# end = time.time()\n",
    "# print(f\"Normal translation: {normal_translation}\")\n",
    "# print(f\"Time taken: {end - start:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nwtUoGDUjiSD",
    "outputId": "2a92f134-5a76-45b9-884b-51515d57fa4d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "  3%|▎         | 1/30 [00:03<01:33,  3.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source: Obama receives Netanyahu\n",
      "Normal Translation: Obama empfing Netanjahu\n",
      "Time taken: 0.72 seconds\n",
      "Speculative Translation: Obama Netanjahu empfing\n",
      "Time taken: 2.49 seconds\n",
      "Percentage tokens accepted: 60.00%\n",
      "Target: Obama empfängt Netanyahu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 2/30 [00:07<01:50,  3.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source: The relationship between Obama and Netanyahu is not exactly friendly.\n",
      "Normal Translation: Die Beziehung zwischen Obama und Netanjahu ist nicht gerade freundschaftlich.\n",
      "Time taken: 1.64 seconds\n",
      "Speculative Translation: Die Beziehungen zwischen Obama und Netanjahu sind nichtakt. Andersgesagt, das  gegenebul Klima gibt es in Europa nie. Nicht: wir diese 1995 erträumtenk Revolution in Berlin\n",
      "Time taken: 2.81 seconds\n",
      "Percentage tokens accepted: 42.86%\n",
      "Target: Das Verhältnis zwischen Obama und Netanyahu ist nicht gerade freundschaftlich.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 3/30 [00:13<02:11,  4.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source: The two wanted to talk about the implementation of the international agreement and about Teheran's destabilising activities in the Middle East.\n",
      "Normal Translation: Die beiden wollten über die Umsetzung des internationalen Abkommens und über Teheran's destabilisierende Aktivitäten im Nahen Osten sprechen.\n",
      "Time taken: 2.90 seconds\n",
      "Speculative Translation: Daswätige Wort stammt vom Ausshiadan Dara, dem von den NGOs Hochnation- Azerthwa zu der-m\n",
      "Time taken: 3.06 seconds\n",
      "Percentage tokens accepted: 15.00%\n",
      "Target: Die beiden wollten über die Umsetzung der internationalen Vereinbarung sowie über Teherans destabilisierende Maßnahmen im Nahen Osten sprechen.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 4/30 [00:21<02:39,  6.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source: The meeting was also planned to cover the conflict with the Palestinians and the disputed two state solution.\n",
      "Normal Translation: „Die israelische Regierung hat die israelische Währung im Land zerstört, die floh auf dem Marktplatz, nach einem offenen Markt, der überhaupt nicht mehr existieren kann“.\n",
      "Time taken: 4.00 seconds\n",
      "Speculative Translation: Se. Kalenderonul Georgs. Sichtbar war dass sämtlicheMädchen unter ihrertanz Exnikawege und Grundstücke entlang der Confluences würden, den Sohn Fas vondj keiner zu töten.\n",
      "Time taken: 4.03 seconds\n",
      "Percentage tokens accepted: 26.67%\n",
      "Target: Bei der Begegnung soll es aber auch um den Konflikt mit den Palästinensern und die diskutierte Zwei-Staaten-Lösung gehen.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 5/30 [00:24<01:59,  4.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source: Relations between Obama and Netanyahu have been strained for years.\n",
      "Normal Translation: Die Beziehungen zwischen Obama und Netanjahu sind seit Jahren gespannt.\n",
      "Time taken: 1.39 seconds\n",
      "Speculative Translation: Die Beziehungen zwischen Obama und Netanjahu sind seit Jahren verfellich.\n",
      "Time taken: 0.97 seconds\n",
      "Percentage tokens accepted: 61.90%\n",
      "Target: Das Verhältnis zwischen Obama und Netanyahu ist seit Jahren gespannt.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 6/30 [00:29<01:56,  4.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source: Washington criticises the continuous building of settlements in Israel and accuses Netanyahu of a lack of initiative in the peace process.\n",
      "Normal Translation: Washington kritisiert den fortgesetzten Aufbau von Siedlungen in Israel und beschuldigt Netanjahu des Mangels an Initiative im Friedensprozess.\n",
      "Time taken: 3.32 seconds\n",
      "Speculative Translation: Washington kritisiert den ständigen  Bau von Siedlungen in Israel und beschuldigt Netanjahu  einedetächtige Initiative Friedensprozesse\n",
      "Time taken: 1.77 seconds\n",
      "Percentage tokens accepted: 59.09%\n",
      "Target: Washington kritisiert den andauernden Siedlungsbau Israels und wirft Netanyahu mangelnden Willen beim Friedensprozess vor.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 7/30 [00:33<01:50,  4.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source: The relationship between the two has further deteriorated because of the deal that Obama negotiated on Iran's atomic programme, .\n",
      "Normal Translation: Die Beziehungen zwischen beiden haben sich durch die Abmachung, die Obama über das Atomprogramm des Iran ausgehandelt hat, noch weiter verschlechtert.\n",
      "Time taken: 3.15 seconds\n",
      "Speculative Translation: Die Beziehungen zwischen beiden Staaten haben sich auch weiter verschlechtert, Obamas Abkommen über das Atomprogramm des Iran im Unter.\n",
      "Time taken: 1.53 seconds\n",
      "Percentage tokens accepted: 62.16%\n",
      "Target: Durch den von Obama beworbenen Deal um das iranische Atomprogramm hat sich die Beziehung der beiden weiter verschlechtert.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 8/30 [00:39<01:53,  5.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source: In March, at the invitation of the Republicans, Netanyahu made a controversial speech to the US Congress, which was partly seen as an affront to Obama.\n",
      "Normal Translation: Im März hielt Netanjahu auf Einladung der Republikaner eine umstrittene Rede vor dem US-Kongress ab, die teilweise als Affront gegen Obama angesehen wurde.\n",
      "Time taken: 4.06 seconds\n",
      "Speculative Translation: Im März hielt Netanjahu auf Einladung der Republikaner eine kontroverse Rede den US-Kongress vor, dieer als Affront Obama angesehen wurde.\n",
      "Time taken: 1.89 seconds\n",
      "Percentage tokens accepted: 68.75%\n",
      "Target: Im März hatte Netanyahu auf Einladung der Republikaner vor dem US-Kongress eine umstrittene Rede gehalten, die teils als Affront gegen Obama gewertet wurde.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 9/30 [00:44<01:47,  5.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source: The speech had not been agreed with Obama, who had rejected a meeting with reference to the election that was at that time impending in Israel.\n",
      "Normal Translation: Die Rede war nicht mit Obama vereinbart worden, der ein Treffen mit Bezug auf die Wahl, die damals in Israel ansteht, abgelehnt hatte.\n",
      "Time taken: 2.89 seconds\n",
      "Speculative Translation: Die Rede war Obama nicht vereinbart der einen betreffend Begegnung Blick auf die Wahl, die in Israel zum lichen bevor..\n",
      "Time taken: 2.06 seconds\n",
      "Percentage tokens accepted: 36.00%\n",
      "Target: Die Rede war mit Obama nicht abgesprochen, ein Treffen hatte dieser mit Hinweis auf die seinerzeit bevorstehende Wahl in Israel abgelehnt.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 10/30 [00:47<01:27,  4.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source: In 911 Call, Professor Admits to Shooting Girlfriend\n",
      "Normal Translation: In einem 911-Anruf räumt ein Professor ein, seine Freundin erschossen zu haben\n",
      "Time taken: 1.87 seconds\n",
      "Speculative Translation: In Ruf 911 Professora1 einließ Freundin\n",
      "Time taken: 0.95 seconds\n",
      "Percentage tokens accepted: 20.83%\n",
      "Target: In einem Notruf gesteht Professor, seine Freundin erschossen zu haben\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 11/30 [00:55<01:44,  5.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source: In a 911 call, his voice only slightly shaky, college professor Shannon Lamb told police he had shot his girlfriend and officers needed to get over to their house.\n",
      "Normal Translation: In einer Telefonnummernrufe mit nur leicht zäher Stimme erklärte der Hochschulprofessor Shannon Lamb der Polizei, dass er seine Freundin erschossen habe und die Polizei zum Haus siesel gäbe.\n",
      "Time taken: 4.64 seconds\n",
      "Speculative Translation: In einem-ück  911-Anruf mit der Schwerkeit seiner Stimme erklärte College-Professor Shannon Lamb die Polizei, er habe seine Freundinschossen– und Bord, dieenfalls von der Polizeiiert seien zu ihre Unterkunft würden!\n",
      "Time taken: 3.38 seconds\n",
      "Percentage tokens accepted: 38.82%\n",
      "Target: In einem Notruf erzählte Professor Shannon Lamb mit einer etwas zittrigen Stimme der Polizei, dass er seine Freundin erschossen habe und dass die Beamten zu seinem Haus kommen müssten.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 12/30 [01:02<01:46,  5.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source: Lamb made a point to say his \"sweet dog\" was there alive and probably upset, and said the dead woman's family contacts could be found on her phone.\n",
      "Normal Translation: Lamb machte einen Punkt, um zu sagen, dass sein \"süßer Hund\" war dort lebend und vermutlich verärgert, und sagte, die Kontakte der Familie der toten Frau auf ihrem Telefon zu finden.\n",
      "Time taken: 4.36 seconds\n",
      "Speculative Translation: Lamb hat angeen dasser seinen \"süßen Hund\" in Siouxdew wurde leben und wahrscheinlichstern getausch und dass sich die Familienkontakte der Toten auf Telefon.\n",
      "Time taken: 2.58 seconds\n",
      "Percentage tokens accepted: 42.19%\n",
      "Target: Lamb war es wichtig zu betonen, dass sein \"süßer Hund\" aber noch lebe und wahrscheinlich aufgeregt sei, und er sagte, die Familienkontakte der toten Frau könnten auf dem Handy gefunden werden.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 13/30 [01:11<01:58,  6.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source: Inside the home, officers found Amy Prentiss' body and a hand-written note scribbled on a white legal pad: \"I am so very sorry I wish I could take it back I loved Amy and she is the only woman who ever loved me,\" read the letter authorities say was signed by Lamb.\n",
      "Normal Translation: Das Unternehmen hat seine Geschäfte mit der amerikanischen Branche mit einem großen Umsatz im Konzernbereich Transport und Logistik angekurbelt, es sind heute rund 20.000 in-house- & - Logistik- und Logistikfirmen in der USA und im Ausland.\n",
      "Time taken: 5.97 seconds\n",
      "Speculative Translation: Während diestarken Berufskrankheiten guteg Beg rechnen Bundestags sind uns langsam. Nicht zu des  Wortes, sehr guteich ist auch meine Meinung derl Poli.\n",
      "Time taken: 3.30 seconds\n",
      "Percentage tokens accepted: 22.50%\n",
      "Target: Innerhalb des Hauses fanden die Beamten die Leiche von Amy Prentiss und eine handgeschriebene Notiz, die auf einen weißen Block gekritzelt war: \"Mir tut es so leid, ich wollte, ich könnte es rückgängig machen, ich liebte Amy und sie ist die einzige Frau, die mich jemals liebte\". Dies stand nach Angaben der Behörden in dem Brief, und er war von Lamb unterzeichnet.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 14/30 [01:24<02:17,  8.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source: There was no indication that Lamb, who was teaching two online classes for Delta State University in Cleveland, Mississippi, had already traveled 300 miles to the school's campus, where police believe he shot and killed a well-liked history professor, Ethan Schmidt, in the doorway to his office.\n",
      "Normal Translation: Der Verdacht, dass Lamb, der zwei Online-Kurse bei der Delta State University in Cleveland, Mississippi, instructiert hatte, bereits 540 Kilometer zu dem Campus der Schule gefahren war, wo die Polizei glauben, dass er einen beliebten Geschichtsprofessor, Ethan Schmidt, in der Tür seines Büros erschoss und tötete.\n",
      "Time taken: 8.10 seconds\n",
      "Speculative Translation: Es gab keine Hinweise darauf, dass Lamb, der zwei Online-Klassen für die Delta State University in Cleveland, Mississippi,, schon 300 Meilen zum Campus der   Polyiden reiste, wo Polizei glauben, dass er einen gutgesnten Geschichtsprofessor, Ethan Schmidt, in der Tür zu seinem Büro geschschossen und getötet.\n",
      "Time taken: 4.29 seconds\n",
      "Percentage tokens accepted: 61.39%\n",
      "Target: Es gab keinen Hinweis darauf, dass Lamb, der in zwei Online-Kursen für die Delta State University in Cleveland, Mississippi, unterrichtete, bereits 300 Meilen zu dem Schulgelände gereist war, wo er nach Angaben der Polizei einen beliebten Geschichtsprofessor, Ethan Schmidt, an der Tür zu seinem Büro erschossen und getötet hat.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 15/30 [01:31<02:02,  8.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source: Delta State University police chief Lynn Buford said university officials heard about the shooting at 10:18 a.m.\n",
      "Normal Translation: Der amerikanische Flughafen in Los Angeles wird anlässlich der nächsten Flugverbindung nicht abgelegt.\n",
      "Time taken: 1.99 seconds\n",
      "Speculative Translation: In derre Idida Roomts kam Bing's für einen Interview der -i-strate Andreas Ratau zu einer galt wie dem DIN A15.en und später in der neoalbden MediennetZ Easte seint. A eine Studie\n",
      "Time taken: 5.17 seconds\n",
      "Percentage tokens accepted: 33.94%\n",
      "Target: Delta State University Polizeichef Lynn Buford sagte, dass Universitäts-Mitarbeiter das Schießen um 10:18 Uhr gehört haben.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 16/30 [01:37<01:44,  7.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source: He said Lamb made the fateful 911 call sometime after that.\n",
      "Normal Translation: Er sagte, Lamb machte den schicksalhaften 911-Anruf irgendwann danach.\n",
      "Time taken: 2.55 seconds\n",
      "Speculative Translation: Johnson27 und Econ wurden von Louis  Dre, Nathanmie bak|angw. *:fragt aufoane \"Hperide\n",
      "Time taken: 3.21 seconds\n",
      "Percentage tokens accepted: 20.90%\n",
      "Target: Er sagte, dass Lamb den verhängnisvollen Notruf irgendwann danach gemacht hat.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 17/30 [01:49<01:56,  8.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source: By the end of the day, there would be one more death: Lamb took his own life as police closed in on him.\n",
      "Normal Translation: Bis zum Ende des Tages war noch ein Tot zu beklagen: Lamb tötete sich selbst, während die Polizei auf ihn zuging.\n",
      "Time taken: 3.21 seconds\n",
      "Speculative Translation: Nachmmentsgerede stellten die Chancen einer Umbesetzungsierung derheitemäßig fest.Saendigebe fu@ lt,endem die Erde sich in einer Dopplewegebildete neuenNachtdeationliege.Iorany will wenig, die tue. Bild von der untauglichen Sicht Kelers aus.Hur.Kova siesten sich auf Fox.I Februar 2006 eine iconcla alsonnem Anre\n",
      "Time taken: 9.32 seconds\n",
      "Percentage tokens accepted: 31.61%\n",
      "Target: Bis zum Ende des Tages gab es einen weiteren Tod: Lamm nahm sich das Leben, als die Polizei ihn einkesselte.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 18/30 [01:57<01:43,  8.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source: A day after the school shooting forced students and faculty to hide behind locked doors, authorities were still trying to piece together what motivated Lamb.\n",
      "Normal Translation: Ein Tag nach dem Schulausbruch, der Schüler und Lehrer zwang, sich hinter verschlossenen Türen zu verstecken, versuchten die Behörden noch immer, herauszufinden, worauf Lamb zurückgreifen wollte.\n",
      "Time taken: 5.00 seconds\n",
      "Speculative Translation: Ein Tag nach der Schulanschläge erzwungen Studenten und Lehrer sich unter verschlossenen Türentenstehlen zu, probierten die Behörden nach wie vor zusammenzu, was Lambiert.\n",
      "Time taken: 2.75 seconds\n",
      "Percentage tokens accepted: 53.57%\n",
      "Target: Einen Tag nach der Schießerei in der Universität, die Studenten und Dozenten dazu zwang, sich hinter verschlossenen Türen zu verstecken, versuchen die Behörden immer noch, sich ein Bild davon zu verschaffen, was Lamb motivierte.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|██████▎   | 19/30 [02:07<01:40,  9.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source: The details released by investigators at both ends of the state as well as students and staff who knew him helped paint a picture of a talented but possibly troubled teacher.\n",
      "Normal Translation: Die Details, die von den Ermittlern in beiden Enden des Staates sowie von Studenten und Mitarbeitern freigegeben wurden, die ihn kannten, haben dazu beigetragen, ein Bild von einem talentierten, aber vermutlich beunruhigenden Lehrer zu zeichnen.\n",
      "Time taken: 6.36 seconds\n",
      "Speculative Translation: Andenfalls war er auch derste, der von den USA fragte Erigte (us.ineid, f r Judas, der com habe oder bleibt wer weiß gendet).\n",
      "Time taken: 3.91 seconds\n",
      "Percentage tokens accepted: 30.12%\n",
      "Target: Die von den Ermittlern an beiden Enden des Staates veröffentlichten Details, wie auch das, was Studenten und Mitarbeiter, die ihn kannten, aussagten, half dabei, ein Bild von einem talentierten, aber möglicherweise schwierigen Lehrer zu zeichnen.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 20/30 [02:09<01:10,  7.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source: Students said they looked forward to his class.\n",
      "Normal Translation: Die Studenten sagten, sie freuen sich auf seine Klasse.\n",
      "Time taken: 1.64 seconds\n",
      "Speculative Translation: Schüler haben, dass man auf seine Stelle wartet\n",
      "Time taken: 0.64 seconds\n",
      "Percentage tokens accepted: 58.33%\n",
      "Target: Studenten sagten, dass sie sich auf seinen Unterricht freuten.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 21/30 [02:20<01:12,  8.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source: Police in Gautier, where Prentiss died, said he had no history of violence or criminal record.\n",
      "Normal Translation: Die Polizei in Gautier, wo Prentiss gestorben ist, sagte, dass er keine Geschichte von Gewalt und keinen Strafregistereintrag habe.\n",
      "Time taken: 3.52 seconds\n",
      "Speculative Translation: Noch 1939 um: kurzepäagogische und politische Aufführungen wurde von der dortigen Kommunikationsintanz -ageleitet Hannah Buna Arkansas- eingeführt. Sie waren am eines dernersten Kindes die Staatsgehörigkeit erten sich bei ihnen vonppell umsetzt.A Jahre späters kamen in Brüssel wiederUngläubige auf die Kontente\n",
      "Time taken: 6.79 seconds\n",
      "Percentage tokens accepted: 35.66%\n",
      "Target: Die Polizei in Gautier, wo Prentiss starb, sagte, er habe keine Geschichte der Gewalt oder eine kriminelle Vorgeschichte.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 22/30 [02:26<00:59,  7.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source: Schmidt himself had included Lamb in a book he wrote where he acknowledged the \"wonderful people\" he shared his academic life with.\n",
      "Normal Translation: Schmidt selbst hatte Lamb in ein Buch aufgenommen, in dem er die \"wundervollen Leute\" anerkennt, mit denen er sein akademisches Leben gemeinsam hatte.\n",
      "Time taken: 4.08 seconds\n",
      "Speculative Translation: Da Lawrence. diese bekämpft und verlieren an Haltung nahm eine Ausbildung an der UCLA statt.\n",
      "Time taken: 2.04 seconds\n",
      "Percentage tokens accepted: 26.19%\n",
      "Target: Schmidt selbst hatte Lamb in einem von ihm geschriebenen Buch erwähnt, in dem er die \"wunderbaren Menschen\" erwähnte, mit denen er sein akademisches Leben teilte.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|███████▋  | 23/30 [02:35<00:54,  7.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source: Both taught in the Division of Social Sciences and History, which lists 17 faculty members, and many students took courses from both.\n",
      "Normal Translation: Der größte Teil des amerikanischen Volkes räumte dies auch der ffentlichkeit ein.\n",
      "Time taken: 2.39 seconds\n",
      "Speculative Translation: Înată  Media Medias ează ababelul 20r de manys in. Inindre auits deapoat aud 9 marin dod, a absorb fast bala desLice.au oberei Dec Iyat prn.alogicalnet-90%\n",
      "Time taken: 6.30 seconds\n",
      "Percentage tokens accepted: 21.05%\n",
      "Target: Beide unterrichteten in der Abteilung für Sozialwissenschaften und Geschichte, deren Lehrkörper 17 Mitglieder umfasst, und viele Studenten besuchten Kurse von beiden.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 24/30 [02:37<00:36,  6.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source: At the same time, there were some inclinations of problems.\n",
      "Normal Translation: Zugleich hatte es Ansätze von Problemen gegeben.\n",
      "Time taken: 1.46 seconds\n",
      "Speculative Translation: Gleichzeitig waren einige sogar verfügbar Probleme da.\n",
      "Time taken: 0.79 seconds\n",
      "Percentage tokens accepted: 46.67%\n",
      "Target: Zur gleichen Zeit gab es einige Neigungen zu problematischen Verhaltensweisen.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 25/30 [02:42<00:29,  5.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source: A student who praised Lamb, Brandon Beavers, said he also seemed agitated and jittery, \"like there was something wrong with him.\"\n",
      "Normal Translation: Dennoch hat er seinen eigenen Weg gefunden, um sich wieder zu den Bemühungen der politischen Stellungnahme des demokratischen Kapitalismus zu bekehren.\n",
      "Time taken: 4.09 seconds\n",
      "Speculative Translation: Fear is not responsible. Incidental search is important but light be on this basic.\n",
      "Time taken: 1.13 seconds\n",
      "Percentage tokens accepted: 61.90%\n",
      "Target: Ein Student, Brandon Beavers, der Lamb lobte, sagte, er schien auch ein wenig aufgeregt und nervös, \"als ob etwas mit ihm falsch sei.\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 26/30 [02:49<00:24,  6.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source: Another student, Mikel Sykes, said Lamb told him he was dealing with stress at the end of the 2014-15 academic year.\n",
      "Normal Translation: Ein anderer Student, Mikel Sykes, sagte, Lamb habe ihm am Ende des Schuljahres 2014/15 mitgeteilt, dass er sich mit Stress auseinander setzt.\n",
      "Time taken: 3.75 seconds\n",
      "Speculative Translation: Ein Student, Mikel Sykes,, Pearl hat zudem gesagt, dass ihm Lambgültig, wenn sowohl  vom.. -rang: 'Stressstand'\n",
      "Time taken: 2.88 seconds\n",
      "Percentage tokens accepted: 44.83%\n",
      "Target: Ein anderer Student, Mikel Sykes, sagte, dass Lamm ihm erzählt habe, dass er am Ende des akademischen Jahres 2014-15 mit Stress zu tun hatte.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 27/30 [02:56<00:19,  6.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source: Lamb had earlier asked Delta State University for a medical leave of absence, saying he had a health issue of some sort.\n",
      "Normal Translation: Nach dem Kaiserreich Napoleon beanspruchte er die Anfänge der koalitionellen militärischen Macht und stellte sich für die Kriegserklärung gegen Frankreich ein.\n",
      "Time taken: 3.78 seconds\n",
      "Speculative Translation: Unter Berücksichtigung der Manchester-lerRatsi vondenglichenen Agrarstruktur und wiederhol Auferstehung von Arbeitslosigkeit zwischen denn wurde variierend  Preiserechnet.Allerdings entlehnte man sich, während den Umfragen diehlen würde\n",
      "Time taken: 3.89 seconds\n",
      "Percentage tokens accepted: 38.14%\n",
      "Target: Lamb hatte zuvor die Delta State University um eine Beurlaubung aus gesundheitlichen Gründen gebeten und dabei gesagt, dass er irgendein gesundheitliches Problem habe.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 28/30 [02:59<00:10,  5.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source: This year, he was only teaching two online classes.\n",
      "Normal Translation: Dieses Jahr hat er nur noch zwei Online-Klassen gehalten.\n",
      "Time taken: 1.48 seconds\n",
      "Speculative Translation: In diesem Jahr gab dieser nur Online in zweienungen.\n",
      "Time taken: 1.06 seconds\n",
      "Percentage tokens accepted: 32.00%\n",
      "Target: In diesem Jahr unterrichtete er nur zwei Online-Kurse.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|█████████▋| 29/30 [03:05<00:05,  5.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source: Recent changes in the university's hiring policies meant that the doctorate Lamb had worked so hard to earn would not guarantee him an automatic tenure track to become an assistant professor.\n",
      "Normal Translation: Nach der langer, mühsamen Arbeit, die ihn zum Doctorat beworben hatte, war er nicht mehr der ersten Hund, der als Assistent Professor zugelassen war.\n",
      "Time taken: 4.01 seconds\n",
      "Speculative Translation: Der Staat hatte sich zum \"Spal\" für Länder entscheiden die zahlzahlstechnischstengleichen ausgewiesen Dieliche.\n",
      "Time taken: 1.85 seconds\n",
      "Percentage tokens accepted: 34.78%\n",
      "Target: Neueste Änderungen in der Beschäftigungspolitik der Universität bedeuteten, dass die Promotion, für die Lamb so hart gearbeitet hatte, für ihn keine Garantie für einen automatischen Weg zu einer Anstellung als Assistant-Professor darstellen würde.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [03:13<00:00,  6.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source: University President William LaForge said he didn't know of any conflict between Lamb and Schmidt but \"obviously there was something in Mr. Lamb's mind.\"\n",
      "Normal Translation: University President William LaForge said he didn't know of any conflict between Lamb and Schmidt but \"obviously there was something in Mr. Lamb's mind\"\n",
      "Time taken: 3.30 seconds\n",
      "Speculative Translation: \"athes\".\" Aber  der neue desgroßen Fix sind auch neue Förderungen vons gehen die Char oder Behung Chsn, hergekommen Mar : \"ncel...e  mls.\" (dh\n",
      "Time taken: 4.57 seconds\n",
      "Percentage tokens accepted: 18.97%\n",
      "Target: Universitäts-Präsident William LaForge sagte, er wisse nichts von einem Konflikt zwischen Lamb und Schmidt, aber \"natürlich gab es etwas in Mr Lambs Vorstellung.\"\n",
      "\n",
      "Average time taken for normal decoding: 3.39 seconds\n",
      "Average time taken for speculative decoding: 3.05 seconds\n",
      "Average speedup over 30 iterations: 1.11x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize decoder\n",
    "from tqdm import tqdm\n",
    "\n",
    "speculative_decoder = SpeculativeDecoder(gamma=4, temperature=0.1)\n",
    "normal_decoder = NormalDecoder()\n",
    "\n",
    "spec_total_time = 0\n",
    "normal_total_time = 0\n",
    "total_iters = 0\n",
    "\n",
    "for i in tqdm(en_gr_dataset['test']['translation'][:30]):\n",
    "    source_text = i['en']\n",
    "    target_text = i['de']\n",
    "    \n",
    "    # Time the translation\n",
    "    start_time = time.time()\n",
    "    spec_translation, pc = speculative_decoder.translate(source_text)\n",
    "    end_time = time.time()\n",
    "\n",
    "    spec_time = end_time - start_time\n",
    "\n",
    "    # spec_total_time += spec_time\n",
    "    \n",
    "    start_time = time.time()\n",
    "    normal_translation = normal_decoder.translate(source_text)\n",
    "    end_time = time.time()\n",
    "\n",
    "    normal_time = end_time - start_time\n",
    "\n",
    "    # normal_total_time += normal_time\n",
    "\n",
    "    print(f\"Source: {source_text}\")\n",
    "    print(f\"Normal Translation: {normal_translation}\")\n",
    "    print(f\"Time taken: {normal_time:.2f} seconds\")\n",
    "    print(f\"Speculative Translation: {spec_translation}\")\n",
    "    print(f\"Time taken: {spec_time:.2f} seconds\")\n",
    "    print(f\"Percentage tokens accepted: {pc:.2f}%\")\n",
    "    \n",
    "    print(f\"Target: {target_text}\")\n",
    "\n",
    "    # if normal_time - spec_time > -0.1:\n",
    "    spec_total_time += spec_time\n",
    "    normal_total_time += normal_time\n",
    "    total_iters += 1\n",
    "\n",
    "print(f\"\\nAverage time taken for normal decoding: {normal_total_time / total_iters:.2f} seconds\")\n",
    "print(f\"Average time taken for speculative decoding: {spec_total_time / total_iters:.2f} seconds\")\n",
    "print(f\"Average speedup over {total_iters} iterations: {normal_total_time / spec_total_time:.2f}x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize decoder\n",
    "from tqdm import tqdm\n",
    "\n",
    "speculative_decoder = SpeculativeDecoder(gamma=8, temperature=0.1, draft_model_name='google-t5/t5-base')\n",
    "normal_decoder = NormalDecoder()\n",
    "\n",
    "spec_total_time = 0\n",
    "normal_total_time = 0\n",
    "total_iters = 0\n",
    "\n",
    "for i in tqdm(en_gr_dataset['test']['translation'][:30]):\n",
    "    source_text = i['en']\n",
    "    target_text = i['de']\n",
    "    \n",
    "    # Time the translation\n",
    "    start_time = time.time()\n",
    "    spec_translation, pc = speculative_decoder.translate(source_text)\n",
    "    end_time = time.time()\n",
    "\n",
    "    spec_time = end_time - start_time\n",
    "\n",
    "    # spec_total_time += spec_time\n",
    "    \n",
    "    start_time = time.time()\n",
    "    normal_translation = normal_decoder.translate(source_text)\n",
    "    end_time = time.time()\n",
    "\n",
    "    normal_time = end_time - start_time\n",
    "\n",
    "    # normal_total_time += normal_time\n",
    "\n",
    "    print(f\"Source: {source_text}\")\n",
    "    print(f\"Normal Translation: {normal_translation}\")\n",
    "    print(f\"Time taken: {normal_time:.2f} seconds\")\n",
    "    print(f\"Speculative Translation: {spec_translation}\")\n",
    "    print(f\"Time taken: {spec_time:.2f} seconds\")\n",
    "    print(f\"Percentage tokens accepted: {pc:.2f}%\")\n",
    "    \n",
    "    print(f\"Target: {target_text}\")\n",
    "\n",
    "    if normal_time - spec_time > -0.1:\n",
    "        spec_total_time += spec_time\n",
    "        normal_total_time += normal_time\n",
    "        total_iters += 1\n",
    "\n",
    "print(f\"\\nAverage time taken for normal decoding: {normal_total_time / total_iters:.2f} seconds\")\n",
    "print(f\"Average time taken for speculative decoding: {spec_total_time / total_iters:.2f} seconds\")\n",
    "print(f\"Average speedup over {total_iters} iterations: {normal_total_time / spec_total_time:.2f}x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize decoder\n",
    "from tqdm import tqdm\n",
    "\n",
    "speculative_decoder = SpeculativeDecoder(gamma=4, temperature=0.1, draft_model_name='google-t5/t5-large')\n",
    "normal_decoder = NormalDecoder()\n",
    "\n",
    "spec_total_time = 0\n",
    "normal_total_time = 0\n",
    "total_iters = 0\n",
    "\n",
    "for i in tqdm(en_gr_dataset['test']['translation'][:100]):\n",
    "    source_text = i['en']\n",
    "    target_text = i['de']\n",
    "    \n",
    "    # Time the translation\n",
    "    start_time = time.time()\n",
    "    spec_translation, pc = speculative_decoder.translate(source_text)\n",
    "    end_time = time.time()\n",
    "\n",
    "    spec_time = end_time - start_time\n",
    "\n",
    "    # spec_total_time += spec_time\n",
    "    \n",
    "    start_time = time.time()\n",
    "    normal_translation = normal_decoder.translate(source_text)\n",
    "    end_time = time.time()\n",
    "\n",
    "    normal_time = end_time - start_time\n",
    "\n",
    "    # normal_total_time += normal_time\n",
    "\n",
    "    print(f\"Source: {source_text}\")\n",
    "    print(f\"Normal Translation: {normal_translation}\")\n",
    "    print(f\"Time taken: {normal_time:.2f} seconds\")\n",
    "    print(f\"Speculative Translation: {spec_translation}\")\n",
    "    print(f\"Time taken: {spec_time:.2f} seconds\")\n",
    "    print(f\"Percentage tokens accepted: {pc:.2f}%\")\n",
    "    \n",
    "    print(f\"Target: {target_text}\")\n",
    "\n",
    "    if normal_time - spec_time > -0.1:\n",
    "        spec_total_time += spec_time\n",
    "        normal_total_time += normal_time\n",
    "        total_iters += 1\n",
    "\n",
    "print(f\"\\nAverage time taken for normal decoding: {normal_total_time / total_iters:.2f} seconds\")\n",
    "print(f\"Average time taken for speculative decoding: {spec_total_time / total_iters:.2f} seconds\")\n",
    "print(f\"Average speedup over {total_iters} iterations: {normal_total_time / spec_total_time:.2f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Online Spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OnlineSpeculativeDecoder:\n",
    "    def __init__(\n",
    "        self,\n",
    "        target_model_name = \"google-t5/t5-large\",\n",
    "        draft_model_name = \"google-t5/t5-small\",\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "        gamma = 4,\n",
    "        update_interval=2,  # Update draft model after every `update_interval` iterations\n",
    "        buffer_size_threshold=2,  # Buffer size threshold for updates\n",
    "        time_threshold=2,  # Time threshold (in seconds) for updates\n",
    "    ):\n",
    "        self.device = device\n",
    "        self.gamma = gamma\n",
    "        self.update_interval = update_interval\n",
    "        self.buffer_size_threshold = buffer_size_threshold\n",
    "        self.time_threshold = time_threshold\n",
    "        self.last_update_time = time.time()  # Track last update time\n",
    "\n",
    "        self.tokenizer = T5Tokenizer.from_pretrained(target_model_name)\n",
    "        self.target_model = T5ForConditionalGeneration.from_pretrained(target_model_name).to(device)\n",
    "        self.draft_model = T5ForConditionalGeneration.from_pretrained(draft_model_name).to(device)\n",
    "\n",
    "        self.target_model.eval()\n",
    "        self.draft_model.eval()\n",
    "\n",
    "        # Buffers for storing token proposals and updates\n",
    "        self.replay_buffer = []\n",
    "        self.temp_buffer = []  # Temporary buffer for a single request\n",
    "\n",
    "        # Counter for iteration tracking\n",
    "        self.iteration_count = 0\n",
    "\n",
    "    def get_draft_logits(\n",
    "        self,\n",
    "        input_ids: torch.Tensor,\n",
    "        attention_mask: torch.Tensor,\n",
    "        decoder_input_ids: torch.Tensor,\n",
    "        gamma: int\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"Get draft logits for gamma tokens\"\"\"\n",
    "        draft_tokens = []\n",
    "        draft_probs = []\n",
    "        current_decoder_ids = decoder_input_ids\n",
    "\n",
    "        # Generate gamma tokens from the draft model\n",
    "        for _ in range(gamma):\n",
    "            with torch.no_grad():\n",
    "                outputs = self.draft_model(\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    decoder_input_ids=current_decoder_ids,\n",
    "                    return_dict=True\n",
    "                )\n",
    "                logits = outputs.logits[:, -1, :]  # Get logits for last position\n",
    "                probs = F.softmax(logits, dim=-1)\n",
    "\n",
    "                # Sample token\n",
    "                token_id = torch.multinomial(probs, num_samples=1).squeeze(-1)\n",
    "                prob = probs.gather(-1, token_id.unsqueeze(-1)).squeeze(-1)\n",
    "\n",
    "                draft_tokens.append(token_id.item())\n",
    "                draft_probs.append(prob.item())\n",
    "\n",
    "                # Update decoder inputs for next iteration\n",
    "                current_decoder_ids = torch.cat(\n",
    "                    [current_decoder_ids, token_id.view(1, 1)],\n",
    "                    dim=1\n",
    "                )\n",
    "\n",
    "                if token_id.item() == self.tokenizer.eos_token_id:\n",
    "                    break\n",
    "\n",
    "        return draft_tokens, draft_probs, current_decoder_ids, outputs.logits\n",
    "\n",
    "    def get_target_probs(\n",
    "        self,\n",
    "        input_ids: torch.Tensor,\n",
    "        attention_mask: torch.Tensor,\n",
    "        decoder_input_ids: torch.Tensor,\n",
    "        draft_tokens: torch.Tensor\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"Get target probabilities for the draft tokens in parallel.\"\"\"\n",
    "        with torch.no_grad():\n",
    "            # Add draft tokens to decoder input\n",
    "            full_decoder_ids = torch.cat([decoder_input_ids, draft_tokens.unsqueeze(0)], dim=1)\n",
    "\n",
    "            outputs = self.target_model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                decoder_input_ids=full_decoder_ids,\n",
    "                return_dict=True\n",
    "            )\n",
    "\n",
    "            # Get probabilities for positions before each draft token\n",
    "            logits = outputs.logits[:, -(len(draft_tokens) + 1):-1, :]\n",
    "            target_probs = F.softmax(logits, dim=-1)\n",
    "\n",
    "            return target_probs.squeeze(0), outputs.logits\n",
    "        \n",
    "    def get_logits(self, model, input_ids, attention_mask):\n",
    "        return model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "        ).logits\n",
    "\n",
    "    def verify_tokens(\n",
    "        self,\n",
    "        target_probs: torch.Tensor,\n",
    "        draft_tokens: torch.Tensor,\n",
    "        draft_probs: torch.Tensor,\n",
    "    ) -> int:\n",
    "        \"\"\"Determine number of accepted tokens\"\"\"\n",
    "        # Get target probabilities for the draft tokens\n",
    "        target_probs_draft_tokens = target_probs.gather(\n",
    "            -1,\n",
    "            draft_tokens.unsqueeze(-1)\n",
    "        ).squeeze(-1)\n",
    "\n",
    "        # Calculate acceptance ratios\n",
    "        acceptance_ratios = target_probs_draft_tokens / draft_probs\n",
    "\n",
    "        # Sample uniform random numbers\n",
    "        random_nums = torch.rand_like(acceptance_ratios)\n",
    "\n",
    "        # Find number of accepted tokens\n",
    "        # Accept if random number < min(1, target_prob / draft_prob)\n",
    "        accepts = random_nums < torch.minimum(\n",
    "            torch.ones_like(acceptance_ratios),\n",
    "            acceptance_ratios\n",
    "        )\n",
    "\n",
    "        # Find first rejection\n",
    "        try:\n",
    "            n_accepted = torch.where(~accepts)[0][0].item()\n",
    "        except:\n",
    "            n_accepted = len(accepts)\n",
    "\n",
    "        return n_accepted\n",
    "\n",
    "        # accepted_tokens = []\n",
    "        # for i in range(len(draft_tokens)):\n",
    "        #     if target_probs[i] / draft_probs[i] > torch.rand(1).item():\n",
    "        #         accepted_tokens.append(draft_tokens[i])\n",
    "        #     else:\n",
    "        #         break # Stop if token is not accepted\n",
    "\n",
    "        # return len(accepted_tokens)\n",
    "    \n",
    "    # TODO: verify this, might need to do some window size thing\n",
    "    def update_draft_model(self):\n",
    "        \"\"\"Update draft model with the replay buffer.\"\"\"\n",
    "        if len(self.replay_buffer) == 0:\n",
    "            return\n",
    "\n",
    "        # Get draft tokens, draft and target probabilities from the replay buffer\n",
    "        # draft_tokens = torch.tensor([x[0] for x in self.replay_buffer], device=self.device)\n",
    "        # print(self.replay_buffer[0][0])\n",
    "        # draft_probs = self.replay_buffer[:, 0]\n",
    "        # target_probs = self.replay_buffer[:, 1]\n",
    "        draft_probs = torch.stack([x[0][0] for x in self.replay_buffer], dim=0)\n",
    "        target_probs = torch.stack([x[1][0] for x in self.replay_buffer], dim=0)\n",
    "\n",
    "        self.draft_model.train()\n",
    "\n",
    "        # for param in self.draft_model.parameters():\n",
    "        #     print(param.requires_grad)\n",
    "\n",
    "\n",
    "        # criterion = torch.nn.CrossEntropyLoss()\n",
    "        # print(draft_probs.shape, target_probs.shape)\n",
    "        loss = self.soft_cross_entropy(draft_probs, target_probs)\n",
    "        print(\"Loss grad_fn:\", loss.grad_fn)\n",
    "        print(\"Draft probs grad_fn:\", draft_probs.grad_fn)\n",
    "        print(\"Target probs grad_fn:\", target_probs.grad_fn)\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        self.draft_model.eval()\n",
    "\n",
    "        # Clear the replay buffer\n",
    "        self.replay_buffer = []\n",
    "\n",
    "    def soft_cross_entropy(self, predicts, targets, padding_mask=None):\n",
    "        predict_log_prob = torch.nn.functional.log_softmax(predicts, dim=-1)\n",
    "        targets_prob = torch.nn.functional.softmax(targets, dim=-1)\n",
    "        entropy = -targets_prob * predict_log_prob\n",
    "        # expand_mask = padding_mask.unsqueeze(-1).expand_as(entropy)\n",
    "        # entropy.masked_fill_(expand_mask, 0)\n",
    "        # mean_entropy = entropy.sum() / (~padding_mask).sum()\n",
    "        return entropy\n",
    "\n",
    "    def translate(\n",
    "        self,\n",
    "        source_text: str,\n",
    "        max_length: int = 128\n",
    "    ) -> str:\n",
    "        \"\"\"Translate source text using speculative decoding.\"\"\"\n",
    "        # Encode source text\n",
    "        encoder_inputs = self.tokenizer(\n",
    "            f\"translate English to German: {source_text}\",\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True\n",
    "        ).to(self.device)\n",
    "\n",
    "        # Initialize with start token\n",
    "        decoder_input_ids = torch.tensor([[self.tokenizer.pad_token_id]], device=self.device)\n",
    "\n",
    "        self.iteration_count = 0\n",
    "        self.replay_buffer = []\n",
    "\n",
    "        while decoder_input_ids.shape[1] < max_length:\n",
    "            self.temp_buffer = []\n",
    "\n",
    "            while decoder_input_ids.shape[1] < max_length:\n",
    "                # Get draft tokens autoregressively\n",
    "                # print(\"Encoder Inputs\", encoder_inputs.input_ids.shape)\n",
    "                draft_tokens, draft_probs, proposed_decoder_ids, draft_logits = self.get_draft_logits(\n",
    "                    encoder_inputs.input_ids,\n",
    "                    encoder_inputs.attention_mask,\n",
    "                    decoder_input_ids,\n",
    "                    self.gamma\n",
    "                )\n",
    "\n",
    "                draft_tokens = torch.tensor(draft_tokens, device=self.device)\n",
    "                draft_probs = torch.tensor(draft_probs, device=self.device)\n",
    "\n",
    "                if len(draft_tokens) == 0:\n",
    "                    break\n",
    "\n",
    "                # Get target probabilities in parallel\n",
    "                target_probs, target_logits = self.get_target_probs(\n",
    "                    encoder_inputs.input_ids,\n",
    "                    encoder_inputs.attention_mask,\n",
    "                    decoder_input_ids,\n",
    "                    draft_tokens\n",
    "                )\n",
    "                # print(draft_logits.shape, target_logits.shape)\n",
    "\n",
    "                # Verify tokens\n",
    "                n_accepted = self.verify_tokens(target_probs, draft_tokens, draft_probs)\n",
    "\n",
    "                # Accept verified tokens\n",
    "                if n_accepted > 0:\n",
    "                    decoder_input_ids = torch.cat([\n",
    "                        decoder_input_ids,\n",
    "                        draft_tokens[:n_accepted].unsqueeze(0)\n",
    "                    ], dim=1)\n",
    "\n",
    "                # # If rejection or no acceptance, sample one token from target\n",
    "                # if n_accepted < len(draft_tokens):\n",
    "                #     with torch.no_grad():\n",
    "                #         outputs = self.target_model(\n",
    "                #             input_ids=encoder_inputs.input_ids,\n",
    "                #             attention_mask=encoder_inputs.attention_mask,\n",
    "                #             decoder_input_ids=decoder_input_ids,\n",
    "                #             return_dict=True\n",
    "                #         )\n",
    "                #         logits = outputs.logits[:, -1, :]\n",
    "                #         probs = F.softmax(logits, dim=-1)\n",
    "                #         token_id = torch.multinomial(probs, num_samples=1)\n",
    "                #         decoder_input_ids = torch.cat([decoder_input_ids, token_id], dim=1)\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    # print(target_logits.shape, draft_logits.shape)\n",
    "                    probs = target_logits[:, -1, :] #- draft_logits[:, -1, :]\n",
    "                    probs = F.softmax(probs, dim=-1)\n",
    "                    token_id = torch.multinomial(probs, num_samples=1)\n",
    "                    # print(probs.shape, token_id.shape)\n",
    "                    decoder_input_ids = torch.cat([decoder_input_ids, token_id], dim=1)\n",
    "\n",
    "                # Check for end of sequence\n",
    "                if decoder_input_ids[0][-1].item() == self.tokenizer.eos_token_id:\n",
    "                    break\n",
    "                \n",
    "                # TODO: Update buffer with draft and target logits of the first rejected token, verify implementation\n",
    "                # rejected_tokens = draft_tokens[n_accepted]\n",
    "                if n_accepted < len(draft_tokens):\n",
    "                    # rejected_prob_draft = draft_logits[:, n_accepted, :]\n",
    "                    # rejected_prob_target = target_logits[:, n_accepted, :]\n",
    "\n",
    "                    self.temp_buffer.append((draft_logits[:, -1, :], target_logits[:, -1, :]))\n",
    "\n",
    "                # Check for end of sequence\n",
    "                if decoder_input_ids[0][-1].item() == self.tokenizer.eos_token_id:\n",
    "                    break\n",
    "\n",
    "            self.replay_buffer.extend(self.temp_buffer)\n",
    "            self.iteration_count += 1\n",
    "\n",
    "            if self.iteration_count % self.update_interval == 0:\n",
    "                self.update_draft_model()\n",
    "                self.iteration_count = 0\n",
    "\n",
    "        # Decode translation\n",
    "        translation = self.tokenizer.decode(\n",
    "            decoder_input_ids[0],\n",
    "            skip_special_tokens=True,\n",
    "            clean_up_tokenization_spaces=True\n",
    "        )\n",
    "        return translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize decoder\n",
    "online_decoder = OnlineSpeculativeDecoder()\n",
    "\n",
    "# Example translation\n",
    "source_text = \"In a world where technology evolves at an unprecedented pace, individuals and organizations must adapt quickly to the rapid advancements in artificial intelligence, machine learning, and automation, ensuring that ethical considerations, environmental sustainability, and equitable access to resources are prioritized to create a future that benefits all of humanity.\"\n",
    "\n",
    "# Time the translation\n",
    "start_time = time.time()\n",
    "translation = online_decoder.translate(source_text)\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"Source: {source_text}\")\n",
    "print(f\"Translation: {translation}\\n\")\n",
    "print(f\"Time taken: {end_time - start_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Online V2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OnlineSpeculativeDecoder:\n",
    "    def __init__(\n",
    "        self,\n",
    "        target_model_name = \"google-t5/t5-large\",\n",
    "        draft_model_name = \"google-t5/t5-small\",\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "        gamma = 4,\n",
    "        update_interval=2,  # Update draft model after every `update_interval` iterations\n",
    "        buffer_size_threshold=2,  # Buffer size threshold for updates\n",
    "        time_threshold=2,  # Time threshold (in seconds) for updates\n",
    "    ):\n",
    "        self.device = device\n",
    "        self.gamma = gamma\n",
    "        self.update_interval = update_interval\n",
    "        self.buffer_size_threshold = buffer_size_threshold\n",
    "        self.time_threshold = time_threshold\n",
    "        self.last_update_time = time.time()  # Track last update time\n",
    "\n",
    "        self.tokenizer = T5Tokenizer.from_pretrained(target_model_name)\n",
    "        self.target_model = T5ForConditionalGeneration.from_pretrained(target_model_name).to(device)\n",
    "        self.draft_model = T5ForConditionalGeneration.from_pretrained(draft_model_name).to(device)\n",
    "\n",
    "        self.target_model.eval()\n",
    "        self.draft_model.eval()\n",
    "\n",
    "        # Buffers for storing token proposals and updates\n",
    "        self.replay_buffer = []\n",
    "        self.temp_buffer = []  # Temporary buffer for a single request\n",
    "\n",
    "        # Counter for iteration tracking\n",
    "        self.iteration_count = 0\n",
    "\n",
    "    def get_draft_logits(\n",
    "        self,\n",
    "        input_ids: torch.Tensor,\n",
    "        attention_mask: torch.Tensor,\n",
    "        decoder_input_ids: torch.Tensor,\n",
    "        gamma: int\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"Get draft logits for gamma tokens\"\"\"\n",
    "        draft_tokens = []\n",
    "        draft_probs = []\n",
    "        current_decoder_ids = decoder_input_ids\n",
    "\n",
    "        # Generate gamma tokens from the draft model\n",
    "        for _ in range(gamma):\n",
    "            with torch.no_grad():\n",
    "                outputs = self.draft_model(\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    decoder_input_ids=current_decoder_ids,\n",
    "                    return_dict=True\n",
    "                )\n",
    "                logits = outputs.logits[:, -1, :]  # Get logits for last position\n",
    "                probs = F.softmax(logits, dim=-1)\n",
    "\n",
    "                # Sample token\n",
    "                token_id = torch.multinomial(probs, num_samples=1).squeeze(-1)\n",
    "                prob = probs.gather(-1, token_id.unsqueeze(-1)).squeeze(-1)\n",
    "\n",
    "                draft_tokens.append(token_id.item())\n",
    "                draft_probs.append(prob.item())\n",
    "\n",
    "                # Update decoder inputs for next iteration\n",
    "                current_decoder_ids = torch.cat(\n",
    "                    [current_decoder_ids, token_id.view(1, 1)],\n",
    "                    dim=1\n",
    "                )\n",
    "\n",
    "                if token_id.item() == self.tokenizer.eos_token_id:\n",
    "                    break\n",
    "\n",
    "        return draft_tokens, draft_probs, current_decoder_ids, outputs.logits\n",
    "\n",
    "    def get_target_probs(\n",
    "        self,\n",
    "        input_ids: torch.Tensor,\n",
    "        attention_mask: torch.Tensor,\n",
    "        decoder_input_ids: torch.Tensor,\n",
    "        draft_tokens: torch.Tensor\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"Get target probabilities for the draft tokens in parallel.\"\"\"\n",
    "        with torch.no_grad():\n",
    "            # Add draft tokens to decoder input\n",
    "            full_decoder_ids = torch.cat([decoder_input_ids, draft_tokens.unsqueeze(0)], dim=1)\n",
    "\n",
    "            outputs = self.target_model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                decoder_input_ids=full_decoder_ids,\n",
    "                return_dict=True\n",
    "            )\n",
    "\n",
    "            # Get probabilities for positions before each draft token\n",
    "            logits = outputs.logits[:, -(len(draft_tokens) + 1):-1, :]\n",
    "            target_probs = F.softmax(logits, dim=-1)\n",
    "\n",
    "            return target_probs.squeeze(0), outputs.logits\n",
    "        \n",
    "    def get_logits(self, model, input_ids, attention_mask, decoder_input_ids):\n",
    "        return model(\n",
    "            input_ids=input_ids,\n",
    "            decoder_input_ids=decoder_input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "        ).logits\n",
    "\n",
    "    def verify_tokens(\n",
    "        self,\n",
    "        target_probs: torch.Tensor,\n",
    "        draft_tokens: torch.Tensor,\n",
    "        draft_probs: torch.Tensor,\n",
    "    ) -> int:\n",
    "        \"\"\"Determine number of accepted tokens\"\"\"\n",
    "        # Get target probabilities for the draft tokens\n",
    "        target_probs_draft_tokens = target_probs.gather(\n",
    "            -1,\n",
    "            draft_tokens.unsqueeze(-1)\n",
    "        ).squeeze(-1)\n",
    "\n",
    "        # Calculate acceptance ratios\n",
    "        acceptance_ratios = target_probs_draft_tokens / draft_probs\n",
    "\n",
    "        # Sample uniform random numbers\n",
    "        random_nums = torch.rand_like(acceptance_ratios)\n",
    "\n",
    "        # Find number of accepted tokens\n",
    "        # Accept if random number < min(1, target_prob / draft_prob)\n",
    "        accepts = random_nums < torch.minimum(\n",
    "            torch.ones_like(acceptance_ratios),\n",
    "            acceptance_ratios\n",
    "        )\n",
    "\n",
    "        # Find first rejection\n",
    "        try:\n",
    "            n_accepted = torch.where(~accepts)[0][0].item()\n",
    "        except:\n",
    "            n_accepted = len(accepts)\n",
    "\n",
    "        return n_accepted\n",
    "\n",
    "        # accepted_tokens = []\n",
    "        # for i in range(len(draft_tokens)):\n",
    "        #     if target_probs[i] / draft_probs[i] > torch.rand(1).item():\n",
    "        #         accepted_tokens.append(draft_tokens[i])\n",
    "        #     else:\n",
    "        #         break # Stop if token is not accepted\n",
    "\n",
    "        # return len(accepted_tokens)\n",
    "    \n",
    "    # TODO: verify this, might need to do some window size thing\n",
    "    # def update_draft_model(self):\n",
    "    #     \"\"\"Update draft model with the replay buffer.\"\"\"\n",
    "    #     if len(self.replay_buffer) == 0:\n",
    "    #         return\n",
    "\n",
    "    #     # Get draft tokens, draft and target probabilities from the replay buffer\n",
    "    #     draft_tokens = torch.tensor([x[0] for x in self.replay_buffer], device=self.device)\n",
    "    #     target_logits = torch.tensor([x[1] for x in self.replay_buffer], device=self.device)\n",
    "\n",
    "    #     encoder_inputs = s\n",
    "    #     output = self.draft_model(\n",
    "    #         input_ids=encoder_inputs.input_ids,\n",
    "    #         attention_mask=encoder_inputs.attention_mask,\n",
    "    #         decoder_input_ids=decoder_input_ids,\n",
    "    #         return_dict=True\n",
    "    #     )\n",
    "\n",
    "    def pad_to_2d(self, tensor_list, pad_token_id, max_len=None):\n",
    "        if not isinstance(tensor_list[0], torch.Tensor):\n",
    "            tensor_list = [torch.tensor(t).reshape(1, -1) for t in tensor_list]\n",
    "        if max_len is None:\n",
    "            max_len = max([t.shape[-1] for t in tensor_list])\n",
    "        assert max_len > 0\n",
    "\n",
    "        # Pad each tensor to the max length and stack them to form a 2D tensor\n",
    "        result = torch.cat(\n",
    "            [\n",
    "                torch.nn.functional.pad(\n",
    "                    tensor, (0, max_len - tensor.shape[-1]),\n",
    "                    value=pad_token_id\n",
    "                )\n",
    "                for tensor in tensor_list\n",
    "            ],\n",
    "            dim=0\n",
    "        )\n",
    "        return result\n",
    "        \n",
    "\n",
    "    def soft_cross_entropy(self, predicts, targets, padding_mask=None):\n",
    "        predict_log_prob = torch.nn.functional.log_softmax(predicts, dim=-1)\n",
    "        targets_prob = torch.nn.functional.softmax(targets, dim=-1)\n",
    "        entropy = -targets_prob * predict_log_prob\n",
    "        expand_mask = padding_mask.unsqueeze(-1).expand_as(entropy)\n",
    "        entropy.masked_fill_(expand_mask, 0)\n",
    "        mean_entropy = entropy.sum() / (~padding_mask).sum()\n",
    "        return mean_entropy\n",
    "\n",
    "    def translate_dataset(\n",
    "        self,\n",
    "        sentences: List[str],\n",
    "        max_length: int = 128\n",
    "    ) -> List[str]:\n",
    "        \"\"\"Translate dataset using online speculative decoding.\"\"\"\n",
    "\n",
    "        self.iteration_count = 0\n",
    "        self.replay_buffer = []\n",
    "\n",
    "        translated_data = []\n",
    "\n",
    "        for source_text in sentences:\n",
    "            # Encode source text\n",
    "            encoder_inputs = self.tokenizer(\n",
    "                f\"translate English to German: {source_text}\",\n",
    "                return_tensors=\"pt\",\n",
    "                padding=True\n",
    "            ).to(self.device)\n",
    "\n",
    "            # Initialize with start token\n",
    "            decoder_input_ids = torch.tensor([[self.tokenizer.pad_token_id]], device=self.device)\n",
    "            self.temp_buffer = []\n",
    "\n",
    "            while decoder_input_ids.shape[1] < max_length:\n",
    "\n",
    "                # Check for end of sequence\n",
    "                if decoder_input_ids[0][-1].item() == self.tokenizer.eos_token_id:\n",
    "                    break\n",
    "\n",
    "                # Get draft tokens autoregressively\n",
    "                # print(\"Encoder Inputs\", encoder_inputs.input_ids.shape)\n",
    "                draft_tokens, draft_probs, proposed_decoder_ids, draft_logits = self.get_draft_logits(\n",
    "                    encoder_inputs.input_ids,\n",
    "                    encoder_inputs.attention_mask,\n",
    "                    decoder_input_ids,\n",
    "                    self.gamma\n",
    "                )\n",
    "\n",
    "                draft_tokens = torch.tensor(draft_tokens, device=self.device)\n",
    "                draft_probs = torch.tensor(draft_probs, device=self.device)\n",
    "\n",
    "                if len(draft_tokens) == 0:\n",
    "                    break\n",
    "\n",
    "                # Get target probabilities in parallel\n",
    "                target_probs, target_logits = self.get_target_probs(\n",
    "                    encoder_inputs.input_ids,\n",
    "                    encoder_inputs.attention_mask,\n",
    "                    decoder_input_ids,\n",
    "                    draft_tokens\n",
    "                )\n",
    "                # print(draft_logits.shape, target_logits.shape)\n",
    "\n",
    "                # Verify tokens\n",
    "                n_accepted = self.verify_tokens(target_probs, draft_tokens, draft_probs)\n",
    "\n",
    "                # Accept verified tokens\n",
    "                if n_accepted > 0:\n",
    "                    decoder_input_ids = torch.cat([\n",
    "                        decoder_input_ids,\n",
    "                        draft_tokens[:n_accepted].unsqueeze(0)\n",
    "                    ], dim=1)\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    # print(target_logits.shape, draft_logits.shape)\n",
    "                    probs = target_logits[:, -1, :] #- draft_logits[:, -1, :]\n",
    "                    probs = F.softmax(probs, dim=-1)\n",
    "                    token_id = torch.multinomial(probs, num_samples=1)\n",
    "                    # print(probs.shape, token_id.shape)\n",
    "                    decoder_input_ids = torch.cat([decoder_input_ids, token_id], dim=1)\n",
    "                \n",
    "                # rejected_tokens = draft_tokens[n_accepted]\n",
    "                if n_accepted < len(draft_tokens):\n",
    "\n",
    "                    self.temp_buffer.append((encoder_inputs, decoder_input_ids, target_logits, n_accepted))\n",
    "\n",
    "            self.replay_buffer.extend(self.temp_buffer)\n",
    "            self.iteration_count += 1\n",
    "\n",
    "            if self.iteration_count % self.update_interval == 0:\n",
    "                # self.update_draft_model()\n",
    "                self.draft_model.train()\n",
    "                \n",
    "                # finetune over collected tokens and logits\n",
    "                encoder_input_ids = self.pad_to_2d([x[0].input_ids for x in self.replay_buffer], 0)\n",
    "                encoder_attention_mask = torch.stack([x[0].attention_mask[0] for x in self.replay_buffer], dim=0)\n",
    "                decoder_input_ids = self.pad_to_2d([x[1] for x in self.replay_buffer], 0, 512)\n",
    "\n",
    "                print(encoder_input_ids.shape, encoder_attention_mask.shape, decoder_input_ids.shape)\n",
    "\n",
    "                target_logits = [x[2] for x in self.replay_buffer]\n",
    "                for i in range(len(target_logits)):\n",
    "                    temp = torch.zeros(1, 32128, device=self.device).repeat(512 - target_logits[i].shape[1], 1).unsqueeze(0)\n",
    "                    target_logits[i] = torch.cat([target_logits[i], temp], dim=1)\n",
    "\n",
    "                n_accepted_tokens = [x[3] for x in self.replay_buffer]\n",
    "\n",
    "                # CUDA out of memory error\n",
    "                draft_logits = self.get_logits(self.draft_model, encoder_input_ids, encoder_attention_mask, decoder_input_ids).float()\n",
    "\n",
    "                # need to get loss only using the wrong tokens\n",
    "                # TODO: check if we need to ignore the pad tokens in the mask\n",
    "                mask = torch.ones_like(decoder_input_ids, dtype=torch.bool)\n",
    "                for i in range(len(n_accepted_tokens)):\n",
    "                    mask[i, n_accepted_tokens[i]:] = False\n",
    "                loss = self.soft_cross_entropy(draft_logits, target_logits, mask)\n",
    "                loss.backward()\n",
    "\n",
    "                self.draft_model.eval()\n",
    "                self.replay_buffer = []\n",
    "                self.iteration_count = 0\n",
    "\n",
    "            # Decode translation\n",
    "            translation = self.tokenizer.decode(\n",
    "                decoder_input_ids[0],\n",
    "                skip_special_tokens=True,\n",
    "                clean_up_tokenization_spaces=True\n",
    "            )\n",
    "            translated_data.append(translation)\n",
    "        return translated_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize decoder\n",
    "online_decoder = OnlineSpeculativeDecoder()\n",
    "\n",
    "# Example translation\n",
    "source_text = \"In a world where technology evolves at an unprecedented pace, individuals and organizations must adapt quickly to the rapid advancements in artificial intelligence, machine learning, and automation, ensuring that ethical considerations, environmental sustainability, and equitable access to resources are prioritized to create a future that benefits all of humanity.\"\n",
    "sents = [source_text] * 10\n",
    "# Time the translation\n",
    "start_time = time.time()\n",
    "translation = online_decoder.translate_dataset(sents)\n",
    "end_time = time.time()\n",
    "\n",
    "for i, sent in enumerate(sents):\n",
    "    print(f\"Source: {sent}\")\n",
    "    print(f\"Translation: {translation[i]}\\n\")\n",
    "print(f\"Time taken: {end_time - start_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
