{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
        "from typing import List, Tuple, Optional\n",
        "import time"
      ],
      "metadata": {
        "id": "EdqllqRvz8YX"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SpeculativeDecoder:\n",
        "    def __init__(\n",
        "        self,\n",
        "        target_model_name = \"t5-large\",\n",
        "        draft_model_name = \"t5-small\",\n",
        "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
        "        gamma = 4\n",
        "    ):\n",
        "        self.device = device\n",
        "        self.gamma = gamma\n",
        "\n",
        "        self.tokenizer = T5Tokenizer.from_pretrained(target_model_name)\n",
        "        self.target_model = T5ForConditionalGeneration.from_pretrained(target_model_name).to(device)\n",
        "        self.draft_model = T5ForConditionalGeneration.from_pretrained(draft_model_name).to(device)\n",
        "\n",
        "        self.target_model.eval()\n",
        "        self.draft_model.eval()\n",
        "\n",
        "    def get_draft_logits(\n",
        "        self,\n",
        "        input_ids: torch.Tensor,\n",
        "        attention_mask: torch.Tensor,\n",
        "        decoder_input_ids: torch.Tensor,\n",
        "        gamma: int\n",
        "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "        \"\"\"Get draft logits for gamma tokens\"\"\"\n",
        "        draft_tokens = []\n",
        "        draft_probs = []\n",
        "        current_decoder_ids = decoder_input_ids\n",
        "\n",
        "        # Generate gamma tokens from the draft model\n",
        "        for _ in range(gamma):\n",
        "            with torch.no_grad():\n",
        "                outputs = self.draft_model(\n",
        "                    input_ids=input_ids,\n",
        "                    attention_mask=attention_mask,\n",
        "                    decoder_input_ids=current_decoder_ids,\n",
        "                    return_dict=True\n",
        "                )\n",
        "                logits = outputs.logits[:, -1, :]  # Get logits for last position\n",
        "                probs = F.softmax(logits, dim=-1)\n",
        "\n",
        "                # Sample token\n",
        "                token_id = torch.multinomial(probs, num_samples=1).squeeze(-1)\n",
        "                prob = probs.gather(-1, token_id.unsqueeze(-1)).squeeze(-1)\n",
        "\n",
        "                draft_tokens.append(token_id.item())\n",
        "                draft_probs.append(prob.item())\n",
        "\n",
        "                # Update decoder inputs for next iteration\n",
        "                current_decoder_ids = torch.cat(\n",
        "                    [current_decoder_ids, token_id.view(1, 1)],\n",
        "                    dim=1\n",
        "                )\n",
        "\n",
        "                if token_id.item() == self.tokenizer.eos_token_id:\n",
        "                    break\n",
        "\n",
        "        return draft_tokens, draft_probs, current_decoder_ids\n",
        "\n",
        "    def get_target_probs(\n",
        "        self,\n",
        "        input_ids: torch.Tensor,\n",
        "        attention_mask: torch.Tensor,\n",
        "        decoder_input_ids: torch.Tensor,\n",
        "        draft_tokens: torch.Tensor\n",
        "    ) -> torch.Tensor:\n",
        "        \"\"\"Get target probabilities for the draft tokens in parallel.\"\"\"\n",
        "        with torch.no_grad():\n",
        "            # Add draft tokens to decoder input\n",
        "            full_decoder_ids = torch.cat([decoder_input_ids, draft_tokens.unsqueeze(0)], dim=1)\n",
        "\n",
        "            outputs = self.target_model(\n",
        "                input_ids=input_ids,\n",
        "                attention_mask=attention_mask,\n",
        "                decoder_input_ids=full_decoder_ids,\n",
        "                return_dict=True\n",
        "            )\n",
        "\n",
        "            # Get probabilities for positions before each draft token\n",
        "            logits = outputs.logits[:, -(len(draft_tokens) + 1):-1, :]\n",
        "            target_probs = F.softmax(logits, dim=-1)\n",
        "\n",
        "            return target_probs.squeeze(0)\n",
        "\n",
        "    def verify_tokens(\n",
        "        self,\n",
        "        target_probs: torch.Tensor,\n",
        "        draft_tokens: torch.Tensor,\n",
        "        draft_probs: torch.Tensor,\n",
        "    ) -> int:\n",
        "        \"\"\"Determine number of accepted tokens\"\"\"\n",
        "        # Get target probabilities for the draft tokens\n",
        "        target_probs_draft_tokens = target_probs.gather(\n",
        "            -1,\n",
        "            draft_tokens.unsqueeze(-1)\n",
        "        ).squeeze(-1)\n",
        "\n",
        "        # Calculate acceptance ratios\n",
        "        acceptance_ratios = target_probs_draft_tokens / draft_probs\n",
        "\n",
        "        # Sample uniform random numbers\n",
        "        random_nums = torch.rand_like(acceptance_ratios)\n",
        "\n",
        "        # Find number of accepted tokens\n",
        "        # Accept if random number < min(1, target_prob / draft_prob)\n",
        "        accepts = random_nums < torch.minimum(\n",
        "            torch.ones_like(acceptance_ratios),\n",
        "            acceptance_ratios\n",
        "        )\n",
        "\n",
        "        # Find first rejection\n",
        "        try:\n",
        "            n_accepted = torch.where(~accepts)[0][0].item()\n",
        "        except:\n",
        "            n_accepted = len(accepts)\n",
        "\n",
        "        return n_accepted\n",
        "\n",
        "    def translate(\n",
        "        self,\n",
        "        source_text: str,\n",
        "        max_length: int = 128\n",
        "    ) -> str:\n",
        "        \"\"\"Translate source text using speculative decoding.\"\"\"\n",
        "        # Encode source text\n",
        "        encoder_inputs = self.tokenizer(\n",
        "            f\"translate English to German: {source_text}\",\n",
        "            return_tensors=\"pt\",\n",
        "            padding=True\n",
        "        ).to(self.device)\n",
        "\n",
        "        # Initialize with start token\n",
        "        decoder_input_ids = torch.tensor([[self.tokenizer.pad_token_id]], device=self.device)\n",
        "\n",
        "        while decoder_input_ids.shape[1] < max_length:\n",
        "            # Get draft tokens autoregressively\n",
        "            draft_tokens, draft_probs, proposed_decoder_ids = self.get_draft_logits(\n",
        "                encoder_inputs.input_ids,\n",
        "                encoder_inputs.attention_mask,\n",
        "                decoder_input_ids,\n",
        "                self.gamma\n",
        "            )\n",
        "\n",
        "            draft_tokens = torch.tensor(draft_tokens, device=self.device)\n",
        "            draft_probs = torch.tensor(draft_probs, device=self.device)\n",
        "\n",
        "            if len(draft_tokens) == 0:\n",
        "                break\n",
        "\n",
        "            # Get target probabilities in parallel\n",
        "            target_probs = self.get_target_probs(\n",
        "                encoder_inputs.input_ids,\n",
        "                encoder_inputs.attention_mask,\n",
        "                decoder_input_ids,\n",
        "                draft_tokens\n",
        "            )\n",
        "\n",
        "            # Verify tokens\n",
        "            n_accepted = self.verify_tokens(target_probs, draft_tokens, draft_probs)\n",
        "\n",
        "            # Accept verified tokens\n",
        "            if n_accepted > 0:\n",
        "                decoder_input_ids = torch.cat([\n",
        "                    decoder_input_ids,\n",
        "                    draft_tokens[:n_accepted].unsqueeze(0)\n",
        "                ], dim=1)\n",
        "\n",
        "            # If rejection or no acceptance, sample one token from target\n",
        "            if n_accepted < len(draft_tokens):\n",
        "                with torch.no_grad():\n",
        "                    outputs = self.target_model(\n",
        "                        input_ids=encoder_inputs.input_ids,\n",
        "                        attention_mask=encoder_inputs.attention_mask,\n",
        "                        decoder_input_ids=decoder_input_ids,\n",
        "                        return_dict=True\n",
        "                    )\n",
        "                    logits = outputs.logits[:, -1, :]\n",
        "                    probs = F.softmax(logits, dim=-1)\n",
        "                    token_id = torch.multinomial(probs, num_samples=1)\n",
        "                    decoder_input_ids = torch.cat([decoder_input_ids, token_id], dim=1)\n",
        "\n",
        "            # Check for end of sequence\n",
        "            if decoder_input_ids[0][-1].item() == self.tokenizer.eos_token_id:\n",
        "                break\n",
        "\n",
        "        # Decode translation\n",
        "        translation = self.tokenizer.decode(\n",
        "            decoder_input_ids[0],\n",
        "            skip_special_tokens=True,\n",
        "            clean_up_tokenization_spaces=True\n",
        "        )\n",
        "        return translation"
      ],
      "metadata": {
        "id": "v3kozZQNyBBQ"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize decoder\n",
        "decoder = SpeculativeDecoder()\n",
        "\n",
        "# Example translation\n",
        "source_text = \"In a world where technology evolves at an unprecedented pace, individuals and organizations must adapt quickly to the rapid advancements in artificial intelligence, machine learning, and automation, ensuring that ethical considerations, environmental sustainability, and equitable access to resources are prioritized to create a future that benefits all of humanity.\"\n",
        "\n",
        "# Time the translation\n",
        "start_time = time.time()\n",
        "translation = decoder.translate(source_text)\n",
        "end_time = time.time()\n",
        "\n",
        "print(f\"Source: {source_text}\")\n",
        "print(f\"Translation: {translation}\")\n",
        "print(f\"Time taken: {end_time - start_time:.2f} seconds\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4KPD_vbpgYMP",
        "outputId": "29bf4bee-629c-4589-ea13-9e6db91d9345"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Source: In a world where technology evolves at an unprecedented pace, individuals and organizations must adapt quickly to the rapid advancements in artificial intelligence, machine learning, and automation, ensuring that ethical considerations, environmental sustainability, and equitable access to resources are prioritized to create a future that benefits all of humanity.\n",
            "Translation: In einer Welt, in der die Technik in nie da gewesenem Tempo weiter wächst, müssen sich die Einzelnen und Organisationen rasch an die raschen Entwicklungen in den Bereichen künstlicher Intelligenz, Maschinelle Lernen und Automatisierung wenden, um zu sichern, dass ethische Erwägungen, ökologische Nachhaltigkeit und gerechter Zugang zu Ressourcen Priorität haben, um damals eine Zukunft zu schaffen, von der die Menschheit insgesamt profitieren kann.\n",
            "Time taken: 4.80 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class NormalDecoder:\n",
        "    def __init__(\n",
        "        self,\n",
        "        model_name: str = \"t5-large\",\n",
        "        device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
        "    ):\n",
        "        self.device = device\n",
        "\n",
        "        # Initialize tokenizer and model\n",
        "        self.tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
        "        self.model = T5ForConditionalGeneration.from_pretrained(model_name).to(device)\n",
        "        self.model.eval()\n",
        "\n",
        "    def get_logits(\n",
        "        self,\n",
        "        input_ids: torch.Tensor,\n",
        "        attention_mask: torch.Tensor,\n",
        "        decoder_input_ids: torch.Tensor\n",
        "    ) -> torch.Tensor:\n",
        "        \"\"\"Get logits from model for the last token.\"\"\"\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model(\n",
        "                input_ids=input_ids,\n",
        "                attention_mask=attention_mask,\n",
        "                decoder_input_ids=decoder_input_ids,\n",
        "                return_dict=True\n",
        "            )\n",
        "            return outputs.logits[:, -1, :]\n",
        "\n",
        "    def sample_token(self, logits: torch.Tensor, temperature: float = 1.0) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "        \"\"\"Sample a token from logits using temperature sampling.\"\"\"\n",
        "        if temperature == 0:\n",
        "            # Greedy sampling\n",
        "            token_id = torch.argmax(logits, dim=-1)\n",
        "            prob = torch.ones_like(token_id, dtype=torch.float)\n",
        "        else:\n",
        "            # Temperature sampling\n",
        "            probs = F.softmax(logits / temperature, dim=-1)\n",
        "            token_id = torch.multinomial(probs, num_samples=1).squeeze(-1)\n",
        "            prob = probs.gather(-1, token_id.unsqueeze(-1)).squeeze(-1)\n",
        "        return token_id, prob\n",
        "\n",
        "    def translate(\n",
        "        self,\n",
        "        source_text: str,\n",
        "        max_length: int = 128,\n",
        "        temperature: float = 0.7\n",
        "    ) -> str:\n",
        "        \"\"\"Translate source text using the normal T5 model.\"\"\"\n",
        "        # Encode source text\n",
        "        encoder_inputs = self.tokenizer(\n",
        "            f\"translate English to German: {source_text}\",\n",
        "            return_tensors=\"pt\",\n",
        "            padding=True\n",
        "        ).to(self.device)\n",
        "\n",
        "        # Initialize decoder input with start token\n",
        "        decoder_input_ids = torch.tensor([[self.tokenizer.pad_token_id]], device=self.device)\n",
        "\n",
        "        while decoder_input_ids.shape[1] < max_length:\n",
        "            # Generate logits for the next token\n",
        "            logits = self.get_logits(\n",
        "                encoder_inputs.input_ids,\n",
        "                encoder_inputs.attention_mask,\n",
        "                decoder_input_ids\n",
        "            )\n",
        "\n",
        "            # Sample a token\n",
        "            token_id, _ = self.sample_token(logits, temperature)\n",
        "\n",
        "            # Add token to the decoder input\n",
        "            decoder_input_ids = torch.cat(\n",
        "                [decoder_input_ids, token_id.view(1, 1)],\n",
        "                dim=1\n",
        "            )\n",
        "\n",
        "            # Break if end token is generated\n",
        "            if token_id.item() == self.tokenizer.eos_token_id:\n",
        "                break\n",
        "\n",
        "        # Decode and return translation\n",
        "        translation = self.tokenizer.decode(\n",
        "            decoder_input_ids[0],\n",
        "            skip_special_tokens=True,\n",
        "            clean_up_tokenization_spaces=True\n",
        "        )\n",
        "        return translation\n"
      ],
      "metadata": {
        "id": "CopfVI-mjdb4"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize decoder\n",
        "base_decoder = NormalDecoder()\n",
        "\n",
        "# Example translation\n",
        "source_text = \"In a world where technology evolves at an unprecedented pace, individuals and organizations must adapt quickly to the rapid advancements in artificial intelligence, machine learning, and automation, ensuring that ethical considerations, environmental sustainability, and equitable access to resources are prioritized to create a future that benefits all of humanity.\"\n",
        "\n",
        "# Time the translation\n",
        "start_time = time.time()\n",
        "translation = base_decoder.translate(source_text)\n",
        "end_time = time.time()\n",
        "\n",
        "print(f\"Source: {source_text}\")\n",
        "print(f\"Translation: {translation}\")\n",
        "print(f\"Time taken: {end_time - start_time:.2f} seconds\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nwtUoGDUjiSD",
        "outputId": "2a92f134-5a76-45b9-884b-51515d57fa4d"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Source: In a world where technology evolves at an unprecedented pace, individuals and organizations must adapt quickly to the rapid advancements in artificial intelligence, machine learning, and automation, ensuring that ethical considerations, environmental sustainability, and equitable access to resources are prioritized to create a future that benefits all of humanity.\n",
            "Translation: In einer Welt, in der sich die Technologie mit einem beispiellosen Tempo weiterentwickelt, müssen sich Individuen und Organisationen rasch an die schnellen Fortschritte in den Bereichen künstliche Intelligenz, maschinelles Lernen und Automatisierung anpassen, um sicherzustellen, dass ethische Überlegungen, Umweltverträglichkeit und gerechter Zugang zu Ressourcen Vorrang haben, um eine Zukunft zu schaffen, von der die gesamte Menschheit profitiert\n",
            "Time taken: 7.74 seconds\n"
          ]
        }
      ]
    }
  ]
}