{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "EdqllqRvz8YX"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home2/sarthak.chittawar/miniconda3/envs/ml/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "from typing import List, Tuple, Optional\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "v3kozZQNyBBQ"
   },
   "outputs": [],
   "source": [
    "class SpeculativeDecoder:\n",
    "    def __init__(\n",
    "        self,\n",
    "        target_model_name = \"google-t5/t5-large\",\n",
    "        draft_model_name = \"google-t5/t5-small\",\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "        gamma = 4\n",
    "    ):\n",
    "        self.device = device\n",
    "        self.gamma = gamma\n",
    "\n",
    "        self.tokenizer = T5Tokenizer.from_pretrained(target_model_name)\n",
    "        self.target_model = T5ForConditionalGeneration.from_pretrained(target_model_name).to(device)\n",
    "        self.draft_model = T5ForConditionalGeneration.from_pretrained(draft_model_name).to(device)\n",
    "\n",
    "        self.target_model.eval()\n",
    "        self.draft_model.eval()\n",
    "\n",
    "    def get_draft_logits(\n",
    "        self,\n",
    "        input_ids: torch.Tensor,\n",
    "        attention_mask: torch.Tensor,\n",
    "        decoder_input_ids: torch.Tensor,\n",
    "        gamma: int\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"Get draft logits for gamma tokens\"\"\"\n",
    "        draft_tokens = []\n",
    "        draft_probs = []\n",
    "        current_decoder_ids = decoder_input_ids\n",
    "\n",
    "        # Generate gamma tokens from the draft model\n",
    "        for _ in range(gamma):\n",
    "            with torch.no_grad():\n",
    "                outputs = self.draft_model(\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    decoder_input_ids=current_decoder_ids,\n",
    "                    return_dict=True\n",
    "                )\n",
    "                logits = outputs.logits[:, -1, :]  # Get logits for last position\n",
    "                probs = F.softmax(logits, dim=-1)\n",
    "\n",
    "                # Sample token\n",
    "                token_id = torch.multinomial(probs, num_samples=1).squeeze(-1)\n",
    "                prob = probs.gather(-1, token_id.unsqueeze(-1)).squeeze(-1)\n",
    "\n",
    "                draft_tokens.append(token_id.item())\n",
    "                draft_probs.append(prob.item())\n",
    "\n",
    "                # Update decoder inputs for next iteration\n",
    "                current_decoder_ids = torch.cat(\n",
    "                    [current_decoder_ids, token_id.view(1, 1)],\n",
    "                    dim=1\n",
    "                )\n",
    "\n",
    "                if token_id.item() == self.tokenizer.eos_token_id:\n",
    "                    break\n",
    "\n",
    "        return draft_tokens, draft_probs, current_decoder_ids, logits\n",
    "\n",
    "    def get_target_probs(\n",
    "        self,\n",
    "        input_ids: torch.Tensor,\n",
    "        attention_mask: torch.Tensor,\n",
    "        decoder_input_ids: torch.Tensor,\n",
    "        draft_tokens: torch.Tensor\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"Get target probabilities for the draft tokens in parallel.\"\"\"\n",
    "        with torch.no_grad():\n",
    "            # Add draft tokens to decoder input\n",
    "            full_decoder_ids = torch.cat([decoder_input_ids, draft_tokens.unsqueeze(0)], dim=1)\n",
    "\n",
    "            outputs = self.target_model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                decoder_input_ids=full_decoder_ids,\n",
    "                return_dict=True\n",
    "            )\n",
    "\n",
    "            # Get probabilities for positions before each draft token\n",
    "            logits = outputs.logits[:, -(len(draft_tokens) + 1):-1, :]\n",
    "            target_probs = F.softmax(logits, dim=-1)\n",
    "\n",
    "            return target_probs.squeeze(0), outputs.logits[:, -1, :]\n",
    "\n",
    "    def verify_tokens(\n",
    "        self,\n",
    "        target_probs: torch.Tensor,\n",
    "        draft_tokens: torch.Tensor,\n",
    "        draft_probs: torch.Tensor,\n",
    "    ) -> int:\n",
    "        \"\"\"Determine number of accepted tokens\"\"\"\n",
    "        # Get target probabilities for the draft tokens\n",
    "        target_probs_draft_tokens = target_probs.gather(\n",
    "            -1,\n",
    "            draft_tokens.unsqueeze(-1)\n",
    "        ).squeeze(-1)\n",
    "\n",
    "        # Calculate acceptance ratios\n",
    "        acceptance_ratios = target_probs_draft_tokens / draft_probs\n",
    "\n",
    "        # Sample uniform random numbers\n",
    "        random_nums = torch.rand_like(acceptance_ratios)\n",
    "\n",
    "        # Find number of accepted tokens\n",
    "        # Accept if random number < min(1, target_prob / draft_prob)\n",
    "        accepts = random_nums < torch.minimum(\n",
    "            torch.ones_like(acceptance_ratios),\n",
    "            acceptance_ratios\n",
    "        )\n",
    "\n",
    "        # Find first rejection\n",
    "        try:\n",
    "            n_accepted = torch.where(~accepts)[0][0].item()\n",
    "        except:\n",
    "            n_accepted = len(accepts)\n",
    "\n",
    "        return n_accepted\n",
    "\n",
    "    def translate(\n",
    "        self,\n",
    "        source_text: str,\n",
    "        max_length: int = 128\n",
    "    ) -> str:\n",
    "        \"\"\"Translate source text using speculative decoding.\"\"\"\n",
    "        # Encode source text\n",
    "        encoder_inputs = self.tokenizer(\n",
    "            f\"translate English to German: {source_text}\",\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True\n",
    "        ).to(self.device)\n",
    "\n",
    "        # Initialize with start token\n",
    "        decoder_input_ids = torch.tensor([[self.tokenizer.pad_token_id]], device=self.device)\n",
    "\n",
    "        while decoder_input_ids.shape[1] < max_length:\n",
    "            # Get draft tokens autoregressively\n",
    "            draft_tokens, draft_probs, proposed_decoder_ids, draft_logits = self.get_draft_logits(\n",
    "                encoder_inputs.input_ids,\n",
    "                encoder_inputs.attention_mask,\n",
    "                decoder_input_ids,\n",
    "                self.gamma\n",
    "            )\n",
    "\n",
    "            draft_tokens = torch.tensor(draft_tokens, device=self.device)\n",
    "            draft_probs = torch.tensor(draft_probs, device=self.device)\n",
    "\n",
    "            if len(draft_tokens) == 0:\n",
    "                break\n",
    "\n",
    "            # Get target probabilities in parallel\n",
    "            target_probs, target_logits = self.get_target_probs(\n",
    "                encoder_inputs.input_ids,\n",
    "                encoder_inputs.attention_mask,\n",
    "                decoder_input_ids,\n",
    "                draft_tokens\n",
    "            )\n",
    "\n",
    "            # Verify tokens\n",
    "            # print(target_probs.shape, draft_probs.shape)\n",
    "            n_accepted = self.verify_tokens(target_probs, draft_tokens, draft_probs)\n",
    "\n",
    "            # Accept verified tokens\n",
    "            if n_accepted > 0:\n",
    "                decoder_input_ids = torch.cat([\n",
    "                    decoder_input_ids,\n",
    "                    draft_tokens[:n_accepted].unsqueeze(0)\n",
    "                ], dim=1)\n",
    "\n",
    "            # If rejection or no acceptance, sample one token from target\n",
    "            # if n_accepted < len(draft_tokens):\n",
    "                # with torch.no_grad():\n",
    "                #     outputs = self.target_model(\n",
    "                #         input_ids=encoder_inputs.input_ids,\n",
    "                #         attention_mask=encoder_inputs.attention_mask,\n",
    "                #         decoder_input_ids=decoder_input_ids,\n",
    "                #         return_dict=True\n",
    "                #     )\n",
    "                #     logits = outputs.logits[:, -1, :]\n",
    "                #     probs = F.softmax(logits, dim=-1)\n",
    "                #     token_id = torch.multinomial(probs, num_samples=1)\n",
    "                #     decoder_input_ids = torch.cat([decoder_input_ids, token_id], dim=1)\n",
    "                # print(target_probs.shape, draft_probs.shape)\n",
    "                \n",
    "            with torch.no_grad():\n",
    "                # print(target_logits.shape, draft_logits.shape)\n",
    "                probs = target_logits #- draft_logits\n",
    "                probs = F.softmax(probs, dim=-1)\n",
    "                token_id = torch.multinomial(probs, num_samples=1)\n",
    "                # print(probs.shape, token_id.shape)\n",
    "                decoder_input_ids = torch.cat([decoder_input_ids, token_id], dim=1)\n",
    "\n",
    "            # Check for end of sequence\n",
    "            if decoder_input_ids[0][-1].item() == self.tokenizer.eos_token_id:\n",
    "                break\n",
    "\n",
    "        # Decode translation\n",
    "        translation = self.tokenizer.decode(\n",
    "            decoder_input_ids[0],\n",
    "            skip_special_tokens=True,\n",
    "            clean_up_tokenization_spaces=True\n",
    "        )\n",
    "        return translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4KPD_vbpgYMP",
    "outputId": "29bf4bee-629c-4589-ea13-9e6db91d9345"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source: In a world where technology evolves at an unprecedented pace, individuals and organizations must adapt quickly to the rapid advancements in artificial intelligence, machine learning, and automation, ensuring that ethical considerations, environmental sustainability, and equitable access to resources are prioritized to create a future that benefits all of humanity.\n",
      "Translation: In einer Welt, in der sich die Technik in einem beispiellosen Tempo müssen sich Menschen und Organisationen schnell den rasanten Entwicklungen im Bereich künstlicher Intelligenz, unds und Automatisierung ein anpassen,eischer, um dass Überlegungen um die den Menschen jeweiligen Lebensstandarddenblicksetzte und an  Ressourcen gerechter an\n",
      "\n",
      "Time taken: 3.98 seconds\n"
     ]
    }
   ],
   "source": [
    "# Initialize decoder\n",
    "decoder = SpeculativeDecoder()\n",
    "\n",
    "# Example translation\n",
    "source_text = \"In a world where technology evolves at an unprecedented pace, individuals and organizations must adapt quickly to the rapid advancements in artificial intelligence, machine learning, and automation, ensuring that ethical considerations, environmental sustainability, and equitable access to resources are prioritized to create a future that benefits all of humanity.\"\n",
    "\n",
    "# Time the translation\n",
    "start_time = time.time()\n",
    "translation = decoder.translate(source_text)\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"Source: {source_text}\")\n",
    "print(f\"Translation: {translation}\\n\")\n",
    "print(f\"Time taken: {end_time - start_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "CopfVI-mjdb4"
   },
   "outputs": [],
   "source": [
    "class NormalDecoder:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name: str = \"google-t5/t5-large\",\n",
    "        device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "    ):\n",
    "        self.device = device\n",
    "\n",
    "        # Initialize tokenizer and model\n",
    "        self.tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "        self.model = T5ForConditionalGeneration.from_pretrained(model_name).to(device)\n",
    "        self.model.eval()\n",
    "\n",
    "    def get_logits(\n",
    "        self,\n",
    "        input_ids: torch.Tensor,\n",
    "        attention_mask: torch.Tensor,\n",
    "        decoder_input_ids: torch.Tensor\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"Get logits from model for the last token.\"\"\"\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                decoder_input_ids=decoder_input_ids,\n",
    "                return_dict=True\n",
    "            )\n",
    "            return outputs.logits[:, -1, :]\n",
    "\n",
    "    def sample_token(self, logits: torch.Tensor, temperature: float = 1.0) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"Sample a token from logits using temperature sampling.\"\"\"\n",
    "        if temperature == 0:\n",
    "            # Greedy sampling\n",
    "            token_id = torch.argmax(logits, dim=-1)\n",
    "            prob = torch.ones_like(token_id, dtype=torch.float)\n",
    "        else:\n",
    "            # Temperature sampling\n",
    "            probs = F.softmax(logits / temperature, dim=-1)\n",
    "            token_id = torch.multinomial(probs, num_samples=1).squeeze(-1)\n",
    "            prob = probs.gather(-1, token_id.unsqueeze(-1)).squeeze(-1)\n",
    "        return token_id, prob\n",
    "\n",
    "    def translate(\n",
    "        self,\n",
    "        source_text: str,\n",
    "        max_length: int = 128,\n",
    "        temperature: float = 0.7\n",
    "    ) -> str:\n",
    "        \"\"\"Translate source text using the normal T5 model.\"\"\"\n",
    "        # Encode source text\n",
    "        encoder_inputs = self.tokenizer(\n",
    "            f\"translate English to German: {source_text}\",\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True\n",
    "        ).to(self.device)\n",
    "\n",
    "        # Initialize decoder input with start token\n",
    "        decoder_input_ids = torch.tensor([[self.tokenizer.pad_token_id]], device=self.device)\n",
    "\n",
    "        while decoder_input_ids.shape[1] < max_length:\n",
    "            # Generate logits for the next token\n",
    "            logits = self.get_logits(\n",
    "                encoder_inputs.input_ids,\n",
    "                encoder_inputs.attention_mask,\n",
    "                decoder_input_ids\n",
    "            )\n",
    "\n",
    "            # Sample a token\n",
    "            token_id, _ = self.sample_token(logits, temperature)\n",
    "\n",
    "            # Add token to the decoder input\n",
    "            decoder_input_ids = torch.cat(\n",
    "                [decoder_input_ids, token_id.view(1, 1)],\n",
    "                dim=1\n",
    "            )\n",
    "\n",
    "            # Break if end token is generated\n",
    "            if token_id.item() == self.tokenizer.eos_token_id:\n",
    "                break\n",
    "\n",
    "        # Decode and return translation\n",
    "        translation = self.tokenizer.decode(\n",
    "            decoder_input_ids[0],\n",
    "            skip_special_tokens=True,\n",
    "            clean_up_tokenization_spaces=True\n",
    "        )\n",
    "        return translation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nwtUoGDUjiSD",
    "outputId": "2a92f134-5a76-45b9-884b-51515d57fa4d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source: In a world where technology evolves at an unprecedented pace, individuals and organizations must adapt quickly to the rapid advancements in artificial intelligence, machine learning, and automation, ensuring that ethical considerations, environmental sustainability, and equitable access to resources are prioritized to create a future that benefits all of humanity.\n",
      "Translation: In einer Welt, in der die Technologie in beispielloser Geschwindigkeit eine Entwicklung durchläuft, müssen sich Menschen und Organisationen schnell auf die rapiden Fortschritte bei der künstlichen Intelligenz, beim maschinellen Lernen und bei der Automatisierung einstellen, indem ethische Überlegungen, ökologische Nachhaltigkeit und gleichberechtigter Zugang zu Ressourcen Priorität erhalten, um eine Zukunft zu schaffen, von der die gesamte Menschheit profitiert.\n",
      "\n",
      "Time taken: 5.54 seconds\n"
     ]
    }
   ],
   "source": [
    "# Initialize decoder\n",
    "base_decoder = NormalDecoder()\n",
    "\n",
    "# Example translation\n",
    "source_text = \"In a world where technology evolves at an unprecedented pace, individuals and organizations must adapt quickly to the rapid advancements in artificial intelligence, machine learning, and automation, ensuring that ethical considerations, environmental sustainability, and equitable access to resources are prioritized to create a future that benefits all of humanity.\"\n",
    "\n",
    "# Time the translation\n",
    "start_time = time.time()\n",
    "translation = base_decoder.translate(source_text)\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"Source: {source_text}\")\n",
    "print(f\"Translation: {translation}\\n\")\n",
    "print(f\"Time taken: {end_time - start_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import jensenshannon as JSD\n",
    "\n",
    "class OnlineSpeculativeDecoder:\n",
    "    def __init__(\n",
    "        self,\n",
    "        target_model_name = \"google-t5/t5-large\",\n",
    "        draft_model_name = \"google-t5/t5-small\",\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "        gamma = 4,\n",
    "        update_interval=2,  # Update draft model after every `update_interval` iterations\n",
    "        buffer_size_threshold=2,  # Buffer size threshold for updates\n",
    "        time_threshold=2,  # Time threshold (in seconds) for updates\n",
    "    ):\n",
    "        self.device = device\n",
    "        self.gamma = gamma\n",
    "        self.update_interval = update_interval\n",
    "        self.buffer_size_threshold = buffer_size_threshold\n",
    "        self.time_threshold = time_threshold\n",
    "        self.last_update_time = time.time()  # Track last update time\n",
    "\n",
    "        self.tokenizer = T5Tokenizer.from_pretrained(target_model_name)\n",
    "        self.target_model = T5ForConditionalGeneration.from_pretrained(target_model_name).to(device)\n",
    "        self.draft_model = T5ForConditionalGeneration.from_pretrained(draft_model_name).to(device)\n",
    "\n",
    "        self.target_model.eval()\n",
    "        self.draft_model.eval()\n",
    "\n",
    "        # Buffers for storing token proposals and updates\n",
    "        self.replay_buffer = []\n",
    "        self.temp_buffer = []  # Temporary buffer for a single request\n",
    "\n",
    "        # Counter for iteration tracking\n",
    "        self.iteration_count = 0\n",
    "\n",
    "    def get_draft_logits(\n",
    "        self,\n",
    "        input_ids: torch.Tensor,\n",
    "        attention_mask: torch.Tensor,\n",
    "        decoder_input_ids: torch.Tensor,\n",
    "        gamma: int\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"Get draft logits for gamma tokens\"\"\"\n",
    "        draft_tokens = []\n",
    "        draft_probs = []\n",
    "        current_decoder_ids = decoder_input_ids\n",
    "\n",
    "        # Generate gamma tokens from the draft model\n",
    "        for _ in range(gamma):\n",
    "            with torch.no_grad():\n",
    "                outputs = self.draft_model(\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    decoder_input_ids=current_decoder_ids,\n",
    "                    return_dict=True\n",
    "                )\n",
    "                logits = outputs.logits[:, -1, :]  # Get logits for last position\n",
    "                probs = F.softmax(logits, dim=-1)\n",
    "\n",
    "                # Sample token\n",
    "                token_id = torch.multinomial(probs, num_samples=1).squeeze(-1)\n",
    "                prob = probs.gather(-1, token_id.unsqueeze(-1)).squeeze(-1)\n",
    "\n",
    "                draft_tokens.append(token_id.item())\n",
    "                draft_probs.append(prob.item())\n",
    "\n",
    "                # Update decoder inputs for next iteration\n",
    "                current_decoder_ids = torch.cat(\n",
    "                    [current_decoder_ids, token_id.view(1, 1)],\n",
    "                    dim=1\n",
    "                )\n",
    "\n",
    "                if token_id.item() == self.tokenizer.eos_token_id:\n",
    "                    break\n",
    "\n",
    "        return draft_tokens, draft_probs, current_decoder_ids, outputs.logits\n",
    "\n",
    "    def get_target_probs(\n",
    "        self,\n",
    "        input_ids: torch.Tensor,\n",
    "        attention_mask: torch.Tensor,\n",
    "        decoder_input_ids: torch.Tensor,\n",
    "        draft_tokens: torch.Tensor\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"Get target probabilities for the draft tokens in parallel.\"\"\"\n",
    "        with torch.no_grad():\n",
    "            # Add draft tokens to decoder input\n",
    "            full_decoder_ids = torch.cat([decoder_input_ids, draft_tokens.unsqueeze(0)], dim=1)\n",
    "\n",
    "            outputs = self.target_model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                decoder_input_ids=full_decoder_ids,\n",
    "                return_dict=True\n",
    "            )\n",
    "\n",
    "            # Get probabilities for positions before each draft token\n",
    "            logits = outputs.logits[:, -(len(draft_tokens) + 1):-1, :]\n",
    "            target_probs = F.softmax(logits, dim=-1)\n",
    "\n",
    "            return target_probs.squeeze(0), outputs.logits\n",
    "\n",
    "    def verify_tokens(\n",
    "        self,\n",
    "        target_probs: torch.Tensor,\n",
    "        draft_tokens: torch.Tensor,\n",
    "        draft_probs: torch.Tensor,\n",
    "    ) -> int:\n",
    "        \"\"\"Determine number of accepted tokens\"\"\"\n",
    "        # Get target probabilities for the draft tokens\n",
    "        target_probs_draft_tokens = target_probs.gather(\n",
    "            -1,\n",
    "            draft_tokens.unsqueeze(-1)\n",
    "        ).squeeze(-1)\n",
    "\n",
    "        # Calculate acceptance ratios\n",
    "        acceptance_ratios = target_probs_draft_tokens / draft_probs\n",
    "\n",
    "        # Sample uniform random numbers\n",
    "        random_nums = torch.rand_like(acceptance_ratios)\n",
    "\n",
    "        # Find number of accepted tokens\n",
    "        # Accept if random number < min(1, target_prob / draft_prob)\n",
    "        accepts = random_nums < torch.minimum(\n",
    "            torch.ones_like(acceptance_ratios),\n",
    "            acceptance_ratios\n",
    "        )\n",
    "\n",
    "        # Find first rejection\n",
    "        try:\n",
    "            n_accepted = torch.where(~accepts)[0][0].item()\n",
    "        except:\n",
    "            n_accepted = len(accepts)\n",
    "\n",
    "        return n_accepted\n",
    "\n",
    "        # accepted_tokens = []\n",
    "        # for i in range(len(draft_tokens)):\n",
    "        #     if target_probs[i] / draft_probs[i] > torch.rand(1).item():\n",
    "        #         accepted_tokens.append(draft_tokens[i])\n",
    "        #     else:\n",
    "        #         break # Stop if token is not accepted\n",
    "\n",
    "        # return len(accepted_tokens)\n",
    "    \n",
    "    # TODO: verify this, might need to do some window size thing\n",
    "    def update_draft_model(self):\n",
    "        \"\"\"Update draft model with the replay buffer.\"\"\"\n",
    "        if len(self.replay_buffer) == 0:\n",
    "            return\n",
    "\n",
    "        # Get draft tokens, draft and target probabilities from the replay buffer\n",
    "        # draft_tokens = torch.tensor([x[0] for x in self.replay_buffer], device=self.device)\n",
    "        # print(self.replay_buffer[0][0])\n",
    "        # draft_probs = self.replay_buffer[:, 0]\n",
    "        # target_probs = self.replay_buffer[:, 1]\n",
    "        draft_probs = torch.stack([x[0][0] for x in self.replay_buffer], dim=0)\n",
    "        target_probs = torch.stack([x[1][0] for x in self.replay_buffer], dim=0)\n",
    "\n",
    "        # Calculate loss and update draft model\n",
    "        print(\"Updating\")\n",
    "        # loss = JSD between draft and target probs\n",
    "        loss = JSD(\n",
    "            F.softmax(draft_probs.cpu(), dim=-1),\n",
    "            F.softmax(target_probs.cpu(), dim=-1)\n",
    "        )\n",
    "\n",
    "        self.draft_model.zero_grad()\n",
    "        \n",
    "        # TODO: figure out how tf to finetune\n",
    "        loss.backward()\n",
    "        self.draft_model.optimizer.step()\n",
    "\n",
    "        # Reset the replay buffer\n",
    "        self.replay_buffer = []\n",
    "\n",
    "    def translate(\n",
    "        self,\n",
    "        source_text: str,\n",
    "        max_length: int = 128\n",
    "    ) -> str:\n",
    "        \"\"\"Translate source text using speculative decoding.\"\"\"\n",
    "        # Encode source text\n",
    "        encoder_inputs = self.tokenizer(\n",
    "            f\"translate English to German: {source_text}\",\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True\n",
    "        ).to(self.device)\n",
    "\n",
    "        # Initialize with start token\n",
    "        decoder_input_ids = torch.tensor([[self.tokenizer.pad_token_id]], device=self.device)\n",
    "\n",
    "        self.iteration_count = 0\n",
    "        self.replay_buffer = []\n",
    "\n",
    "        while decoder_input_ids.shape[1] < max_length:\n",
    "            self.temp_buffer = []\n",
    "\n",
    "            while decoder_input_ids.shape[1] < max_length:\n",
    "                # Get draft tokens autoregressively\n",
    "                draft_tokens, draft_probs, proposed_decoder_ids, draft_logits = self.get_draft_logits(\n",
    "                    encoder_inputs.input_ids,\n",
    "                    encoder_inputs.attention_mask,\n",
    "                    decoder_input_ids,\n",
    "                    self.gamma\n",
    "                )\n",
    "\n",
    "                draft_tokens = torch.tensor(draft_tokens, device=self.device)\n",
    "                draft_probs = torch.tensor(draft_probs, device=self.device)\n",
    "\n",
    "                if len(draft_tokens) == 0:\n",
    "                    break\n",
    "\n",
    "                # Get target probabilities in parallel\n",
    "                target_probs, target_logits = self.get_target_probs(\n",
    "                    encoder_inputs.input_ids,\n",
    "                    encoder_inputs.attention_mask,\n",
    "                    decoder_input_ids,\n",
    "                    draft_tokens\n",
    "                )\n",
    "\n",
    "                # Verify tokens\n",
    "                n_accepted = self.verify_tokens(target_probs, draft_tokens, draft_probs)\n",
    "\n",
    "                # Accept verified tokens\n",
    "                if n_accepted > 0:\n",
    "                    decoder_input_ids = torch.cat([\n",
    "                        decoder_input_ids,\n",
    "                        draft_tokens[:n_accepted].unsqueeze(0)\n",
    "                    ], dim=1)\n",
    "\n",
    "                # # If rejection or no acceptance, sample one token from target\n",
    "                # if n_accepted < len(draft_tokens):\n",
    "                #     with torch.no_grad():\n",
    "                #         outputs = self.target_model(\n",
    "                #             input_ids=encoder_inputs.input_ids,\n",
    "                #             attention_mask=encoder_inputs.attention_mask,\n",
    "                #             decoder_input_ids=decoder_input_ids,\n",
    "                #             return_dict=True\n",
    "                #         )\n",
    "                #         logits = outputs.logits[:, -1, :]\n",
    "                #         probs = F.softmax(logits, dim=-1)\n",
    "                #         token_id = torch.multinomial(probs, num_samples=1)\n",
    "                #         decoder_input_ids = torch.cat([decoder_input_ids, token_id], dim=1)\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    # print(target_logits.shape, draft_logits.shape)\n",
    "                    probs = target_logits[:, -1, :] #- draft_logits[:, -1, :]\n",
    "                    probs = F.softmax(probs, dim=-1)\n",
    "                    token_id = torch.multinomial(probs, num_samples=1)\n",
    "                    # print(probs.shape, token_id.shape)\n",
    "                    decoder_input_ids = torch.cat([decoder_input_ids, token_id], dim=1)\n",
    "\n",
    "                # Check for end of sequence\n",
    "                if decoder_input_ids[0][-1].item() == self.tokenizer.eos_token_id:\n",
    "                    break\n",
    "                \n",
    "                # TODO: Update buffer with draft and target logits of the first rejected token, verify implementation\n",
    "                # rejected_tokens = draft_tokens[n_accepted]\n",
    "                if n_accepted < len(draft_tokens):\n",
    "                    rejected_prob_draft = draft_logits[:, n_accepted, :]\n",
    "                    rejected_prob_target = target_logits[:, n_accepted, :]\n",
    "\n",
    "                    self.temp_buffer.append((rejected_prob_draft, rejected_prob_target))\n",
    "\n",
    "                # Check for end of sequence\n",
    "                if decoder_input_ids[0][-1].item() == self.tokenizer.eos_token_id:\n",
    "                    break\n",
    "\n",
    "            self.replay_buffer.extend(self.temp_buffer)\n",
    "            self.iteration_count += 1\n",
    "\n",
    "            if self.iteration_count % self.update_interval == 0:\n",
    "                self.update_draft_model()\n",
    "                self.iteration_count = 0\n",
    "\n",
    "        # Decode translation\n",
    "        translation = self.tokenizer.decode(\n",
    "            decoder_input_ids[0],\n",
    "            skip_special_tokens=True,\n",
    "            clean_up_tokenization_spaces=True\n",
    "        )\n",
    "        return translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'backward'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Time the translation\u001b[39;00m\n\u001b[1;32m      8\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m----> 9\u001b[0m translation \u001b[38;5;241m=\u001b[39m \u001b[43monline_decoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranslate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource_text\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSource: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msource_text\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[8], line 271\u001b[0m, in \u001b[0;36mOnlineSpeculativeDecoder.translate\u001b[0;34m(self, source_text, max_length)\u001b[0m\n\u001b[1;32m    268\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miteration_count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    270\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miteration_count \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate_interval \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 271\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate_draft_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    272\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miteration_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    274\u001b[0m \u001b[38;5;66;03m# Decode translation\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[8], line 168\u001b[0m, in \u001b[0;36mOnlineSpeculativeDecoder.update_draft_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    162\u001b[0m loss \u001b[38;5;241m=\u001b[39m JSD(\n\u001b[1;32m    163\u001b[0m     F\u001b[38;5;241m.\u001b[39msoftmax(draft_probs\u001b[38;5;241m.\u001b[39mcpu(), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m),\n\u001b[1;32m    164\u001b[0m     F\u001b[38;5;241m.\u001b[39msoftmax(target_probs\u001b[38;5;241m.\u001b[39mcpu(), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    165\u001b[0m )\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdraft_model\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m--> 168\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m()\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdraft_model\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m    171\u001b[0m \u001b[38;5;66;03m# Reset the replay buffer\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'backward'"
     ]
    }
   ],
   "source": [
    "# Initialize decoder\n",
    "online_decoder = OnlineSpeculativeDecoder()\n",
    "\n",
    "# Example translation\n",
    "source_text = \"In a world where technology evolves at an unprecedented pace, individuals and organizations must adapt quickly to the rapid advancements in artificial intelligence, machine learning, and automation, ensuring that ethical considerations, environmental sustainability, and equitable access to resources are prioritized to create a future that benefits all of humanity.\"\n",
    "\n",
    "# Time the translation\n",
    "start_time = time.time()\n",
    "translation = online_decoder.translate(source_text)\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"Source: {source_text}\")\n",
    "print(f\"Translation: {translation}\\n\")\n",
    "print(f\"Time taken: {end_time - start_time:.2f} seconds\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
