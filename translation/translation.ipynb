{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cMxcc2rM1OgB"
      },
      "outputs": [],
      "source": [
        "!pip install transformers datasets torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "eJlTEiUL1uao"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "import torch\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "br-Og7xn2XeS",
        "outputId": "a6951ea1-f5d8-4758-91cf-44e521dd6592"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "47i8AUmJ1eTW"
      },
      "outputs": [],
      "source": [
        "dataset = load_dataset('wmt/wmt19', 'de-en', split='validation')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rqeLidWx8Vn7",
        "outputId": "790c4ab7-9ccf-499f-80cd-b1c67023d025"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Munich 1856: Four maps that will change your view of the city\n"
          ]
        }
      ],
      "source": [
        "print(dataset['translation'][0]['en'])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def translate_batch(input_ids, model):\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(input_ids)\n",
        "    return tokenizer.batch_decode(outputs, skip_special_tokens=True)"
      ],
      "metadata": {
        "id": "Inu-hpsVymEM"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_inference_time(model):\n",
        "    batch_size = 16\n",
        "    total_inference_time = 0\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    translations = []\n",
        "\n",
        "    for i in range(0, len(dataset['translation']), batch_size):\n",
        "        batch_input_ids = inputs.input_ids[i:i + batch_size]\n",
        "        translations.extend(translate_batch(batch_input_ids, model))\n",
        "\n",
        "    end_time = time.time()\n",
        "\n",
        "    total_inference_time = end_time - start_time\n",
        "\n",
        "    return total_inference_time, translations"
      ],
      "metadata": {
        "id": "0LObvBTNyzqR"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "3MJJqci79Lzy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "25309768-3ed3-47a9-c688-300f1719f72a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1258: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total inference time for large model: 205.51281762123108 seconds\n"
          ]
        }
      ],
      "source": [
        "large_model_name = 't5-large'\n",
        "tokenizer = T5Tokenizer.from_pretrained(large_model_name)\n",
        "large_model = T5ForConditionalGeneration.from_pretrained(large_model_name)\n",
        "large_model = large_model.to(device)\n",
        "\n",
        "inputs = tokenizer([\"translate English to German: \" + entry['en'] for entry in dataset['translation']],\n",
        "                   return_tensors=\"pt\", padding=True, truncation=True)\n",
        "inputs = inputs.to(device)\n",
        "\n",
        "total_inference_time_large, translations_large = calculate_inference_time(large_model)\n",
        "print(f\"Total inference time for large model: {total_inference_time_large} seconds\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X8OqbBK73AYX",
        "outputId": "695ae23c-2387-4945-9530-dadc508a4ca4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original: Munich 1856: Four maps that will change your view of the city => Translated: München 1856: Vier Karten, die Ihre Sicht auf die Stadt verändern\n",
            "Original: A mental asylum, where today young people are said to meet. => Translated: Ein psychisches Asyl, wo sich heute junge Menschen treffen sollen.\n",
            "Original: A crypt chapel, where they are now digging tunnels for the S-Bahn. => Translated: Eine Krypta, wo jetzt Tunnel für die S-Bahn gegraben\n",
            "Original: Allotment holders cultivate the soil of former farmers. => Translated: Die Besitzer von Feldern bewirtschaften den Boden ehemaliger Bauern.\n",
            "Original: The oldest official map of Munich brings captivating stories to light. => Translated: Die älteste offizielle Karte von München erfährt spannende Geschichten.\n",
            "Original: It is annoying when geographical maps are not up-to-date. => Translated: Es ist ärgerlich, wenn die geographischen Karten nicht auf dem neuesten Stand\n",
            "Original: Anyone who has ever got worked up because the car's sat-nav is showing a green field instead of a bypass knows that. => Translated: Jeder, der sich jemals darüber aufgeregt hat, dass der Navigationssystem\n",
            "Original: The historical maps of the digital BayernAtlas, an offering from the State Government's Geoportal Bayern, are anything but up-to-date – and yet it is precisely for this reason that they are so informative. => Translated: Die historischen Karten des digitalen BayernAtlas, ein Angebot des Landeshauptstadt\n",
            "Original: Especially when one compares them with current online maps. => Translated: Besonders wenn man sie mit aktuellen Online-Karten vergleicht.\n",
            "Original: Then it becomes clear how the towns and municipalities in the distribution area of Munich's Merkur newspaper have changed since the 19th century. => Translated: Dann wird deutlich, wie sich die Städte und Gemeinden im Vertriebsgebiet der Münchner\n"
          ]
        }
      ],
      "source": [
        "for i, (original, translated) in enumerate(zip([entry['en'] for entry in dataset['translation'][:10]], translations_large[:10])):\n",
        "    print(f\"Original: {original} => Translated: {translated}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "small_model_name = 't5-small'\n",
        "tokenizer = T5Tokenizer.from_pretrained(small_model_name)\n",
        "small_model = T5ForConditionalGeneration.from_pretrained(small_model_name)\n",
        "small_model = small_model.to(device)\n",
        "\n",
        "inputs = tokenizer([\"translate English to German: \" + entry['en'] for entry in dataset['translation']],\n",
        "                   return_tensors=\"pt\", padding=True, truncation=True)\n",
        "inputs = inputs.to(device)\n",
        "\n",
        "total_inference_time_small, translations_small = calculate_inference_time(small_model)\n",
        "print(f\"Total inference time for small model: {total_inference_time_small} seconds\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pn74LjuyyW9O",
        "outputId": "4304224e-4a21-4916-a20c-7605def9d1a5"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total inference time for small model: 49.66063165664673 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i, (original, translated) in enumerate(zip([entry['en'] for entry in dataset['translation'][:10]], translations_small[:10])):\n",
        "    print(f\"Original: {original} => Translated: {translated}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S-O5Jy4IyqSp",
        "outputId": "7bafa040-339f-44b3-bca4-4eb85570a694"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original: Munich 1856: Four maps that will change your view of the city => Translated: München 1856: Vier Karten, die Ihren Blick auf die Stadt verändern werden\n",
            "Original: A mental asylum, where today young people are said to meet. => Translated: Ein geistiges Asyl, wo sich heute junge Menschen treffen sollen.\n",
            "Original: A crypt chapel, where they are now digging tunnels for the S-Bahn. => Translated: Eine Kryptkapelle, wo sie nun Tunnel für die S-Bahn gra\n",
            "Original: Allotment holders cultivate the soil of former farmers. => Translated: Die Besitzer der Zucht pflanzen den Boden ehemaliger Bauern.\n",
            "Original: The oldest official map of Munich brings captivating stories to light. => Translated: Die älteste offizielle Karte Münchens bringt faszinierende Geschichten ins Licht.\n",
            "Original: It is annoying when geographical maps are not up-to-date. => Translated: Es ist ärgerlich, wenn geografische Karten nicht aktuell sind.\n",
            "Original: Anyone who has ever got worked up because the car's sat-nav is showing a green field instead of a bypass knows that. => Translated: Wer jemals daran gearbeitet hat, weil der Sat-Nav des Autos ein\n",
            "Original: The historical maps of the digital BayernAtlas, an offering from the State Government's Geoportal Bayern, are anything but up-to-date – and yet it is precisely for this reason that they are so informative. => Translated: Die historischen Karten des digitalen BayernAtlas, ein Angebot des Staatsreg\n",
            "Original: Especially when one compares them with current online maps. => Translated: Besonders wenn man sie mit aktuellen Karten online vergleicht.\n",
            "Original: Then it becomes clear how the towns and municipalities in the distribution area of Munich's Merkur newspaper have changed since the 19th century. => Translated: Dann wird klar, wie sich die Städte und Gemeinden im Verteilungsbereich der Mün\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Inference time for large model: {total_inference_time_large} seconds\")\n",
        "print(f\"Inference time for small model: {total_inference_time_small} seconds\")\n",
        "print(f\"The smaller model is {total_inference_time_large / total_inference_time_small} times faster than the larger model\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AQ5v5K5n1YpU",
        "outputId": "c6ffd7e8-0f24-4bbc-eb23-5a5a1072f259"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inference time for large model: 205.51281762123108 seconds\n",
            "Inference time for small model: 49.66063165664673 seconds\n",
            "The smaller model is 4.1383448169194725 times faster than the larger model\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}