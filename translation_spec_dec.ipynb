{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "if not os.path.exists(\"/scratch/sanika\"):\n",
    "    os.makedirs(\"/scratch/sanika\")\n",
    "\n",
    "os.environ[\"HF_HOME\"] = \"/scratch/sanika/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "EdqllqRvz8YX"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import T5ForConditionalGeneration, T5Tokenizer, AutoModelForSeq2SeqLM\n",
    "from typing import List, Tuple, Optional\n",
    "import time\n",
    "import numpy as np\n",
    "import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # WMT16 EN-DE dataset\n",
    "# def preprocess_function_ende(examples, tokenizer, args, train_args, prefix=\"translate English to German: \"):\n",
    "#     if train_args.debug:\n",
    "#         all_text = examples['translation'][:2]\n",
    "#     else:\n",
    "#         all_text = examples['translation']\n",
    "        \n",
    "#     inputs = []\n",
    "#     targets = []\n",
    "#     for excerpt in all_text:\n",
    "#         en_text = prefix + excerpt['en']\n",
    "#         de_text = excerpt['de']\n",
    "\n",
    "#         inputs.append(en_text)\n",
    "#         targets.append(de_text)\n",
    "            \n",
    "#     padding = 'max_length'\n",
    "#     model_inputs = tokenizer(\n",
    "#         inputs,\n",
    "#         max_length=args.source_max_length,\n",
    "#         padding=padding,\n",
    "#         truncation=True,\n",
    "#         return_tensors=\"pt\",\n",
    "#     )\n",
    "#     # Tokenize targets with the `text_target` keyword argument\n",
    "#     labels = tokenizer(\n",
    "#         text_target=targets,\n",
    "#         max_length=args.train_target_max_length,\n",
    "#         padding=padding,\n",
    "#         truncation=True,\n",
    "    #     return_tensors=\"pt\",\n",
    "    # )\n",
    "\n",
    "    # if padding == \"max_length\":\n",
    "    #             labels[\"input_ids\"] = [\n",
    "    #                 [(l if l != tokenizer.pad_token_id else -100) for l in label] for label in labels[\"input_ids\"]\n",
    "    #             ]\n",
    "\n",
    "    # model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    # model_inputs[\"decoder_attention_mask\"] = labels[\"attention_mask\"]\n",
    "    # return model_inputs\n",
    "\n",
    "en_gr_dataset = datasets.load_dataset('wmt16', 'de-en', split='validation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Base Speculative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "v3kozZQNyBBQ"
   },
   "outputs": [],
   "source": [
    "class SpeculativeDecoder:\n",
    "    def __init__(\n",
    "        self,\n",
    "        target_model_name = \"google-t5/t5-3b\",\n",
    "        draft_model_name = \"google-t5/t5-small\",\n",
    "        gamma = 4,\n",
    "        temperature = 1.0\n",
    "    ):\n",
    "        self.gamma = gamma\n",
    "\n",
    "        self.temperature = temperature\n",
    "\n",
    "        self.tokenizer = T5Tokenizer.from_pretrained(target_model_name)\n",
    "        self.target_model = AutoModelForSeq2SeqLM.from_pretrained(target_model_name, device_map='auto')\n",
    "\n",
    "        self.draft_model = T5ForConditionalGeneration.from_pretrained(draft_model_name, device_map='auto')\n",
    "\n",
    "        self.target_model.eval()\n",
    "        self.draft_model.eval()\n",
    "\n",
    "    def get_draft_logits(\n",
    "        self,\n",
    "        input_ids: torch.Tensor,\n",
    "        attention_mask: torch.Tensor,\n",
    "        decoder_input_ids: torch.Tensor,\n",
    "        gamma: int\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"Get draft logits for gamma tokens\"\"\"\n",
    "        draft_tokens = []\n",
    "        draft_probs = []\n",
    "        current_decoder_ids = decoder_input_ids\n",
    "\n",
    "        # Generate gamma tokens from the draft model\n",
    "        for _ in range(gamma):\n",
    "            with torch.no_grad():\n",
    "                outputs = self.draft_model(\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    decoder_input_ids=current_decoder_ids,\n",
    "                    return_dict=True,\n",
    "                )\n",
    "                logits = outputs.logits[:, -1, :]  # Get logits for last position\n",
    "                probs = logits\n",
    "                # probs = F.softmax(logits, dim=-1)\n",
    "\n",
    "                token_id = torch.argmax(probs, dim=-1)\n",
    "                prob = probs.gather(-1, token_id.unsqueeze(-1)).squeeze(-1)\n",
    "\n",
    "                draft_tokens.append(token_id.item())\n",
    "                draft_probs.append(prob.item())\n",
    "\n",
    "                # Update decoder inputs for next iteration\n",
    "                current_decoder_ids = torch.cat(\n",
    "                    [current_decoder_ids, token_id.view(1, 1)],\n",
    "                    dim=1\n",
    "                )\n",
    "\n",
    "                if token_id.item() == self.tokenizer.eos_token_id:\n",
    "                    break\n",
    "\n",
    "        return draft_tokens, draft_probs\n",
    "\n",
    "    def get_target_probs(\n",
    "        self,\n",
    "        input_ids: torch.Tensor,\n",
    "        attention_mask: torch.Tensor,\n",
    "        decoder_input_ids: torch.Tensor,\n",
    "        draft_tokens: torch.Tensor,\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"Get target probabilities for the draft tokens in parallel.\"\"\"\n",
    "        with torch.no_grad():\n",
    "            # Add draft tokens to decoder input\n",
    "            full_decoder_ids = torch.cat([decoder_input_ids, draft_tokens.unsqueeze(0)], dim=1)\n",
    "            # print(input_ids_batched.shape, attention_mask_batched.shape, padded_decoder_ids.shape)\n",
    "            # decoder_mask = torch.triu(\n",
    "            #     torch.ones((full_decoder_ids.shape[1], full_decoder_ids.shape[1] + 1))\n",
    "            # )\n",
    "            # decoder_mask = decoder_mask[-(len(draft_tokens) + 1):, :-1]\n",
    "            # decoder_mask = 1 - decoder_mask\n",
    "            \n",
    "            # the shapes that we want to see are:\n",
    "            # torch.Size([11, 12]) torch.Size([1, 12])\n",
    "            # torch.Size([11, 12]) torch.Size([11, 12])\n",
    "            # torch.Size([11, 32128])\n",
    "            # torch.Size([11, 32128]) torch.Size([11, 32128])\n",
    "\n",
    "            # What im getting\n",
    "            # torch.Size([1, 12]) torch.Size([1, 12])\n",
    "            # torch.Size([1, 12]) torch.Size([1, 12])\n",
    "            # torch.Size([1, 11, 32128])\n",
    "\n",
    "\n",
    "            decoder_mask = torch.ones(full_decoder_ids.shape[1])\n",
    "            decoder_mask = decoder_mask.unsqueeze(0)\n",
    "\n",
    "\n",
    "            # print(decoder_mask.shape, full_decoder_ids.shape)\n",
    "\n",
    "            # conver to a batched input\n",
    "            # input_ids = input_ids.repeat(len(draft_tokens) + 1, 1)\n",
    "            # attention_mask = attention_mask.repeat(len(draft_tokens) + 1, 1)\n",
    "            # full_decoder_ids = full_decoder_ids.repeat(len(draft_tokens) + 1, 1)\n",
    "\n",
    "            # print(decoder_mask.shape, full_decoder_ids.shape)\n",
    "\n",
    "\n",
    "            outputs = self.target_model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                decoder_input_ids=full_decoder_ids,\n",
    "                decoder_attention_mask=decoder_mask,\n",
    "                return_dict=True\n",
    "            )\n",
    "\n",
    "            \n",
    "            # dim_0_indices = torch.arange(len(draft_tokens) + 1)\n",
    "            # dim_1_indices = torch.arange(len(draft_tokens) + 1) + full_decoder_ids.shape[1] - 1 - len(draft_tokens)\n",
    "            # logits = outputs.logits[dim_0_indices, dim_1_indices, :]\n",
    "            logits = outputs.logits[:, -(len(draft_tokens) + 1):, :]\n",
    "            logits = logits.squeeze(0)\n",
    "\n",
    "            # print(logits.shape)\n",
    "            \n",
    "            # Get probabilities for positions before each draft token\n",
    "            # logits = outputs.logits[:, -(len(draft_tokens) + 1):-1, :]\n",
    "            target_probs = F.softmax(logits, dim=-1)\n",
    "\n",
    "            # print(target_probs.shape, target_probs.squeeze(0).shape)\n",
    "            \n",
    "\n",
    "            return target_probs.squeeze(0), logits\n",
    "\n",
    "    def verify_tokens(\n",
    "        self,\n",
    "        target_probs: torch.Tensor,\n",
    "        draft_tokens: torch.Tensor,\n",
    "        draft_probs: torch.Tensor,\n",
    "    ) -> int:\n",
    "        \"\"\"Determine number of accepted tokens\"\"\"\n",
    "        # Get target probabilities for the draft tokens\n",
    "        # get the probabilities of the tokens at the indices of the draft tokens\n",
    "        target_probs_draft_tokens = torch.gather(target_probs, 1, draft_tokens.unsqueeze(0))\n",
    "        # Calculate acceptance ratios\n",
    "        acceptance_ratios = target_probs_draft_tokens / draft_probs.clamp(min=1e-10)\n",
    "\n",
    "        # Sample uniform random numbers \n",
    "        random_nums = torch.rand_like(acceptance_ratios)\n",
    "        acceptance_mask = random_nums <= acceptance_ratios\n",
    "\n",
    "        num_accepted = (acceptance_mask.cumsum(dim=-1) == torch.arange(1, len(acceptance_ratios) + 1)).sum().item()\n",
    "\n",
    "        return num_accepted\n",
    "\n",
    "    def translate(\n",
    "        self,\n",
    "        source_text: str,\n",
    "        max_length: int = 128\n",
    "    ) -> str:\n",
    "        \"\"\"Translate source text using speculative decoding.\"\"\"\n",
    "        # Encode source text\n",
    "        encoder_inputs = self.tokenizer(\n",
    "            f\"translate English to German: {source_text}\",\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True\n",
    "        )\n",
    "\n",
    "        # Initialize with start token\n",
    "        decoder_input_ids = torch.tensor([[self.tokenizer.pad_token_id]])\n",
    "\n",
    "        output = self.target_model(\n",
    "            input_ids=encoder_inputs.input_ids,\n",
    "            attention_mask=encoder_inputs.attention_mask,\n",
    "            decoder_input_ids=decoder_input_ids,\n",
    "            return_dict=True\n",
    "        )\n",
    "\n",
    "        probs = output.logits[:, -1, :]\n",
    "                    \n",
    "        probs = F.softmax(probs, dim=-1)\n",
    "        token_id = torch.multinomial(probs, num_samples=1)\n",
    "\n",
    "        decoder_input_ids = torch.tensor([[self.tokenizer.pad_token_id, token_id.item()]])\n",
    "\n",
    "        total_tokens = 0\n",
    "        accepted_tokens = 0\n",
    "\n",
    "        while decoder_input_ids.shape[1] < max_length:\n",
    "            # Get draft tokens autoregressively\n",
    "            draft_tokens, draft_probs = self.get_draft_logits(\n",
    "                encoder_inputs.input_ids,\n",
    "                encoder_inputs.attention_mask,\n",
    "                decoder_input_ids,\n",
    "                self.gamma\n",
    "            )\n",
    "\n",
    "            draft_tokens = torch.tensor(draft_tokens)\n",
    "            draft_probs = torch.tensor(draft_probs)\n",
    "\n",
    "            # softmax the draft probs\n",
    "            draft_probs = F.softmax(draft_probs, dim=-1)\n",
    "\n",
    "            if len(draft_tokens) == 0:\n",
    "                raise ValueError(\"Draft tokens not generated.\")\n",
    "\n",
    "            # Get target probabilities in parallel\n",
    "            # start = time.time()\n",
    "            target_probs, target_logits = self.get_target_probs(\n",
    "                encoder_inputs.input_ids,\n",
    "                encoder_inputs.attention_mask,\n",
    "                decoder_input_ids,\n",
    "                draft_tokens\n",
    "            )\n",
    "            # target probs are the logits but with softmax applied\n",
    "\n",
    "            # Verify tokens\n",
    "            n_accepted = self.verify_tokens(target_probs, draft_tokens, draft_probs)\n",
    "            # Accept verified tokens\n",
    "            if n_accepted > 0:\n",
    "                decoder_input_ids = torch.cat([\n",
    "                    decoder_input_ids,\n",
    "                    draft_tokens[:n_accepted].unsqueeze(0)\n",
    "                ], dim=1)\n",
    "                \n",
    "            with torch.no_grad():\n",
    "                # n_rejected = self.gamma - n_accepted\n",
    "                n_rejected = len(draft_tokens) - n_accepted \n",
    "                total_tokens += len(draft_tokens)\n",
    "                accepted_tokens += n_accepted\n",
    "\n",
    "                if n_rejected > 0:\n",
    "                    probs = target_logits[-n_rejected, :] #- draft_logits[:, 1-n_rejected, :]\n",
    "                else:\n",
    "                    probs = target_logits[-1, :]\n",
    "                    \n",
    "                probs = F.softmax(probs, dim=-1)\n",
    "                token_id = torch.multinomial(probs, num_samples=1).unsqueeze(0)\n",
    "\n",
    "                decoder_input_ids = torch.cat([decoder_input_ids, token_id], dim=1)\n",
    "\n",
    "            # Check for end of sequence\n",
    "            if decoder_input_ids[0][-1].item() == self.tokenizer.eos_token_id:\n",
    "                break\n",
    "\n",
    "            # or if a full stop is generated\n",
    "            if decoder_input_ids[0][-1].item() == self.tokenizer.convert_tokens_to_ids('.'):\n",
    "                break\n",
    "\n",
    "        # Decode translation\n",
    "        translation = self.tokenizer.decode(\n",
    "            decoder_input_ids[0],\n",
    "            skip_special_tokens=True,\n",
    "            clean_up_tokenization_spaces=True\n",
    "        )\n",
    "        perc_accepted = accepted_tokens / total_tokens * 100\n",
    "        return translation, perc_accepted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Big model translation:  Ich mag pfel und Bananen zu essen.\n",
      "Time taken:  1.7336390018463135\n"
     ]
    }
   ],
   "source": [
    "# just translate using the big model\n",
    "big_model = AutoModelForSeq2SeqLM.from_pretrained(\"google-t5/t5-3b\", device_map='auto')\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"google-t5/t5-3b\")\n",
    "\n",
    "source_text = \"translate English to German: I like to eat apples and bananas.\"\n",
    "# source_text = \"translate English to German: India is also reportedly hoping for a deal on defence collaboration between the two nations.\"\n",
    "start = time.time()\n",
    "\n",
    "input_ids = tokenizer(source_text, return_tensors=\"pt\").input_ids.to(big_model.device)\n",
    "decoder_input_ids = torch.tensor([[tokenizer.pad_token_id]], device=big_model.device)  # Start with PAD token\n",
    "eos_token_id = tokenizer.eos_token_id\n",
    "\n",
    "output_tokens = []\n",
    "while True:\n",
    "    # Predict the next token\n",
    "    outputs = big_model(input_ids=input_ids, decoder_input_ids=decoder_input_ids)\n",
    "    next_token_logits = outputs.logits[:, -1, :]  # Get logits for the last token\n",
    "    next_token_id = torch.argmax(next_token_logits, dim=-1)  # Greedy decoding\n",
    "\n",
    "    # Append the generated token to the decoder input\n",
    "    decoder_input_ids = torch.cat([decoder_input_ids, next_token_id.unsqueeze(0)], dim=-1)\n",
    "    output_tokens.append(next_token_id.item())\n",
    "\n",
    "    # Break the loop if the EOS token is generated\n",
    "    if next_token_id.item() == eos_token_id:\n",
    "        break\n",
    "\n",
    "big_translation = tokenizer.decode(output_tokens, skip_special_tokens=True)\n",
    "\n",
    "# Print the results\n",
    "print(\"Big model translation: \", big_translation)\n",
    "print(\"Time taken: \", time.time() - start)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    }
   ],
   "source": [
    "speculative_decoder = SpeculativeDecoder(gamma=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Speculative translation: Ich essen gernepfel und Banan.\n",
      "Percentage tokens accepted: 41.18%\n",
      "Time taken: 0.6877 seconds\n"
     ]
    }
   ],
   "source": [
    "source_text = \"I like to eat apples and bananas.\"\n",
    "# source_text = \"India is also reportedly hoping for a deal on defence collaboration between the two nations.\"\n",
    "speculative_decoder.translate(source_text)\n",
    "start = time.time()\n",
    "speculative_translation, pc = speculative_decoder.translate(source_text)\n",
    "end = time.time()\n",
    "print(f\"Speculative translation: {speculative_translation}\")\n",
    "print(f\"Percentage tokens accepted: {pc:.2f}%\")\n",
    "print(f\"Time taken: {end - start:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "CopfVI-mjdb4"
   },
   "outputs": [],
   "source": [
    "class NormalDecoder:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name: str = \"google-t5/t5-3b\",\n",
    "        device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "    ):\n",
    "        self.device = device\n",
    "\n",
    "        # Initialize tokenizer and model\n",
    "        self.tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "        # self.model = T5ForConditionalGeneration.from_pretrained(model_name).to(device)\n",
    "        self.model = AutoModelForSeq2SeqLM.from_pretrained(model_name, device_map='auto')\n",
    "        self.model.eval()\n",
    "\n",
    "    def get_logits(\n",
    "        self,\n",
    "        input_ids: torch.Tensor,\n",
    "        attention_mask: torch.Tensor,\n",
    "        decoder_input_ids: torch.Tensor\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"Get logits from model for the last token.\"\"\"\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                decoder_input_ids=decoder_input_ids,\n",
    "                return_dict=True\n",
    "            )\n",
    "            return outputs.logits[:, -1, :]\n",
    "\n",
    "    def sample_token(self, logits: torch.Tensor, temperature: float = 1.0) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"Sample a token from logits using temperature sampling.\"\"\"\n",
    "        if temperature == 0:\n",
    "            # Greedy sampling\n",
    "            token_id = torch.argmax(logits, dim=-1)\n",
    "            prob = torch.ones_like(token_id, dtype=torch.float)\n",
    "        else:\n",
    "            # Temperature sampling\n",
    "            probs = F.softmax(logits / temperature, dim=-1)\n",
    "            token_id = torch.multinomial(probs, num_samples=1).squeeze(-1)\n",
    "            prob = probs.gather(-1, token_id.unsqueeze(-1)).squeeze(-1)\n",
    "        return token_id, prob\n",
    "\n",
    "    def translate(\n",
    "        self,\n",
    "        source_text: str,\n",
    "        max_length: int = 128,\n",
    "        temperature: float = 0.7\n",
    "    ) -> str:\n",
    "        \"\"\"Translate source text using the normal T5 model.\"\"\"\n",
    "        # Encode source text\n",
    "        encoder_inputs = self.tokenizer(\n",
    "            f\"translate English to German: {source_text}\",\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True\n",
    "        ).to(self.device)\n",
    "\n",
    "        # Initialize decoder input with start token\n",
    "        decoder_input_ids = torch.tensor([[self.tokenizer.pad_token_id]], device=self.device)\n",
    "\n",
    "        while decoder_input_ids.shape[1] < max_length:\n",
    "            # Generate logits for the next token\n",
    "            logits = self.get_logits(\n",
    "                encoder_inputs.input_ids,\n",
    "                encoder_inputs.attention_mask,\n",
    "                decoder_input_ids\n",
    "            )\n",
    "\n",
    "            # Sample a token\n",
    "            token_id, _ = self.sample_token(logits, temperature)\n",
    "\n",
    "            # Add token to the decoder input\n",
    "            decoder_input_ids = torch.cat(\n",
    "                [decoder_input_ids, token_id.view(1, 1)],\n",
    "                dim=1\n",
    "            )\n",
    "\n",
    "            # Break if end token is generated\n",
    "            if token_id.item() == self.tokenizer.eos_token_id:\n",
    "                break\n",
    "\n",
    "        # Decode and return translation\n",
    "        translation = self.tokenizer.decode(\n",
    "            decoder_input_ids[0],\n",
    "            skip_special_tokens=True,\n",
    "            clean_up_tokenization_spaces=True\n",
    "        )\n",
    "        return translation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# speculative_decoder = SpeculativeDecoder()\n",
    "# normal_decoder = NormalDecoder()\n",
    "\n",
    "# source_text = \"He said Lamb made the fateful 911 call sometime after that.\" # spec does not work\n",
    "\n",
    "# speculative_decoder.translate(source_text)\n",
    "# start = time.time()\n",
    "# speculative_translation, pc = speculative_decoder.translate(source_text)\n",
    "# end = time.time()\n",
    "# print(f\"Speculative translation: {speculative_translation}\")\n",
    "# print(f\"Percentage tokens accepted: {pc:.2f}%\")\n",
    "# print(f\"Time taken: {end - start:.4f} seconds\")\n",
    "\n",
    "# start = time.time()\n",
    "# normal_translation = normal_decoder.translate(source_text)\n",
    "# end = time.time()\n",
    "# print(f\"Normal translation: {normal_translation}\")\n",
    "# print(f\"Time taken: {end - start:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nwtUoGDUjiSD",
    "outputId": "2a92f134-5a76-45b9-884b-51515d57fa4d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 1/30 [00:01<00:40,  1.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source: India and Japan prime ministers meet in Tokyo\n",
      "Normal Translation: Indien trifft auf Japan\n",
      "Time taken: 0.65 seconds\n",
      "Speculative Translation: Indien und Japans Premierminister treffen sich in Tokio\n",
      "Time taken: 0.74 seconds\n",
      "Percentage tokens accepted: 100.00%\n",
      "Target: Die Premierminister Indiens und Japans trafen sich in Tokio.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 2/30 [00:10<02:41,  5.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source: India's new prime minister, Narendra Modi, is meeting his Japanese counterpart, Shinzo Abe, in Tokyo to discuss economic and security ties, on his first major foreign visit since winning May's election.\n",
      "Normal Translation: Der neue indische Premierminister Narendra Modi trifft auf seinem ersten großen Auslandsbesuch seit seinem Wahlsieg im Mai sein japanisches Amtskollege Shinzo Abe in Tokio, um Wirtschafts- und Sicherheitsbeziehungen zu diskutieren.\n",
      "Time taken: 6.15 seconds\n",
      "Speculative Translation: Auf seinemersten großen Außenbesuch seit dem Sieg der Wahlen im Mai trifft Indiens neuer Premierminister Narendra Modi sein japanisches Amtskollegen Shinzo Abe in Tokio, um überwirtschaftliche und Sicherheitsbeziehungen zu diskutieren.\n",
      "Time taken: 2.70 seconds\n",
      "Percentage tokens accepted: 75.71%\n",
      "Target: Indiens neuer Premierminister Narendra Modi trifft bei seinem ersten wichtigen Auslandsbesuch seit seinem Wahlsieg im Mai seinen japanischen Amtskollegen Shinzo Abe in Toko, um wirtschaftliche und sicherheitspolitische Beziehungen zu besprechen.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 3/30 [00:15<02:29,  5.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source: Mr Modi is on a five-day trip to Japan to strengthen economic ties with the third largest economy in the world.\n",
      "Normal Translation: Herr Modi ist auf einer fünftägigen Reise nach Japan, um die wirtschaftlichen Beziehungen mit der drittgrößten Volkswirtschaft der Welt zu verstärken.\n",
      "Time taken: 3.58 seconds\n",
      "Speculative Translation: Vor fünf Tagenreist Herr Mod nach Japan, um die wirtschaftlichen Beziehungen zur drittgrößten Wirtschaft der Welt zu stärken.\n",
      "Time taken: 1.64 seconds\n",
      "Percentage tokens accepted: 62.50%\n",
      "Target: Herr Modi befindet sich auf einer fünftägigen Reise nach Japan, um die wirtschaftlichen Beziehungen mit der drittgrößten Wirtschaftsnation der Welt zu festigen.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 4/30 [00:17<01:52,  4.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source: High on the agenda are plans for greater nuclear co-operation.\n",
      "Normal Translation: Eine hohe Priorität hat die Intensivierung der nuklearen Zusammenarbeit.\n",
      "Time taken: 1.63 seconds\n",
      "Speculative Translation: Ein Schwerpunkt auf der Agenda sind diePläne für eine verstärkte nukleare Zusammenarbeit\n",
      "Time taken: 0.85 seconds\n",
      "Percentage tokens accepted: 70.00%\n",
      "Target: Pläne für eine stärkere kerntechnische Zusammenarbeit stehen ganz oben auf der Tagesordnung.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 5/30 [00:21<01:44,  4.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source: India is also reportedly hoping for a deal on defence collaboration between the two nations.\n",
      "Normal Translation: Indien erhofft sich angeblich auch eine Abmachung über Verteidigungskooperationen zwischen den beiden Nationen.\n",
      "Time taken: 2.83 seconds\n",
      "Speculative Translation: Unter anderem hoff Indien auf eine Vereinbarung über die Zusammenarbeit im Verteidigungsbereich zwischen den beiden Nation.\n",
      "Time taken: 1.15 seconds\n",
      "Percentage tokens accepted: 73.33%\n",
      "Target: Berichten zufolge hofft Indien darüber hinaus auf einen Vertrag zur Verteidigungszusammenarbeit zwischen den beiden Nationen.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 6/30 [00:25<01:34,  3.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source: Karratha police arrest 20-year-old after high speed motorcycle chase\n",
      "Normal Translation: Die Polizei in Karratha verhaftet 20-jährige Mute nach einer schnellen Motorradjagd\n",
      "Time taken: 2.17 seconds\n",
      "Speculative Translation: Die Polizei inratha verhafte einen 20-jährigen Nachfolger von Hochgeschwindigkeitsjagd\n",
      "Time taken: 1.29 seconds\n",
      "Percentage tokens accepted: 54.55%\n",
      "Target: Polizei von Karratha verhaftet 20-Jährigen nach schneller Motorradjagd\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 7/30 [00:28<01:21,  3.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source: A motorcycle has been seized after it was ridden at 125km/h in a 70km/h zone and through bushland to escape police in the Pilbara.\n",
      "Normal Translation: Die deutschen rzte haben zwei neue medizinische Geräte für den Menschen entwickelt.\n",
      "Time taken: 1.86 seconds\n",
      "Speculative Translation: Carlo hat sich in der letzten Zeit mit externalen nderungen befass.\n",
      "Time taken: 0.83 seconds\n",
      "Percentage tokens accepted: 84.21%\n",
      "Target: Ein Motorrad wurde beschlagnahmt, nachdem der Fahrer es mit 125 km/h in einer 70 km/h-Zone und durch Buschland gefahren hatte, um der Polizei in Bilbara zu entkommen.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 8/30 [00:35<01:47,  4.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source: Traffic police on patrol in Karratha this morning tried to pull over a blue motorcycle when they spotted it reaching 125km/h as it pulled out of a service station on Bathgate Road.\n",
      "Normal Translation: 15 Menschen sind im Gefängnis in der südlichsten Region des Landes verhaftet worden. Die Polizei hat vier Personen inhaftiert und die Polizisten sind weiterhin im Gefängnis.\n",
      "Time taken: 4.83 seconds\n",
      "Speculative Translation: EU-Behörde für Polizei und Polizei in Karratha hat heute Morgen versucht, ein blaues Motorrad zu überziehen, als sie sahen, dass e 125 km/h erreicht, alses aus einer Servicestation auf der Straß Bathgate zog.\n",
      "Time taken: 2.93 seconds\n",
      "Percentage tokens accepted: 74.67%\n",
      "Target: Verkehrspolizisten in Karratha versuchten heute morgen, ein blaues Motorrad zu stoppen, nachdem sie es dabei beobachtet hatten, wie es mit 125 km/h eine Tankstelle auf der Bathdate Road verließ.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 9/30 [00:42<01:56,  5.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source: Police say the rider then failed to stop and continued on to Burgess Road before turning into bushland, causing the officers to lose sight of it.\n",
      "Normal Translation: In der Schlacht um den Brand stürzte der künstlerische Trainer der Lindauer Fussballnationalmannschaft, Claude Vias, in einem Unfall die Turbine, die das Fahrrad auf dem Straßenrand verschossen hatte.\n",
      "Time taken: 5.86 seconds\n",
      "Speculative Translation: One of the most important things about the ride was the fact that the rider was not in auto- mode.\n",
      "Time taken: 1.20 seconds\n",
      "Percentage tokens accepted: 74.07%\n",
      "Target: Die Polizei berichtet, dass der Fahrer die Haltesignale dann ignorierte und weiter auf der Burgess Road fuhr, bevor er in das Buschland abbog, wo die Beamten es aus den Augen verloren.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 10/30 [00:47<01:44,  5.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source: The motorcycle and a person matching the description of the rider was then spotted at a house on Walcott Way in Bulgarra.\n",
      "Normal Translation: Der Motorrad und ein Mensch, der der Beschreibung des Fahrers entsprach, wurden dann in einem Haus auf der Walcott Way in Bulgarra gesehen.\n",
      "Time taken: 3.26 seconds\n",
      "Speculative Translation: Ein Motorrad und eine Person, die der Beschreibung des Fahrers entspricht, wurden dann in einem Haus auf der Walcott Way in Bulgarra gesehen.\n",
      "Time taken: 1.17 seconds\n",
      "Percentage tokens accepted: 100.00%\n",
      "Target: Das Motorrad sowie eine Person, die der Beschreibung des Fahrers entsprach wurden später bei einem Haus im Walcott Way in Bulgarra gesehen.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 11/30 [00:52<01:38,  5.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source: Karratha Police have charged a 20-year-old man with failing to stop and reckless driving.\n",
      "Normal Translation: Die Polizei von Karratha hat einen 20-Jährigen wegen Fahrverbot und Fahrenstumpf beschuldigt.\n",
      "Time taken: 2.74 seconds\n",
      "Speculative Translation: Auf der Polizei von Karratha wurde ein 20-jähriger Mann angeklag, er sei vorgeworf worden, nicht gleich zu bleiben und fahrlässig fahren zu müssen\n",
      "Time taken: 2.39 seconds\n",
      "Percentage tokens accepted: 52.31%\n",
      "Target: Die Polizei von Karratha beschuldigt einen 20-jährigen Mann der Nichtbeachtung eines Haltesignals sowie rücksichtslosen Fahrens.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 12/30 [00:55<01:20,  4.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source: He is due to appear in Karratha Magistrates Court on September 23.\n",
      "Normal Translation: Der Mann soll am 23. September vor dem Magistrate in Karratha erscheinen.\n",
      "Time taken: 1.71 seconds\n",
      "Speculative Translation: Aber er wird am 23 September im Magistra von Karratha erscheinen.\n",
      "Time taken: 1.03 seconds\n",
      "Percentage tokens accepted: 53.85%\n",
      "Target: Er soll am 23. September vor dem Amtsgericht in Karratha erscheinen.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 13/30 [00:58<01:08,  4.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source: The motorcycle was seized and impounded for three months.\n",
      "Normal Translation: Das Motorrad wurde für drei Monate beschlagnahmt und verwahrt.\n",
      "Time taken: 1.61 seconds\n",
      "Speculative Translation: 400 m2–150 m Fläche, diezwänglich errichtet wurde\n",
      "Time taken: 1.42 seconds\n",
      "Percentage tokens accepted: 38.46%\n",
      "Target: Das Motorrad wurde sichergestellt und für drei Monate beschlagnahmt.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 14/30 [01:02<01:04,  4.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source: George Webster accused of Nairn and Pitlochry hotel rapes\n",
      "Normal Translation: George Webster angeklagt für die Vergewaltigungen in Hotels in Nairn und Pitlochry\n",
      "Time taken: 2.35 seconds\n",
      "Speculative Translation: George Web hat der Vergewaligung von Hotelgäst in Nair und Pitlochry beschuldigt\n",
      "Time taken: 1.72 seconds\n",
      "Percentage tokens accepted: 40.43%\n",
      "Target: George Webster wegen Hotelvergewaltigungen in Naim und Pitlochry angeklagt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 15/30 [01:06<01:02,  4.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source: A man is to stand trial accused of raping women at two hotels.\n",
      "Normal Translation: Ein Mann soll vor Gericht gestellt werden, weil er angeklagt ist, Frauen in zwei Hotels vergewaltigt zu haben.\n",
      "Time taken: 2.91 seconds\n",
      "Speculative Translation: Ein Mann wird vor Gericht gestellt,  könne Frauen zweier Wohnungen vergewaltig haben.\n",
      "Time taken: 1.57 seconds\n",
      "Percentage tokens accepted: 41.46%\n",
      "Target: Ein Mann steht wegen der Vergewaltigung von Frauen in zwei Hotels vor Gericht.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 16/30 [01:11<00:58,  4.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source: George Webster, 28, faced the charges during a hearing at the High Court in Glasgow.\n",
      "Normal Translation: George Webster, 28 Jahre alt, leistete sich dieser Anklage in einer Anhörung des High Court in Glasgow.\n",
      "Time taken: 2.73 seconds\n",
      "Speculative Translation: Der 28-jährige George Webster hat sichwährend einer Anhörung am Hohen Gericht Glasgow mit vorgeworfen\n",
      "Time taken: 1.54 seconds\n",
      "Percentage tokens accepted: 56.41%\n",
      "Target: George Webster, 28, wurde die Anklage bei einer Anhörung von dem Obersten Gericht in Glasgow verlesen.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 17/30 [01:14<00:52,  4.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source: He is alleged to have raped a woman at the Scotland's Hotel in Pitlochry in Perthshire on June 7, 2013.\n",
      "Normal Translation: Er soll am 7. Juni 2013 im Scotland's Hotel in Pitlochry im Perthshire eine Frau vergewaltigt haben.\n",
      "Time taken: 2.59 seconds\n",
      "Speculative Translation: Er soll am 7. Juni 2013 eine Frau im Scottish's Hotel in Pitlochry in Perthshire vergewaltig haben.\n",
      "Time taken: 1.15 seconds\n",
      "Percentage tokens accepted: 85.19%\n",
      "Target: Er wird beschuldigt, am 7. Juni 2013 eine Frau im Scotland's Hotel in Pitlochry in Perthshire vergewaltigt zu haben.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 18/30 [01:19<00:52,  4.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source: It is claimed Webster attacked her while she was \"unconscious, asleep and incapable of giving consent.\"\n",
      "Normal Translation: Sie wohnte in einem kleinen Haus in Woodsboro, Virginia.\n",
      "Time taken: 1.71 seconds\n",
      "Speculative Translation: Internet hat im Jahr 2373 dieamerikanischenen Dienstestoßeschließlich erlitten. 012365 hat eine deutscheAdresse, dienennt sie Webster..\n",
      "Time taken: 3.36 seconds\n",
      "Percentage tokens accepted: 35.96%\n",
      "Target: Die Anklage lautet, dass Webster sie angriff, während sie \"bewusstlos war, schlief, und kein Einverständnis signalisieren konnte.\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|██████▎   | 19/30 [01:25<00:51,  4.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source: Webster is then charged with raping a second woman at the Golf View Hotel in Nairn in the Highlands on May 4, 2014.\n",
      "Normal Translation: Webster wird dann angeklagt, eine zweite Frau im Golf View Hotel in Nairn in der Highlands am 4. Mai 2014 vergewaltigt zu haben.\n",
      "Time taken: 3.54 seconds\n",
      "Speculative Translation: Weil Web einen zweiten Mann im Golf View Hotel in Nairn im Highland verschleppt hat, wird Webster am 4. Mai 2014 angeklagt.\n",
      "Time taken: 1.80 seconds\n",
      "Percentage tokens accepted: 65.31%\n",
      "Target: Webster wird darüber hinaus vorgeworfen, am 4. Mai 2014 eine zweite Frau im Golf View Hotel in Naim im schottischen Hochland vergewaltigt zu haben.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 20/30 [01:27<00:39,  3.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source: Judge Lady Rae set a trial date for November 17 at the High Court in Edinburgh.\n",
      "Normal Translation: Der Prozess wird am 17. November vor dem High Court von Edinburgh stattfinden.\n",
      "Time taken: 1.35 seconds\n",
      "Speculative Translation: Laut Richterin Lady Rae wurde vor dem Hohen Gericht in Edinburgh ein Verfahrensdatum für den 17. November festgelegt.\n",
      "Time taken: 1.07 seconds\n",
      "Percentage tokens accepted: 85.71%\n",
      "Target: Richterin Lady Rae setzte den Verhandlungstermin für den 17. November am Obersten Gericht in Edinburgh an.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 21/30 [01:30<00:33,  3.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source: Reconnecting With the Very American Ideal That Labor Rights Are Human Rights\n",
      "Normal Translation: Die Wiederherstellung des amerikanischen Ideals, wonach Arbeitsrechte Menschenrechte sind\n",
      "Time taken: 1.88 seconds\n",
      "Speculative Translation: Wiederherstellung der Verbindung dem sehramerikanischen Ideal, dass Arbeitsrechte Menschenrechte sind\n",
      "Time taken: 1.23 seconds\n",
      "Percentage tokens accepted: 51.61%\n",
      "Target: Rückbesinnung auf das sehr amerikanische Ideal der Arbeitsrechte als Menschenrechte\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 22/30 [01:35<00:31,  3.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source: Congressmen Keith Ellison and John Lewis have proposed legislation to protect union organizing as a civil right.\n",
      "Normal Translation: Die Kongressabgeordneten Keith Ellison und John Lewis haben Gesetzesvorschläge zur Sicherung der Gewerkschaftsorganisation als Bürgerrecht vorgelegt.\n",
      "Time taken: 3.40 seconds\n",
      "Speculative Translation: Die Kongressabgeordneten Keith Ellison und John Lewis haben Gesetze vorgeschlagen, um Gewerkschaftsorganisationen als Bürgerrecht zu schützen.\n",
      "Time taken: 1.13 seconds\n",
      "Percentage tokens accepted: 100.00%\n",
      "Target: Die Kongressabgeordneten Keith Ellison und John Lewis haben einen Gesetzesvorschlag eingebracht, um die Organisation von Gewerkschaften als Bürgerrecht zu etablieren.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|███████▋  | 23/30 [01:42<00:35,  5.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source: \"As go unions, so go middle-class jobs,\" says Ellison, the Minnesota Democrat who serves as a Congressional Progressive Caucus co-chair.\n",
      "Normal Translation: \"Was mit Gewerkschaften geht, geht auch mit den Arbeitsplätzen der Mittelschicht\" sagt Ellison, der Demokrat aus Minnesota, der als Ko-Vorsitzender des Progressiven Kongressausschusses fungiert.\n",
      "Time taken: 5.24 seconds\n",
      "Speculative Translation: \"So wie Gewerkschaften gehen, so gehen die Arbeitsplätze derschichtsträchtig\", sagt Ellison, der Demokrat in Minnesota, der als Mitvorsitzender des Progressiven Kongressausschusse im Kongress fungiert\n",
      "Time taken: 2.44 seconds\n",
      "Percentage tokens accepted: 67.69%\n",
      "Target: \"So wie Gewerkschaften sterben, sterben auch die Mittelklassejobs,\" sagte Ellison, ein Demokrat aus Minnesota und stellvertretender Vorsitzender des Progressive Caucus im Kongress.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 24/30 [01:47<00:28,  4.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source: That's why I'm proud to introduce the Employee Empowerment Act with civil rights icon John Lewis.\n",
      "Normal Translation: Aus diesem Grund freue ich mich, den Employee Empowerment Act mit dem Bürgerrechts-Ikonen John Lewis vorstellen zu können.\n",
      "Time taken: 2.84 seconds\n",
      "Speculative Translation: Das ist der Grund,warum ich stolz bin, das Arbeitnehmereigenschaftsgesetz mit dem Bürgerrechtsikon John Lewis präsentieren zu dürfen\n",
      "Time taken: 1.33 seconds\n",
      "Percentage tokens accepted: 76.47%\n",
      "Target: Daher stelle ich stolz gemeinsam mit der Bürgerrechtsikone John Lewis das Mitarbeiterermächtigungsgesetz vor.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 25/30 [01:56<00:30,  6.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source: This ground-breaking legislation will give workers the same legal options for union organizing discrimination as for other forms of discrimination - stopping anti-union forces in their tracks\n",
      "Normal Translation: Diese wegweisende Gesetzgebung wird Arbeitnehmern die gleichen gesetzlichen Möglichkeiten für die Diskriminierung aufgrund gewerkschaftlicher Organisationen geben wie für andere Formen der Diskriminierung - und antigewerkschaftliche Kräfte aufhalten;\n",
      "Time taken: 5.40 seconds\n",
      "Speculative Translation: Mit dieserwegweisenden Geetzgebung werden die Arbeitnehmer die gleichen rechtlichen Möglichkeiten für die Organisation von Diskriminierungen in gewerkschaftlichen Organisationen haben wie für andere Formen der Diskriminierung - die Gewerkschaftseinsätze in ihrenenen Richtung stoppen.\n",
      "Time taken: 3.57 seconds\n",
      "Percentage tokens accepted: 58.06%\n",
      "Target: Dieses bahnbrechende Gesetz gibt Arbeitern die gleichen rechtlichen Möglichkeiten bei Diskriminierung wegen der Organisation von Gewerkschaften wie bei anderen Formen der Diskriminierung - und stoppt so antigewerkschaftlich eingestellte Kräfte.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 26/30 [02:07<00:30,  7.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source: Amending the National Labor Relations Act to allow workers who face discrimination for engaging in union organizing to sue for justice in the civil courts - and to collect compensatory and punitive damages - is a sound and necessary initiative.\n",
      "Normal Translation: Die nderung des \"National Labor Relations Act\", das es Arbeitnehmern, die wegen ihrer Beteiligung beim Gewerkschaftsaufbau diskriminiert werden, erlaubt, sich vor Zivilgerichten für Gerechtigkeit einzusetzen - und Entschädigungs- und Strafgelder einzufordern - ist eine sinnvolle und notwendige Initiative.\n",
      "Time taken: 7.71 seconds\n",
      "Speculative Translation: des National Relations Act zu erlauben, Arbeitnehmern, die Diskriminierungandrohungen ausgesetzt sind, die Gewerkschaftsorganisation an den Zivilgericht klagen zu lassen  und entschädigende undstrafbare Schäden einzuziehen, ist eine gute und notwendige Initiative.\n",
      "Time taken: 3.91 seconds\n",
      "Percentage tokens accepted: 55.24%\n",
      "Target: Die Ergänzung des nationalen Arbeitsrechtsgesetzes, um eine Möglichkeit für einer Diskriminierung ausgesetzte Arbeitern zur Organisation einer Gewerkschaftsvertretung zu schaffen, um vor einem Zivilgericht um Gerechtigkeit zu klagen - und um Schadensersatz oder Strafgelder zu erhalten - ist eine sinnvolle und notwendige Initiative.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 27/30 [02:11<00:19,  6.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source: But it in certainly not a radical initiative - at least by American standards.\n",
      "Normal Translation: Doch ist dies sicherlich keine radikale Initiative - zumindest nach amerikanischen Maßstäben.\n",
      "Time taken: 2.53 seconds\n",
      "Speculative Translation: Es ist jedochsicherlich keine radikale Initiative - zumindest in amerikanischen Standards.\n",
      "Time taken: 1.03 seconds\n",
      "Percentage tokens accepted: 69.23%\n",
      "Target: Aber es ist mit Sicherheit keine radikale Initiative - jedenfalls nicht nach amerikanischen Standards.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 28/30 [02:18<00:13,  6.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source: Indeed, the best way to understand what Ellison, Lewis and the cosponsors of their legislation are proposing is as a reconnection with a very American idea.\n",
      "Normal Translation: Tatsächlich ist das, was Ellison, Lewis und die Mitsponsoren ihrer Gesetzesvorlage vorschlagen, am besten als eine Wiederverbindung mit einer ziemlich amerikanischen Idee zu verstehen.\n",
      "Time taken: 4.66 seconds\n",
      "Speculative Translation: Tatsächlich ist die beste Möglichkeit, zu verstehen, was Ellison Lewis und die Mitverfasser ihrer Gesetzgebung vorschlagen, eine Verknüpfung mit sehr amerikanischer Idee.\n",
      "Time taken: 2.11 seconds\n",
      "Percentage tokens accepted: 74.55%\n",
      "Target: Tatsächlich ist die beste Art und Weise zum Verständnis dessen, was Ellison, Lewis und die weiteren Sponsoren ihrer Gesetzesvorlage vorschlagen, die Verbindung zurück zu einer sehr amerikanischen Idee.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|█████████▋| 29/30 [02:29<00:08,  8.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source: Despite the battering that unions have taken in recent years - in Wisconsin, Michigan and states across the country - Americans once encouraged countries around the world to embrace, extend and respect labor rights.\n",
      "Normal Translation: Trotz des starken Schlags, den die Gewerkschaften in den letzten Jahren - in Wisconsin, Michigan und in weiteren Bundesstaaten - erlitten, ermutigten die Amerikaner einst Länder auf der ganzen Welt, ihre Arbeitsrechte aufzunehmen, auszubauen und zu respektieren.\n",
      "Time taken: 7.22 seconds\n",
      "Speculative Translation: Trotz der erschütternden Haltung, die diewerklichen Gewerkschaft in denletzten Jahren - in Wisconsin, Michigan und in allen Bundesstaat des Landes- eingeleitet, ermutigten die Amerikaner einst Länder auf der ganzen Welt, ihre Arbeitsrechte zu berücksichtig, auszuweit und zu respektieren\n",
      "Time taken: 4.67 seconds\n",
      "Percentage tokens accepted: 49.61%\n",
      "Target: Trotz der Rückschläge, denen die Gewerkschaften in den vergangenen Jahren ausgesetzt waren - in Wisconsin, Michigan und anderen Staaten im ganzen Land - haben Amerikaner einst Länder in aller Welt dazu ermutigt, Arbeitsrechte anzuerkennen, auszuweiten und einzuhalten.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [02:37<00:00,  5.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source: There was a time, within the living memory of millions of Americans, when this country championed democracy, freedom of speech, freedom of the press and the right to organize in the same breath.\n",
      "Normal Translation: Es gab eine Zeit, an die Millionen von Amerikanern noch erinnern, als dieses Land im selben Atemzug für Demokratie, Rede- und Pressefreiheit und das Recht auf Organisation eintrat.\n",
      "Time taken: 4.78 seconds\n",
      "Speculative Translation: Es gab eine Zeit, in der Millionen von Amerikanern leben, als dieses Land Demokratie, Redfreiheit, Pressfreiheit und das Recht, sich in gleichem Atem organisieren zu können, für sich kämpfte.\n",
      "Time taken: 2.58 seconds\n",
      "Percentage tokens accepted: 64.29%\n",
      "Target: Es gab eine Zeit, an die sich Millionen von Amerikanern noch erinnern, als dieses Land Demokratie, Redefreiheit, Pressefreiheit und das Vereinigungsrecht in einem Atemzug nannte.\n",
      "\n",
      "Average time taken for normal decoding: 3.39 seconds\n",
      "Average time taken for speculative decoding: 1.85 seconds\n",
      "Average speedup over 30 iterations: 1.83x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize decoder\n",
    "from tqdm import tqdm\n",
    "\n",
    "speculative_decoder = SpeculativeDecoder(gamma=10, temperature=0.1)\n",
    "normal_decoder = NormalDecoder()\n",
    "\n",
    "spec_total_time = 0\n",
    "normal_total_time = 0\n",
    "total_iters = 0\n",
    "\n",
    "for i in tqdm(en_gr_dataset['translation'][:30]):\n",
    "    source_text = i['en']\n",
    "    target_text = i['de']\n",
    "    \n",
    "    # Time the translation\n",
    "    start_time = time.time()\n",
    "    spec_translation, pc = speculative_decoder.translate(source_text)\n",
    "    end_time = time.time()\n",
    "\n",
    "    spec_time = end_time - start_time\n",
    "\n",
    "    # spec_total_time += spec_time\n",
    "    \n",
    "    start_time = time.time()\n",
    "    normal_translation = normal_decoder.translate(source_text)\n",
    "    end_time = time.time()\n",
    "\n",
    "    normal_time = end_time - start_time\n",
    "\n",
    "    # normal_total_time += normal_time\n",
    "\n",
    "    print(f\"Source: {source_text}\")\n",
    "    print(f\"Normal Translation: {normal_translation}\")\n",
    "    print(f\"Time taken: {normal_time:.2f} seconds\")\n",
    "    print(f\"Speculative Translation: {spec_translation}\")\n",
    "    print(f\"Time taken: {spec_time:.2f} seconds\")\n",
    "    print(f\"Percentage tokens accepted: {pc:.2f}%\")\n",
    "    \n",
    "    print(f\"Target: {target_text}\")\n",
    "\n",
    "    # if normal_time - spec_time > -0.1:\n",
    "    spec_total_time += spec_time\n",
    "    normal_total_time += normal_time\n",
    "    total_iters += 1\n",
    "\n",
    "print(f\"\\nAverage time taken for normal decoding: {normal_total_time / total_iters:.2f} seconds\")\n",
    "print(f\"Average time taken for speculative decoding: {spec_total_time / total_iters:.2f} seconds\")\n",
    "print(f\"Average speedup over {total_iters} iterations: {normal_total_time / spec_total_time:.2f}x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize decoder\n",
    "from tqdm import tqdm\n",
    "\n",
    "speculative_decoder = SpeculativeDecoder(gamma=8, temperature=0.1, draft_model_name='google-t5/t5-base')\n",
    "normal_decoder = NormalDecoder()\n",
    "\n",
    "spec_total_time = 0\n",
    "normal_total_time = 0\n",
    "total_iters = 0\n",
    "\n",
    "for i in tqdm(en_gr_dataset['test']['translation'][:30]):\n",
    "    source_text = i['en']\n",
    "    target_text = i['de']\n",
    "    \n",
    "    # Time the translation\n",
    "    start_time = time.time()\n",
    "    spec_translation, pc = speculative_decoder.translate(source_text)\n",
    "    end_time = time.time()\n",
    "\n",
    "    spec_time = end_time - start_time\n",
    "\n",
    "    # spec_total_time += spec_time\n",
    "    \n",
    "    start_time = time.time()\n",
    "    normal_translation = normal_decoder.translate(source_text)\n",
    "    end_time = time.time()\n",
    "\n",
    "    normal_time = end_time - start_time\n",
    "\n",
    "    # normal_total_time += normal_time\n",
    "\n",
    "    print(f\"Source: {source_text}\")\n",
    "    print(f\"Normal Translation: {normal_translation}\")\n",
    "    print(f\"Time taken: {normal_time:.2f} seconds\")\n",
    "    print(f\"Speculative Translation: {spec_translation}\")\n",
    "    print(f\"Time taken: {spec_time:.2f} seconds\")\n",
    "    print(f\"Percentage tokens accepted: {pc:.2f}%\")\n",
    "    \n",
    "    print(f\"Target: {target_text}\")\n",
    "\n",
    "    if normal_time - spec_time > -0.1:\n",
    "        spec_total_time += spec_time\n",
    "        normal_total_time += normal_time\n",
    "        total_iters += 1\n",
    "\n",
    "print(f\"\\nAverage time taken for normal decoding: {normal_total_time / total_iters:.2f} seconds\")\n",
    "print(f\"Average time taken for speculative decoding: {spec_total_time / total_iters:.2f} seconds\")\n",
    "print(f\"Average speedup over {total_iters} iterations: {normal_total_time / spec_total_time:.2f}x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize decoder\n",
    "from tqdm import tqdm\n",
    "\n",
    "speculative_decoder = SpeculativeDecoder(gamma=4, temperature=0.1, draft_model_name='google-t5/t5-large')\n",
    "normal_decoder = NormalDecoder()\n",
    "\n",
    "spec_total_time = 0\n",
    "normal_total_time = 0\n",
    "total_iters = 0\n",
    "\n",
    "for i in tqdm(en_gr_dataset['test']['translation'][:100]):\n",
    "    source_text = i['en']\n",
    "    target_text = i['de']\n",
    "    \n",
    "    # Time the translation\n",
    "    start_time = time.time()\n",
    "    spec_translation, pc = speculative_decoder.translate(source_text)\n",
    "    end_time = time.time()\n",
    "\n",
    "    spec_time = end_time - start_time\n",
    "\n",
    "    # spec_total_time += spec_time\n",
    "    \n",
    "    start_time = time.time()\n",
    "    normal_translation = normal_decoder.translate(source_text)\n",
    "    end_time = time.time()\n",
    "\n",
    "    normal_time = end_time - start_time\n",
    "\n",
    "    # normal_total_time += normal_time\n",
    "\n",
    "    print(f\"Source: {source_text}\")\n",
    "    print(f\"Normal Translation: {normal_translation}\")\n",
    "    print(f\"Time taken: {normal_time:.2f} seconds\")\n",
    "    print(f\"Speculative Translation: {spec_translation}\")\n",
    "    print(f\"Time taken: {spec_time:.2f} seconds\")\n",
    "    print(f\"Percentage tokens accepted: {pc:.2f}%\")\n",
    "    \n",
    "    print(f\"Target: {target_text}\")\n",
    "\n",
    "    if normal_time - spec_time > -0.1:\n",
    "        spec_total_time += spec_time\n",
    "        normal_total_time += normal_time\n",
    "        total_iters += 1\n",
    "\n",
    "print(f\"\\nAverage time taken for normal decoding: {normal_total_time / total_iters:.2f} seconds\")\n",
    "print(f\"Average time taken for speculative decoding: {spec_total_time / total_iters:.2f} seconds\")\n",
    "print(f\"Average speedup over {total_iters} iterations: {normal_total_time / spec_total_time:.2f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Online Spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OnlineSpeculativeDecoder:\n",
    "    def __init__(\n",
    "        self,\n",
    "        target_model_name = \"google-t5/t5-large\",\n",
    "        draft_model_name = \"google-t5/t5-small\",\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "        gamma = 4,\n",
    "        update_interval=2,  # Update draft model after every `update_interval` iterations\n",
    "        buffer_size_threshold=2,  # Buffer size threshold for updates\n",
    "        time_threshold=2,  # Time threshold (in seconds) for updates\n",
    "    ):\n",
    "        self.device = device\n",
    "        self.gamma = gamma\n",
    "        self.update_interval = update_interval\n",
    "        self.buffer_size_threshold = buffer_size_threshold\n",
    "        self.time_threshold = time_threshold\n",
    "        self.last_update_time = time.time()  # Track last update time\n",
    "\n",
    "        self.tokenizer = T5Tokenizer.from_pretrained(target_model_name)\n",
    "        self.target_model = T5ForConditionalGeneration.from_pretrained(target_model_name).to(device)\n",
    "        self.draft_model = T5ForConditionalGeneration.from_pretrained(draft_model_name).to(device)\n",
    "\n",
    "        self.target_model.eval()\n",
    "        self.draft_model.eval()\n",
    "\n",
    "        # Buffers for storing token proposals and updates\n",
    "        self.replay_buffer = []\n",
    "        self.temp_buffer = []  # Temporary buffer for a single request\n",
    "\n",
    "        # Counter for iteration tracking\n",
    "        self.iteration_count = 0\n",
    "\n",
    "    def get_draft_logits(\n",
    "        self,\n",
    "        input_ids: torch.Tensor,\n",
    "        attention_mask: torch.Tensor,\n",
    "        decoder_input_ids: torch.Tensor,\n",
    "        gamma: int\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"Get draft logits for gamma tokens\"\"\"\n",
    "        draft_tokens = []\n",
    "        draft_probs = []\n",
    "        current_decoder_ids = decoder_input_ids\n",
    "\n",
    "        # Generate gamma tokens from the draft model\n",
    "        for _ in range(gamma):\n",
    "            with torch.no_grad():\n",
    "                outputs = self.draft_model(\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    decoder_input_ids=current_decoder_ids,\n",
    "                    return_dict=True\n",
    "                )\n",
    "                logits = outputs.logits[:, -1, :]  # Get logits for last position\n",
    "                probs = F.softmax(logits, dim=-1)\n",
    "\n",
    "                # Sample token\n",
    "                token_id = torch.multinomial(probs, num_samples=1).squeeze(-1)\n",
    "                prob = probs.gather(-1, token_id.unsqueeze(-1)).squeeze(-1)\n",
    "\n",
    "                draft_tokens.append(token_id.item())\n",
    "                draft_probs.append(prob.item())\n",
    "\n",
    "                # Update decoder inputs for next iteration\n",
    "                current_decoder_ids = torch.cat(\n",
    "                    [current_decoder_ids, token_id.view(1, 1)],\n",
    "                    dim=1\n",
    "                )\n",
    "\n",
    "                if token_id.item() == self.tokenizer.eos_token_id:\n",
    "                    break\n",
    "\n",
    "        return draft_tokens, draft_probs, current_decoder_ids, outputs.logits\n",
    "\n",
    "    def get_target_probs(\n",
    "        self,\n",
    "        input_ids: torch.Tensor,\n",
    "        attention_mask: torch.Tensor,\n",
    "        decoder_input_ids: torch.Tensor,\n",
    "        draft_tokens: torch.Tensor\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"Get target probabilities for the draft tokens in parallel.\"\"\"\n",
    "        with torch.no_grad():\n",
    "            # Add draft tokens to decoder input\n",
    "            full_decoder_ids = torch.cat([decoder_input_ids, draft_tokens.unsqueeze(0)], dim=1)\n",
    "\n",
    "            outputs = self.target_model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                decoder_input_ids=full_decoder_ids,\n",
    "                return_dict=True\n",
    "            )\n",
    "\n",
    "            # Get probabilities for positions before each draft token\n",
    "            logits = outputs.logits[:, -(len(draft_tokens) + 1):-1, :]\n",
    "            target_probs = F.softmax(logits, dim=-1)\n",
    "\n",
    "            return target_probs.squeeze(0), outputs.logits\n",
    "        \n",
    "    def get_logits(self, model, input_ids, attention_mask):\n",
    "        return model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "        ).logits\n",
    "\n",
    "    def verify_tokens(\n",
    "        self,\n",
    "        target_probs: torch.Tensor,\n",
    "        draft_tokens: torch.Tensor,\n",
    "        draft_probs: torch.Tensor,\n",
    "    ) -> int:\n",
    "        \"\"\"Determine number of accepted tokens\"\"\"\n",
    "        # Get target probabilities for the draft tokens\n",
    "        target_probs_draft_tokens = target_probs.gather(\n",
    "            -1,\n",
    "            draft_tokens.unsqueeze(-1)\n",
    "        ).squeeze(-1)\n",
    "\n",
    "        # Calculate acceptance ratios\n",
    "        acceptance_ratios = target_probs_draft_tokens / draft_probs\n",
    "\n",
    "        # Sample uniform random numbers\n",
    "        random_nums = torch.rand_like(acceptance_ratios)\n",
    "\n",
    "        # Find number of accepted tokens\n",
    "        # Accept if random number < min(1, target_prob / draft_prob)\n",
    "        accepts = random_nums < torch.minimum(\n",
    "            torch.ones_like(acceptance_ratios),\n",
    "            acceptance_ratios\n",
    "        )\n",
    "\n",
    "        # Find first rejection\n",
    "        try:\n",
    "            n_accepted = torch.where(~accepts)[0][0].item()\n",
    "        except:\n",
    "            n_accepted = len(accepts)\n",
    "\n",
    "        return n_accepted\n",
    "\n",
    "        # accepted_tokens = []\n",
    "        # for i in range(len(draft_tokens)):\n",
    "        #     if target_probs[i] / draft_probs[i] > torch.rand(1).item():\n",
    "        #         accepted_tokens.append(draft_tokens[i])\n",
    "        #     else:\n",
    "        #         break # Stop if token is not accepted\n",
    "\n",
    "        # return len(accepted_tokens)\n",
    "    \n",
    "    # TODO: verify this, might need to do some window size thing\n",
    "    def update_draft_model(self):\n",
    "        \"\"\"Update draft model with the replay buffer.\"\"\"\n",
    "        if len(self.replay_buffer) == 0:\n",
    "            return\n",
    "\n",
    "        # Get draft tokens, draft and target probabilities from the replay buffer\n",
    "        # draft_tokens = torch.tensor([x[0] for x in self.replay_buffer], device=self.device)\n",
    "        # print(self.replay_buffer[0][0])\n",
    "        # draft_probs = self.replay_buffer[:, 0]\n",
    "        # target_probs = self.replay_buffer[:, 1]\n",
    "        draft_probs = torch.stack([x[0][0] for x in self.replay_buffer], dim=0)\n",
    "        target_probs = torch.stack([x[1][0] for x in self.replay_buffer], dim=0)\n",
    "\n",
    "        self.draft_model.train()\n",
    "\n",
    "        # for param in self.draft_model.parameters():\n",
    "        #     print(param.requires_grad)\n",
    "\n",
    "\n",
    "        # criterion = torch.nn.CrossEntropyLoss()\n",
    "        # print(draft_probs.shape, target_probs.shape)\n",
    "        loss = self.soft_cross_entropy(draft_probs, target_probs)\n",
    "        print(\"Loss grad_fn:\", loss.grad_fn)\n",
    "        print(\"Draft probs grad_fn:\", draft_probs.grad_fn)\n",
    "        print(\"Target probs grad_fn:\", target_probs.grad_fn)\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        self.draft_model.eval()\n",
    "\n",
    "        # Clear the replay buffer\n",
    "        self.replay_buffer = []\n",
    "\n",
    "    def soft_cross_entropy(self, predicts, targets, padding_mask=None):\n",
    "        predict_log_prob = torch.nn.functional.log_softmax(predicts, dim=-1)\n",
    "        targets_prob = torch.nn.functional.softmax(targets, dim=-1)\n",
    "        entropy = -targets_prob * predict_log_prob\n",
    "        # expand_mask = padding_mask.unsqueeze(-1).expand_as(entropy)\n",
    "        # entropy.masked_fill_(expand_mask, 0)\n",
    "        # mean_entropy = entropy.sum() / (~padding_mask).sum()\n",
    "        return entropy\n",
    "\n",
    "    def translate(\n",
    "        self,\n",
    "        source_text: str,\n",
    "        max_length: int = 128\n",
    "    ) -> str:\n",
    "        \"\"\"Translate source text using speculative decoding.\"\"\"\n",
    "        # Encode source text\n",
    "        encoder_inputs = self.tokenizer(\n",
    "            f\"translate English to German: {source_text}\",\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True\n",
    "        ).to(self.device)\n",
    "\n",
    "        # Initialize with start token\n",
    "        decoder_input_ids = torch.tensor([[self.tokenizer.pad_token_id]], device=self.device)\n",
    "\n",
    "        self.iteration_count = 0\n",
    "        self.replay_buffer = []\n",
    "\n",
    "        while decoder_input_ids.shape[1] < max_length:\n",
    "            self.temp_buffer = []\n",
    "\n",
    "            while decoder_input_ids.shape[1] < max_length:\n",
    "                # Get draft tokens autoregressively\n",
    "                # print(\"Encoder Inputs\", encoder_inputs.input_ids.shape)\n",
    "                draft_tokens, draft_probs, proposed_decoder_ids, draft_logits = self.get_draft_logits(\n",
    "                    encoder_inputs.input_ids,\n",
    "                    encoder_inputs.attention_mask,\n",
    "                    decoder_input_ids,\n",
    "                    self.gamma\n",
    "                )\n",
    "\n",
    "                draft_tokens = torch.tensor(draft_tokens, device=self.device)\n",
    "                draft_probs = torch.tensor(draft_probs, device=self.device)\n",
    "\n",
    "                if len(draft_tokens) == 0:\n",
    "                    break\n",
    "\n",
    "                # Get target probabilities in parallel\n",
    "                target_probs, target_logits = self.get_target_probs(\n",
    "                    encoder_inputs.input_ids,\n",
    "                    encoder_inputs.attention_mask,\n",
    "                    decoder_input_ids,\n",
    "                    draft_tokens\n",
    "                )\n",
    "                # print(draft_logits.shape, target_logits.shape)\n",
    "\n",
    "                # Verify tokens\n",
    "                n_accepted = self.verify_tokens(target_probs, draft_tokens, draft_probs)\n",
    "\n",
    "                # Accept verified tokens\n",
    "                if n_accepted > 0:\n",
    "                    decoder_input_ids = torch.cat([\n",
    "                        decoder_input_ids,\n",
    "                        draft_tokens[:n_accepted].unsqueeze(0)\n",
    "                    ], dim=1)\n",
    "\n",
    "                # # If rejection or no acceptance, sample one token from target\n",
    "                # if n_accepted < len(draft_tokens):\n",
    "                #     with torch.no_grad():\n",
    "                #         outputs = self.target_model(\n",
    "                #             input_ids=encoder_inputs.input_ids,\n",
    "                #             attention_mask=encoder_inputs.attention_mask,\n",
    "                #             decoder_input_ids=decoder_input_ids,\n",
    "                #             return_dict=True\n",
    "                #         )\n",
    "                #         logits = outputs.logits[:, -1, :]\n",
    "                #         probs = F.softmax(logits, dim=-1)\n",
    "                #         token_id = torch.multinomial(probs, num_samples=1)\n",
    "                #         decoder_input_ids = torch.cat([decoder_input_ids, token_id], dim=1)\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    # print(target_logits.shape, draft_logits.shape)\n",
    "                    probs = target_logits[:, -1, :] #- draft_logits[:, -1, :]\n",
    "                    probs = F.softmax(probs, dim=-1)\n",
    "                    token_id = torch.multinomial(probs, num_samples=1)\n",
    "                    # print(probs.shape, token_id.shape)\n",
    "                    decoder_input_ids = torch.cat([decoder_input_ids, token_id], dim=1)\n",
    "\n",
    "                # Check for end of sequence\n",
    "                if decoder_input_ids[0][-1].item() == self.tokenizer.eos_token_id:\n",
    "                    break\n",
    "                \n",
    "                # TODO: Update buffer with draft and target logits of the first rejected token, verify implementation\n",
    "                # rejected_tokens = draft_tokens[n_accepted]\n",
    "                if n_accepted < len(draft_tokens):\n",
    "                    # rejected_prob_draft = draft_logits[:, n_accepted, :]\n",
    "                    # rejected_prob_target = target_logits[:, n_accepted, :]\n",
    "\n",
    "                    self.temp_buffer.append((draft_logits[:, -1, :], target_logits[:, -1, :]))\n",
    "\n",
    "                # Check for end of sequence\n",
    "                if decoder_input_ids[0][-1].item() == self.tokenizer.eos_token_id:\n",
    "                    break\n",
    "\n",
    "            self.replay_buffer.extend(self.temp_buffer)\n",
    "            self.iteration_count += 1\n",
    "\n",
    "            if self.iteration_count % self.update_interval == 0:\n",
    "                self.update_draft_model()\n",
    "                self.iteration_count = 0\n",
    "\n",
    "        # Decode translation\n",
    "        translation = self.tokenizer.decode(\n",
    "            decoder_input_ids[0],\n",
    "            skip_special_tokens=True,\n",
    "            clean_up_tokenization_spaces=True\n",
    "        )\n",
    "        return translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize decoder\n",
    "online_decoder = OnlineSpeculativeDecoder()\n",
    "\n",
    "# Example translation\n",
    "source_text = \"In a world where technology evolves at an unprecedented pace, individuals and organizations must adapt quickly to the rapid advancements in artificial intelligence, machine learning, and automation, ensuring that ethical considerations, environmental sustainability, and equitable access to resources are prioritized to create a future that benefits all of humanity.\"\n",
    "\n",
    "# Time the translation\n",
    "start_time = time.time()\n",
    "translation = online_decoder.translate(source_text)\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"Source: {source_text}\")\n",
    "print(f\"Translation: {translation}\\n\")\n",
    "print(f\"Time taken: {end_time - start_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Online V2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OnlineSpeculativeDecoder:\n",
    "    def __init__(\n",
    "        self,\n",
    "        target_model_name = \"google-t5/t5-large\",\n",
    "        draft_model_name = \"google-t5/t5-small\",\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "        gamma = 4,\n",
    "        update_interval=2,  # Update draft model after every `update_interval` iterations\n",
    "        buffer_size_threshold=2,  # Buffer size threshold for updates\n",
    "        time_threshold=2,  # Time threshold (in seconds) for updates\n",
    "    ):\n",
    "        self.device = device\n",
    "        self.gamma = gamma\n",
    "        self.update_interval = update_interval\n",
    "        self.buffer_size_threshold = buffer_size_threshold\n",
    "        self.time_threshold = time_threshold\n",
    "        self.last_update_time = time.time()  # Track last update time\n",
    "\n",
    "        self.tokenizer = T5Tokenizer.from_pretrained(target_model_name)\n",
    "        self.target_model = T5ForConditionalGeneration.from_pretrained(target_model_name).to(device)\n",
    "        self.draft_model = T5ForConditionalGeneration.from_pretrained(draft_model_name).to(device)\n",
    "\n",
    "        self.target_model.eval()\n",
    "        self.draft_model.eval()\n",
    "\n",
    "        # Buffers for storing token proposals and updates\n",
    "        self.replay_buffer = []\n",
    "        self.temp_buffer = []  # Temporary buffer for a single request\n",
    "\n",
    "        # Counter for iteration tracking\n",
    "        self.iteration_count = 0\n",
    "\n",
    "    def get_draft_logits(\n",
    "        self,\n",
    "        input_ids: torch.Tensor,\n",
    "        attention_mask: torch.Tensor,\n",
    "        decoder_input_ids: torch.Tensor,\n",
    "        gamma: int\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"Get draft logits for gamma tokens\"\"\"\n",
    "        draft_tokens = []\n",
    "        draft_probs = []\n",
    "        current_decoder_ids = decoder_input_ids\n",
    "\n",
    "        # Generate gamma tokens from the draft model\n",
    "        for _ in range(gamma):\n",
    "            with torch.no_grad():\n",
    "                outputs = self.draft_model(\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    decoder_input_ids=current_decoder_ids,\n",
    "                    return_dict=True\n",
    "                )\n",
    "                logits = outputs.logits[:, -1, :]  # Get logits for last position\n",
    "                probs = F.softmax(logits, dim=-1)\n",
    "\n",
    "                # Sample token\n",
    "                token_id = torch.multinomial(probs, num_samples=1).squeeze(-1)\n",
    "                prob = probs.gather(-1, token_id.unsqueeze(-1)).squeeze(-1)\n",
    "\n",
    "                draft_tokens.append(token_id.item())\n",
    "                draft_probs.append(prob.item())\n",
    "\n",
    "                # Update decoder inputs for next iteration\n",
    "                current_decoder_ids = torch.cat(\n",
    "                    [current_decoder_ids, token_id.view(1, 1)],\n",
    "                    dim=1\n",
    "                )\n",
    "\n",
    "                if token_id.item() == self.tokenizer.eos_token_id:\n",
    "                    break\n",
    "\n",
    "        return draft_tokens, draft_probs, current_decoder_ids, outputs.logits\n",
    "\n",
    "    def get_target_probs(\n",
    "        self,\n",
    "        input_ids: torch.Tensor,\n",
    "        attention_mask: torch.Tensor,\n",
    "        decoder_input_ids: torch.Tensor,\n",
    "        draft_tokens: torch.Tensor\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"Get target probabilities for the draft tokens in parallel.\"\"\"\n",
    "        with torch.no_grad():\n",
    "            # Add draft tokens to decoder input\n",
    "            full_decoder_ids = torch.cat([decoder_input_ids, draft_tokens.unsqueeze(0)], dim=1)\n",
    "\n",
    "            outputs = self.target_model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                decoder_input_ids=full_decoder_ids,\n",
    "                return_dict=True\n",
    "            )\n",
    "\n",
    "            # Get probabilities for positions before each draft token\n",
    "            logits = outputs.logits[:, -(len(draft_tokens) + 1):-1, :]\n",
    "            target_probs = F.softmax(logits, dim=-1)\n",
    "\n",
    "            return target_probs.squeeze(0), outputs.logits\n",
    "        \n",
    "    def get_logits(self, model, input_ids, attention_mask, decoder_input_ids):\n",
    "        return model(\n",
    "            input_ids=input_ids,\n",
    "            decoder_input_ids=decoder_input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "        ).logits\n",
    "\n",
    "    def verify_tokens(\n",
    "        self,\n",
    "        target_probs: torch.Tensor,\n",
    "        draft_tokens: torch.Tensor,\n",
    "        draft_probs: torch.Tensor,\n",
    "    ) -> int:\n",
    "        \"\"\"Determine number of accepted tokens\"\"\"\n",
    "        # Get target probabilities for the draft tokens\n",
    "        target_probs_draft_tokens = target_probs.gather(\n",
    "            -1,\n",
    "            draft_tokens.unsqueeze(-1)\n",
    "        ).squeeze(-1)\n",
    "\n",
    "        # Calculate acceptance ratios\n",
    "        acceptance_ratios = target_probs_draft_tokens / draft_probs\n",
    "\n",
    "        # Sample uniform random numbers\n",
    "        random_nums = torch.rand_like(acceptance_ratios)\n",
    "\n",
    "        # Find number of accepted tokens\n",
    "        # Accept if random number < min(1, target_prob / draft_prob)\n",
    "        accepts = random_nums < torch.minimum(\n",
    "            torch.ones_like(acceptance_ratios),\n",
    "            acceptance_ratios\n",
    "        )\n",
    "\n",
    "        # Find first rejection\n",
    "        try:\n",
    "            n_accepted = torch.where(~accepts)[0][0].item()\n",
    "        except:\n",
    "            n_accepted = len(accepts)\n",
    "\n",
    "        return n_accepted\n",
    "\n",
    "        # accepted_tokens = []\n",
    "        # for i in range(len(draft_tokens)):\n",
    "        #     if target_probs[i] / draft_probs[i] > torch.rand(1).item():\n",
    "        #         accepted_tokens.append(draft_tokens[i])\n",
    "        #     else:\n",
    "        #         break # Stop if token is not accepted\n",
    "\n",
    "        # return len(accepted_tokens)\n",
    "    \n",
    "    # TODO: verify this, might need to do some window size thing\n",
    "    # def update_draft_model(self):\n",
    "    #     \"\"\"Update draft model with the replay buffer.\"\"\"\n",
    "    #     if len(self.replay_buffer) == 0:\n",
    "    #         return\n",
    "\n",
    "    #     # Get draft tokens, draft and target probabilities from the replay buffer\n",
    "    #     draft_tokens = torch.tensor([x[0] for x in self.replay_buffer], device=self.device)\n",
    "    #     target_logits = torch.tensor([x[1] for x in self.replay_buffer], device=self.device)\n",
    "\n",
    "    #     encoder_inputs = s\n",
    "    #     output = self.draft_model(\n",
    "    #         input_ids=encoder_inputs.input_ids,\n",
    "    #         attention_mask=encoder_inputs.attention_mask,\n",
    "    #         decoder_input_ids=decoder_input_ids,\n",
    "    #         return_dict=True\n",
    "    #     )\n",
    "\n",
    "    def pad_to_2d(self, tensor_list, pad_token_id, max_len=None):\n",
    "        if not isinstance(tensor_list[0], torch.Tensor):\n",
    "            tensor_list = [torch.tensor(t).reshape(1, -1) for t in tensor_list]\n",
    "        if max_len is None:\n",
    "            max_len = max([t.shape[-1] for t in tensor_list])\n",
    "        assert max_len > 0\n",
    "\n",
    "        # Pad each tensor to the max length and stack them to form a 2D tensor\n",
    "        result = torch.cat(\n",
    "            [\n",
    "                torch.nn.functional.pad(\n",
    "                    tensor, (0, max_len - tensor.shape[-1]),\n",
    "                    value=pad_token_id\n",
    "                )\n",
    "                for tensor in tensor_list\n",
    "            ],\n",
    "            dim=0\n",
    "        )\n",
    "        return result\n",
    "        \n",
    "\n",
    "    def soft_cross_entropy(self, predicts, targets, padding_mask=None):\n",
    "        predict_log_prob = torch.nn.functional.log_softmax(predicts, dim=-1)\n",
    "        targets_prob = torch.nn.functional.softmax(targets, dim=-1)\n",
    "        entropy = -targets_prob * predict_log_prob\n",
    "        expand_mask = padding_mask.unsqueeze(-1).expand_as(entropy)\n",
    "        entropy.masked_fill_(expand_mask, 0)\n",
    "        mean_entropy = entropy.sum() / (~padding_mask).sum()\n",
    "        return mean_entropy\n",
    "\n",
    "    def translate_dataset(\n",
    "        self,\n",
    "        sentences: List[str],\n",
    "        max_length: int = 128\n",
    "    ) -> List[str]:\n",
    "        \"\"\"Translate dataset using online speculative decoding.\"\"\"\n",
    "\n",
    "        self.iteration_count = 0\n",
    "        self.replay_buffer = []\n",
    "\n",
    "        translated_data = []\n",
    "\n",
    "        for source_text in sentences:\n",
    "            # Encode source text\n",
    "            encoder_inputs = self.tokenizer(\n",
    "                f\"translate English to German: {source_text}\",\n",
    "                return_tensors=\"pt\",\n",
    "                padding=True\n",
    "            ).to(self.device)\n",
    "\n",
    "            # Initialize with start token\n",
    "            decoder_input_ids = torch.tensor([[self.tokenizer.pad_token_id]], device=self.device)\n",
    "            self.temp_buffer = []\n",
    "\n",
    "            while decoder_input_ids.shape[1] < max_length:\n",
    "\n",
    "                # Check for end of sequence\n",
    "                if decoder_input_ids[0][-1].item() == self.tokenizer.eos_token_id:\n",
    "                    break\n",
    "\n",
    "                # Get draft tokens autoregressively\n",
    "                # print(\"Encoder Inputs\", encoder_inputs.input_ids.shape)\n",
    "                draft_tokens, draft_probs, proposed_decoder_ids, draft_logits = self.get_draft_logits(\n",
    "                    encoder_inputs.input_ids,\n",
    "                    encoder_inputs.attention_mask,\n",
    "                    decoder_input_ids,\n",
    "                    self.gamma\n",
    "                )\n",
    "\n",
    "                draft_tokens = torch.tensor(draft_tokens, device=self.device)\n",
    "                draft_probs = torch.tensor(draft_probs, device=self.device)\n",
    "\n",
    "                if len(draft_tokens) == 0:\n",
    "                    break\n",
    "\n",
    "                # Get target probabilities in parallel\n",
    "                target_probs, target_logits = self.get_target_probs(\n",
    "                    encoder_inputs.input_ids,\n",
    "                    encoder_inputs.attention_mask,\n",
    "                    decoder_input_ids,\n",
    "                    draft_tokens\n",
    "                )\n",
    "                # print(draft_logits.shape, target_logits.shape)\n",
    "\n",
    "                # Verify tokens\n",
    "                n_accepted = self.verify_tokens(target_probs, draft_tokens, draft_probs)\n",
    "\n",
    "                # Accept verified tokens\n",
    "                if n_accepted > 0:\n",
    "                    decoder_input_ids = torch.cat([\n",
    "                        decoder_input_ids,\n",
    "                        draft_tokens[:n_accepted].unsqueeze(0)\n",
    "                    ], dim=1)\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    # print(target_logits.shape, draft_logits.shape)\n",
    "                    probs = target_logits[:, -1, :] #- draft_logits[:, -1, :]\n",
    "                    probs = F.softmax(probs, dim=-1)\n",
    "                    token_id = torch.multinomial(probs, num_samples=1)\n",
    "                    # print(probs.shape, token_id.shape)\n",
    "                    decoder_input_ids = torch.cat([decoder_input_ids, token_id], dim=1)\n",
    "                \n",
    "                # rejected_tokens = draft_tokens[n_accepted]\n",
    "                if n_accepted < len(draft_tokens):\n",
    "\n",
    "                    self.temp_buffer.append((encoder_inputs, decoder_input_ids, target_logits, n_accepted))\n",
    "\n",
    "            self.replay_buffer.extend(self.temp_buffer)\n",
    "            self.iteration_count += 1\n",
    "\n",
    "            if self.iteration_count % self.update_interval == 0:\n",
    "                # self.update_draft_model()\n",
    "                self.draft_model.train()\n",
    "                \n",
    "                # finetune over collected tokens and logits\n",
    "                encoder_input_ids = self.pad_to_2d([x[0].input_ids for x in self.replay_buffer], 0)\n",
    "                encoder_attention_mask = torch.stack([x[0].attention_mask[0] for x in self.replay_buffer], dim=0)\n",
    "                decoder_input_ids = self.pad_to_2d([x[1] for x in self.replay_buffer], 0, 512)\n",
    "\n",
    "                print(encoder_input_ids.shape, encoder_attention_mask.shape, decoder_input_ids.shape)\n",
    "\n",
    "                target_logits = [x[2] for x in self.replay_buffer]\n",
    "                for i in range(len(target_logits)):\n",
    "                    temp = torch.zeros(1, 32128, device=self.device).repeat(512 - target_logits[i].shape[1], 1).unsqueeze(0)\n",
    "                    target_logits[i] = torch.cat([target_logits[i], temp], dim=1)\n",
    "\n",
    "                n_accepted_tokens = [x[3] for x in self.replay_buffer]\n",
    "\n",
    "                # CUDA out of memory error\n",
    "                draft_logits = self.get_logits(self.draft_model, encoder_input_ids, encoder_attention_mask, decoder_input_ids).float()\n",
    "\n",
    "                # need to get loss only using the wrong tokens\n",
    "                # TODO: check if we need to ignore the pad tokens in the mask\n",
    "                mask = torch.ones_like(decoder_input_ids, dtype=torch.bool)\n",
    "                for i in range(len(n_accepted_tokens)):\n",
    "                    mask[i, n_accepted_tokens[i]:] = False\n",
    "                loss = self.soft_cross_entropy(draft_logits, target_logits, mask)\n",
    "                loss.backward()\n",
    "\n",
    "                self.draft_model.eval()\n",
    "                self.replay_buffer = []\n",
    "                self.iteration_count = 0\n",
    "\n",
    "            # Decode translation\n",
    "            translation = self.tokenizer.decode(\n",
    "                decoder_input_ids[0],\n",
    "                skip_special_tokens=True,\n",
    "                clean_up_tokenization_spaces=True\n",
    "            )\n",
    "            translated_data.append(translation)\n",
    "        return translated_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize decoder\n",
    "online_decoder = OnlineSpeculativeDecoder()\n",
    "\n",
    "# Example translation\n",
    "source_text = \"In a world where technology evolves at an unprecedented pace, individuals and organizations must adapt quickly to the rapid advancements in artificial intelligence, machine learning, and automation, ensuring that ethical considerations, environmental sustainability, and equitable access to resources are prioritized to create a future that benefits all of humanity.\"\n",
    "sents = [source_text] * 10\n",
    "# Time the translation\n",
    "start_time = time.time()\n",
    "translation = online_decoder.translate_dataset(sents)\n",
    "end_time = time.time()\n",
    "\n",
    "for i, sent in enumerate(sents):\n",
    "    print(f\"Source: {sent}\")\n",
    "    print(f\"Translation: {translation[i]}\\n\")\n",
    "print(f\"Time taken: {end_time - start_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
