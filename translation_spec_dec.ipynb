{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "if not os.path.exists(\"/scratch/sarthak\"):\n",
    "    os.makedirs(\"/scratch/sarthak\")\n",
    "\n",
    "os.environ[\"HF_HOME\"] = \"/scratch/sarthak/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EdqllqRvz8YX"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import T5ForConditionalGeneration, T5Tokenizer, AutoModelForSeq2SeqLM\n",
    "from typing import List, Tuple, Optional\n",
    "import time\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "import torch\n",
    "import datasets\n",
    "from transformers import T5Tokenizer\n",
    "\n",
    "# # WMT16 EN-DE dataset\n",
    "# def preprocess_function_ende(examples, tokenizer, args, train_args, prefix=\"translate English to German: \"):\n",
    "#     if train_args.debug:\n",
    "#         all_text = examples['translation'][:2]\n",
    "#     else:\n",
    "#         all_text = examples['translation']\n",
    "        \n",
    "#     inputs = []\n",
    "#     targets = []\n",
    "#     for excerpt in all_text:\n",
    "#         en_text = prefix + excerpt['en']\n",
    "#         de_text = excerpt['de']\n",
    "\n",
    "#         inputs.append(en_text)\n",
    "#         targets.append(de_text)\n",
    "            \n",
    "#     padding = 'max_length'\n",
    "#     model_inputs = tokenizer(\n",
    "#         inputs,\n",
    "#         max_length=args.source_max_length,\n",
    "#         padding=padding,\n",
    "#         truncation=True,\n",
    "#         return_tensors=\"pt\",\n",
    "#     )\n",
    "#     # Tokenize targets with the `text_target` keyword argument\n",
    "#     labels = tokenizer(\n",
    "#         text_target=targets,\n",
    "#         max_length=args.train_target_max_length,\n",
    "#         padding=padding,\n",
    "#         truncation=True,\n",
    "    #     return_tensors=\"pt\",\n",
    "    # )\n",
    "\n",
    "    # if padding == \"max_length\":\n",
    "    #             labels[\"input_ids\"] = [\n",
    "    #                 [(l if l != tokenizer.pad_token_id else -100) for l in label] for label in labels[\"input_ids\"]\n",
    "    #             ]\n",
    "\n",
    "    # model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    # model_inputs[\"decoder_attention_mask\"] = labels[\"attention_mask\"]\n",
    "    # return model_inputs\n",
    "\n",
    "en_gr_dataset = datasets.load_dataset('wmt16', 'de-en')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Base Speculative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v3kozZQNyBBQ"
   },
   "outputs": [],
   "source": [
    "class SpeculativeDecoder:\n",
    "    def __init__(\n",
    "        self,\n",
    "        target_model_name = \"google-t5/t5-3b\",\n",
    "        draft_model_name = \"google-t5/t5-small\",\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "        gamma = 4\n",
    "    ):\n",
    "        self.device = device\n",
    "        self.gamma = gamma\n",
    "\n",
    "        self.tokenizer = T5Tokenizer.from_pretrained(target_model_name)\n",
    "        \n",
    "        # self.target_model = T5ForConditionalGeneration.from_pretrained(target_model_name).to(device)\n",
    "        self.target_model = AutoModelForSeq2SeqLM.from_pretrained(target_model_name, device_map='auto')\n",
    "\n",
    "        self.draft_model = T5ForConditionalGeneration.from_pretrained(draft_model_name).to(device)\n",
    "\n",
    "        self.target_model.eval()\n",
    "        self.draft_model.eval()\n",
    "\n",
    "    def get_draft_logits(\n",
    "        self,\n",
    "        input_ids: torch.Tensor,\n",
    "        attention_mask: torch.Tensor,\n",
    "        decoder_input_ids: torch.Tensor,\n",
    "        gamma: int\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"Get draft logits for gamma tokens\"\"\"\n",
    "        draft_tokens = []\n",
    "        draft_probs = []\n",
    "        current_decoder_ids = decoder_input_ids\n",
    "\n",
    "        # Generate gamma tokens from the draft model\n",
    "        for _ in range(gamma):\n",
    "            with torch.no_grad():\n",
    "                outputs = self.draft_model(\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    decoder_input_ids=current_decoder_ids,\n",
    "                    return_dict=True\n",
    "                )\n",
    "                logits = outputs.logits[:, -1, :]  # Get logits for last position\n",
    "                probs = F.softmax(logits, dim=-1)\n",
    "\n",
    "                # Sample token\n",
    "                token_id = torch.multinomial(probs, num_samples=1).squeeze(-1)\n",
    "                prob = probs.gather(-1, token_id.unsqueeze(-1)).squeeze(-1)\n",
    "\n",
    "                draft_tokens.append(token_id.item())\n",
    "                draft_probs.append(prob.item())\n",
    "\n",
    "                # Update decoder inputs for next iteration\n",
    "                current_decoder_ids = torch.cat(\n",
    "                    [current_decoder_ids, token_id.view(1, 1)],\n",
    "                    dim=1\n",
    "                )\n",
    "\n",
    "                if token_id.item() == self.tokenizer.eos_token_id:\n",
    "                    break\n",
    "\n",
    "        return draft_tokens, draft_probs#, current_decoder_ids, outputs.logits\n",
    "\n",
    "    # def get_target_probs(\n",
    "    #     self,\n",
    "    #     input_ids: torch.Tensor,\n",
    "    #     attention_mask: torch.Tensor,\n",
    "    #     decoder_input_ids: torch.Tensor,\n",
    "    #     draft_tokens: torch.Tensor\n",
    "    # ) -> torch.Tensor:\n",
    "    #     \"\"\"Get target probabilities for the draft tokens in parallel.\"\"\"\n",
    "    #     with torch.no_grad():\n",
    "    #         # Add draft tokens to decoder input\n",
    "    #         # full_decoder_ids = torch.cat([decoder_input_ids, draft_tokens.unsqueeze(0)], dim=1)\n",
    "\n",
    "    #         full_decoder_ids = [decoder_input_ids]\n",
    "    #         for i in range(len(draft_tokens)):\n",
    "    #             x = torch.cat([decoder_input_ids, draft_tokens.unsqueeze(0)[:, :i+1]], dim=1)\n",
    "    #             full_decoder_ids.append(x)\n",
    "\n",
    "    #         maxlen = max([x.shape[1] for x in full_decoder_ids])\n",
    "\n",
    "    #         padded_decoder_ids = torch.stack([torch.tensor(\n",
    "    #             F.pad(x, (0, maxlen - x.shape[1]), value=self.tokenizer.pad_token_id)[0]\n",
    "    #         , device=self.device) for x in full_decoder_ids])\n",
    "\n",
    "    #         batch_size = padded_decoder_ids.shape[0]\n",
    "    #         input_ids_batched = input_ids.repeat(batch_size, 1)\n",
    "    #         attention_mask_batched = attention_mask.repeat(batch_size, 1)\n",
    "\n",
    "    #         # make it a triangular attention mask\n",
    "\n",
    "\n",
    "    #         # print(input_ids_batched.shape, attention_mask_batched.shape, padded_decoder_ids.shape)\n",
    "\n",
    "    #         # outputs = self.target_model(\n",
    "    #         #     input_ids=input_ids,\n",
    "    #         #     attention_mask=attention_mask,\n",
    "    #         #     decoder_input_ids=full_decoder_ids,\n",
    "    #         #     return_dict=True\n",
    "    #         # )\n",
    "    #         outputs = self.target_model(\n",
    "    #             input_ids=input_ids_batched,\n",
    "    #             attention_mask=attention_mask_batched,\n",
    "    #             decoder_input_ids=padded_decoder_ids,\n",
    "    #             # decoder_attention_mask=torch.triu(\n",
    "    #             #     torch.zeros((padded_decoder_ids.shape[0], padded_decoder_ids.shape[1]), device=self.device)\n",
    "    #             # ),\n",
    "    #             return_dict=True\n",
    "    #         )\n",
    "    #         # print(\"passes target model\")\n",
    "\n",
    "    #         batched_logits = outputs.logits\n",
    "    #         # print(batched_logits.shape)\n",
    "    #         # 5, 5, 32128\n",
    "    #         logits = torch.zeros((batched_logits.shape[1], batched_logits.shape[2]), device=self.device)\n",
    "    #         for i in range(len(draft_tokens)):\n",
    "    #             logits[i] = batched_logits[i, i, :]\n",
    "\n",
    "    #         # print(logits.shape, batched_logits[-1].shape)\n",
    "\n",
    "    #         target_probs = F.softmax(logits, dim=-1)\n",
    "\n",
    "    #         # # Get probabilities for positions before each draft token\n",
    "    #         # logits = outputs.logits[:, -(len(draft_tokens) + 1):-1, :]\n",
    "    #         # target_probs = F.softmax(logits, dim=-1)\n",
    "\n",
    "    #         # print(batched_logits[-1].unsqueeze(0).shape)\n",
    "\n",
    "    #         return target_probs.squeeze(0), logits.unsqueeze(0)\n",
    "\n",
    "    def get_target_probs(\n",
    "        self,\n",
    "        input_ids: torch.Tensor,\n",
    "        attention_mask: torch.Tensor,\n",
    "        decoder_input_ids: torch.Tensor,\n",
    "        draft_tokens: torch.Tensor\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"Get target probabilities for the draft tokens in parallel.\"\"\"\n",
    "        with torch.no_grad():\n",
    "            # Add draft tokens to decoder input\n",
    "            full_decoder_ids = torch.cat([decoder_input_ids, draft_tokens.unsqueeze(0)], dim=1)\n",
    "\n",
    "            # print(input_ids_batched.shape, attention_mask_batched.shape, padded_decoder_ids.shape)\n",
    "\n",
    "            outputs = self.target_model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                decoder_input_ids=full_decoder_ids,\n",
    "                # decoder_attention_mask=torch.triu(\n",
    "                #     torch.zeros((full_decoder_ids.shape[0], full_decoder_ids.shape[1]), device=self.device)\n",
    "                # ),\n",
    "                return_dict=True\n",
    "            )\n",
    "\n",
    "            # Get probabilities for positions before each draft token\n",
    "            logits = outputs.logits[:, -(len(draft_tokens) + 1):-1, :]\n",
    "            target_probs = F.softmax(logits, dim=-1)\n",
    "\n",
    "            return target_probs.squeeze(0), outputs.logits\n",
    "\n",
    "    # def verify_tokens(\n",
    "    #     self,\n",
    "    #     target_probs: torch.Tensor,\n",
    "    #     draft_tokens: torch.Tensor,\n",
    "    #     draft_probs: torch.Tensor,\n",
    "    # ) -> int:\n",
    "    #     \"\"\"Determine number of accepted tokens\"\"\"\n",
    "    #     # Get target probabilities for the draft tokens\n",
    "    #     target_probs_draft_tokens = target_probs.gather(\n",
    "    #         -1,\n",
    "    #         draft_tokens.unsqueeze(-1)\n",
    "    #     ).squeeze(-1)\n",
    "\n",
    "    #     # Calculate acceptance ratios\n",
    "    #     acceptance_ratios = target_probs_draft_tokens / draft_probs\n",
    "\n",
    "    #     # Sample uniform random numbers\n",
    "    #     random_nums = torch.rand_like(acceptance_ratios)\n",
    "\n",
    "    #     # Find number of accepted tokens\n",
    "    #     # Accept if random number < min(1, target_prob / draft_prob)\n",
    "    #     accepts = random_nums < torch.minimum(\n",
    "    #         torch.ones_like(acceptance_ratios),\n",
    "    #         acceptance_ratios\n",
    "    #     )\n",
    "\n",
    "    #     # Find first rejection\n",
    "    #     try:\n",
    "    #         n_accepted = torch.where(~accepts)[0][0].item()\n",
    "    #     except:\n",
    "    #         n_accepted = len(accepts)\n",
    "\n",
    "    #     return n_accepted\n",
    "\n",
    "    def verify_tokens(\n",
    "        self,\n",
    "        target_probs: torch.Tensor,\n",
    "        draft_tokens: torch.Tensor,\n",
    "        draft_probs: torch.Tensor,\n",
    "    ) -> int:\n",
    "        \"\"\"Determine number of accepted tokens\"\"\"\n",
    "        # Get target probabilities for the draft tokens\n",
    "        target_probs_draft_tokens = target_probs.gather(\n",
    "            -1,\n",
    "            draft_tokens.unsqueeze(-1)\n",
    "        ).squeeze(-1)\n",
    "\n",
    "        # Calculate acceptance ratios\n",
    "        acceptance_ratios = target_probs_draft_tokens.float() / draft_probs\n",
    "\n",
    "        # Sample uniform random numbers \n",
    "        random_nums = torch.zeros_like(target_probs_draft_tokens).float().uniform_()\n",
    "\n",
    "        mask = random_nums > acceptance_ratios\n",
    "        num_accepted = (mask.cumsum(dim = -1) == 0).sum(dim = -1)\n",
    "\n",
    "        return num_accepted.int().item()\n",
    "\n",
    "    def translate(\n",
    "        self,\n",
    "        source_text: str,\n",
    "        max_length: int = 128\n",
    "    ) -> str:\n",
    "        \"\"\"Translate source text using speculative decoding.\"\"\"\n",
    "        # Encode source text\n",
    "        encoder_inputs = self.tokenizer(\n",
    "            f\"translate English to German: {source_text}\",\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True\n",
    "        ).to(self.device)\n",
    "\n",
    "        # Initialize with start token\n",
    "        decoder_input_ids = torch.tensor([[self.tokenizer.pad_token_id]], device=self.device)\n",
    "\n",
    "        while decoder_input_ids.shape[1] < max_length:\n",
    "            # Get draft tokens autoregressively\n",
    "            draft_tokens, draft_probs = self.get_draft_logits(\n",
    "                encoder_inputs.input_ids,\n",
    "                encoder_inputs.attention_mask,\n",
    "                decoder_input_ids,\n",
    "                self.gamma\n",
    "            )\n",
    "\n",
    "            draft_tokens = torch.tensor(draft_tokens, device=self.device)\n",
    "            draft_probs = torch.tensor(draft_probs, device=self.device)\n",
    "\n",
    "            if len(draft_tokens) == 0:\n",
    "                raise ValueError(\"Draft tokens not generated.\")\n",
    "\n",
    "            # Get target probabilities in parallel\n",
    "            # start = time.time()\n",
    "            target_probs, target_logits = self.get_target_probs(\n",
    "                encoder_inputs.input_ids,\n",
    "                encoder_inputs.attention_mask,\n",
    "                decoder_input_ids,\n",
    "                draft_tokens\n",
    "            )\n",
    "            # print(\"Time taken for target probs: \", time.time() - start)\n",
    "\n",
    "            # Verify tokens\n",
    "            n_accepted = self.verify_tokens(target_probs, draft_tokens, draft_probs)\n",
    "            # print(n_accepted)\n",
    "\n",
    "            # Accept verified tokens\n",
    "            if n_accepted > 0:\n",
    "                decoder_input_ids = torch.cat([\n",
    "                    decoder_input_ids,\n",
    "                    draft_tokens[:n_accepted].unsqueeze(0)\n",
    "                ], dim=1)\n",
    "                \n",
    "            with torch.no_grad():\n",
    "                n_rejected = self.gamma - n_accepted\n",
    "                if n_rejected > 0:\n",
    "                    probs = target_logits[:, -n_rejected, :] #- draft_logits[:, 1-n_rejected, :]\n",
    "                else:\n",
    "                    probs = target_logits[:, -1, :]\n",
    "                    \n",
    "                probs = F.softmax(probs, dim=-1)\n",
    "                token_id = torch.multinomial(probs, num_samples=1)\n",
    "\n",
    "                decoder_input_ids = torch.cat([decoder_input_ids, token_id], dim=1)\n",
    "\n",
    "            # Check for end of sequence\n",
    "            if decoder_input_ids[0][-1].item() == self.tokenizer.eos_token_id:\n",
    "                break\n",
    "\n",
    "        # Decode translation\n",
    "        translation = self.tokenizer.decode(\n",
    "            decoder_input_ids[0],\n",
    "            skip_special_tokens=True,\n",
    "            clean_up_tokenization_spaces=True\n",
    "        )\n",
    "        return translation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CopfVI-mjdb4"
   },
   "outputs": [],
   "source": [
    "class NormalDecoder:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name: str = \"google-t5/t5-3b\",\n",
    "        device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "    ):\n",
    "        self.device = device\n",
    "\n",
    "        # Initialize tokenizer and model\n",
    "        self.tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "        # self.model = T5ForConditionalGeneration.from_pretrained(model_name).to(device)\n",
    "        self.model = AutoModelForSeq2SeqLM.from_pretrained(model_name, device_map='auto')\n",
    "        self.model.eval()\n",
    "\n",
    "    def get_logits(\n",
    "        self,\n",
    "        input_ids: torch.Tensor,\n",
    "        attention_mask: torch.Tensor,\n",
    "        decoder_input_ids: torch.Tensor\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"Get logits from model for the last token.\"\"\"\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                decoder_input_ids=decoder_input_ids,\n",
    "                return_dict=True\n",
    "            )\n",
    "            return outputs.logits[:, -1, :]\n",
    "\n",
    "    def sample_token(self, logits: torch.Tensor, temperature: float = 1.0) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"Sample a token from logits using temperature sampling.\"\"\"\n",
    "        if temperature == 0:\n",
    "            # Greedy sampling\n",
    "            token_id = torch.argmax(logits, dim=-1)\n",
    "            prob = torch.ones_like(token_id, dtype=torch.float)\n",
    "        else:\n",
    "            # Temperature sampling\n",
    "            probs = F.softmax(logits / temperature, dim=-1)\n",
    "            token_id = torch.multinomial(probs, num_samples=1).squeeze(-1)\n",
    "            prob = probs.gather(-1, token_id.unsqueeze(-1)).squeeze(-1)\n",
    "        return token_id, prob\n",
    "\n",
    "    def translate(\n",
    "        self,\n",
    "        source_text: str,\n",
    "        max_length: int = 128,\n",
    "        temperature: float = 0.7\n",
    "    ) -> str:\n",
    "        \"\"\"Translate source text using the normal T5 model.\"\"\"\n",
    "        # Encode source text\n",
    "        encoder_inputs = self.tokenizer(\n",
    "            f\"translate English to German: {source_text}\",\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True\n",
    "        ).to(self.device)\n",
    "\n",
    "        # Initialize decoder input with start token\n",
    "        decoder_input_ids = torch.tensor([[self.tokenizer.pad_token_id]], device=self.device)\n",
    "\n",
    "        while decoder_input_ids.shape[1] < max_length:\n",
    "            # Generate logits for the next token\n",
    "            logits = self.get_logits(\n",
    "                encoder_inputs.input_ids,\n",
    "                encoder_inputs.attention_mask,\n",
    "                decoder_input_ids\n",
    "            )\n",
    "\n",
    "            # Sample a token\n",
    "            token_id, _ = self.sample_token(logits, temperature)\n",
    "\n",
    "            # Add token to the decoder input\n",
    "            decoder_input_ids = torch.cat(\n",
    "                [decoder_input_ids, token_id.view(1, 1)],\n",
    "                dim=1\n",
    "            )\n",
    "\n",
    "            # Break if end token is generated\n",
    "            if token_id.item() == self.tokenizer.eos_token_id:\n",
    "                break\n",
    "\n",
    "        # Decode and return translation\n",
    "        translation = self.tokenizer.decode(\n",
    "            decoder_input_ids[0],\n",
    "            skip_special_tokens=True,\n",
    "            clean_up_tokenization_spaces=True\n",
    "        )\n",
    "        return translation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# speculative_decoder = SpeculativeDecoder()\n",
    "# normal_decoder = NormalDecoder()\n",
    "\n",
    "# source_text = \"The meeting was also planned to cover the conflict with the Palestinians and the disputed two state solution.\"\n",
    "\n",
    "# speculative_decoder.translate(source_text)\n",
    "# start = time.time()\n",
    "# speculative_translation = speculative_decoder.translate(source_text)\n",
    "# end = time.time()\n",
    "# print(f\"Speculative translation: {speculative_translation}\")\n",
    "# print(f\"Time taken: {end - start:.4f} seconds\")\n",
    "\n",
    "# start = time.time()\n",
    "# normal_translation = normal_decoder.translate(source_text)\n",
    "# end = time.time()\n",
    "# print(f\"Normal translation: {normal_translation}\")\n",
    "# print(f\"Time taken: {end - start:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nwtUoGDUjiSD",
    "outputId": "2a92f134-5a76-45b9-884b-51515d57fa4d"
   },
   "outputs": [],
   "source": [
    "# Initialize decoder\n",
    "from tqdm import tqdm\n",
    "\n",
    "speculative_decoder = SpeculativeDecoder()\n",
    "normal_decoder = NormalDecoder()\n",
    "\n",
    "spec_total_time = 0\n",
    "normal_total_time = 0\n",
    "\n",
    "for i in tqdm(en_gr_dataset['test']['translation'][:100]):\n",
    "    source_text = i['en']\n",
    "    target_text = i['de']\n",
    "    \n",
    "    # Time the translation\n",
    "    start_time = time.time()\n",
    "    spec_translation = speculative_decoder.translate(source_text)\n",
    "    end_time = time.time()\n",
    "\n",
    "    spec_total_time += end_time - start_time\n",
    "    \n",
    "    start_time = time.time()\n",
    "    normal_translation = normal_decoder.translate(source_text)\n",
    "    end_time = time.time()\n",
    "\n",
    "    normal_total_time += end_time - start_time\n",
    "\n",
    "    print(f\"Source: {source_text}\")\n",
    "    print(f\"Normal Translation: {normal_translation}\")\n",
    "    print(f\"Speculative Translation: {spec_translation}\")\n",
    "    print(f\"Target: {target_text}\")\n",
    "    print(f\"Time taken: {end_time - start_time:.2f} seconds\\n\")\n",
    "\n",
    "print(f\"Average time taken for normal decoding: {normal_total_time / len(en_gr_dataset['test']['translation']):.2f} seconds\")\n",
    "print(f\"Average time taken for speculative decoding: {spec_total_time / len(en_gr_dataset['test']['translation']):.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Online Spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OnlineSpeculativeDecoder:\n",
    "    def __init__(\n",
    "        self,\n",
    "        target_model_name = \"google-t5/t5-large\",\n",
    "        draft_model_name = \"google-t5/t5-small\",\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "        gamma = 4,\n",
    "        update_interval=2,  # Update draft model after every `update_interval` iterations\n",
    "        buffer_size_threshold=2,  # Buffer size threshold for updates\n",
    "        time_threshold=2,  # Time threshold (in seconds) for updates\n",
    "    ):\n",
    "        self.device = device\n",
    "        self.gamma = gamma\n",
    "        self.update_interval = update_interval\n",
    "        self.buffer_size_threshold = buffer_size_threshold\n",
    "        self.time_threshold = time_threshold\n",
    "        self.last_update_time = time.time()  # Track last update time\n",
    "\n",
    "        self.tokenizer = T5Tokenizer.from_pretrained(target_model_name)\n",
    "        self.target_model = T5ForConditionalGeneration.from_pretrained(target_model_name).to(device)\n",
    "        self.draft_model = T5ForConditionalGeneration.from_pretrained(draft_model_name).to(device)\n",
    "\n",
    "        self.target_model.eval()\n",
    "        self.draft_model.eval()\n",
    "\n",
    "        # Buffers for storing token proposals and updates\n",
    "        self.replay_buffer = []\n",
    "        self.temp_buffer = []  # Temporary buffer for a single request\n",
    "\n",
    "        # Counter for iteration tracking\n",
    "        self.iteration_count = 0\n",
    "\n",
    "    def get_draft_logits(\n",
    "        self,\n",
    "        input_ids: torch.Tensor,\n",
    "        attention_mask: torch.Tensor,\n",
    "        decoder_input_ids: torch.Tensor,\n",
    "        gamma: int\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"Get draft logits for gamma tokens\"\"\"\n",
    "        draft_tokens = []\n",
    "        draft_probs = []\n",
    "        current_decoder_ids = decoder_input_ids\n",
    "\n",
    "        # Generate gamma tokens from the draft model\n",
    "        for _ in range(gamma):\n",
    "            with torch.no_grad():\n",
    "                outputs = self.draft_model(\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    decoder_input_ids=current_decoder_ids,\n",
    "                    return_dict=True\n",
    "                )\n",
    "                logits = outputs.logits[:, -1, :]  # Get logits for last position\n",
    "                probs = F.softmax(logits, dim=-1)\n",
    "\n",
    "                # Sample token\n",
    "                token_id = torch.multinomial(probs, num_samples=1).squeeze(-1)\n",
    "                prob = probs.gather(-1, token_id.unsqueeze(-1)).squeeze(-1)\n",
    "\n",
    "                draft_tokens.append(token_id.item())\n",
    "                draft_probs.append(prob.item())\n",
    "\n",
    "                # Update decoder inputs for next iteration\n",
    "                current_decoder_ids = torch.cat(\n",
    "                    [current_decoder_ids, token_id.view(1, 1)],\n",
    "                    dim=1\n",
    "                )\n",
    "\n",
    "                if token_id.item() == self.tokenizer.eos_token_id:\n",
    "                    break\n",
    "\n",
    "        return draft_tokens, draft_probs, current_decoder_ids, outputs.logits\n",
    "\n",
    "    def get_target_probs(\n",
    "        self,\n",
    "        input_ids: torch.Tensor,\n",
    "        attention_mask: torch.Tensor,\n",
    "        decoder_input_ids: torch.Tensor,\n",
    "        draft_tokens: torch.Tensor\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"Get target probabilities for the draft tokens in parallel.\"\"\"\n",
    "        with torch.no_grad():\n",
    "            # Add draft tokens to decoder input\n",
    "            full_decoder_ids = torch.cat([decoder_input_ids, draft_tokens.unsqueeze(0)], dim=1)\n",
    "\n",
    "            outputs = self.target_model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                decoder_input_ids=full_decoder_ids,\n",
    "                return_dict=True\n",
    "            )\n",
    "\n",
    "            # Get probabilities for positions before each draft token\n",
    "            logits = outputs.logits[:, -(len(draft_tokens) + 1):-1, :]\n",
    "            target_probs = F.softmax(logits, dim=-1)\n",
    "\n",
    "            return target_probs.squeeze(0), outputs.logits\n",
    "        \n",
    "    def get_logits(self, model, input_ids, attention_mask):\n",
    "        return model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "        ).logits\n",
    "\n",
    "    def verify_tokens(\n",
    "        self,\n",
    "        target_probs: torch.Tensor,\n",
    "        draft_tokens: torch.Tensor,\n",
    "        draft_probs: torch.Tensor,\n",
    "    ) -> int:\n",
    "        \"\"\"Determine number of accepted tokens\"\"\"\n",
    "        # Get target probabilities for the draft tokens\n",
    "        target_probs_draft_tokens = target_probs.gather(\n",
    "            -1,\n",
    "            draft_tokens.unsqueeze(-1)\n",
    "        ).squeeze(-1)\n",
    "\n",
    "        # Calculate acceptance ratios\n",
    "        acceptance_ratios = target_probs_draft_tokens / draft_probs\n",
    "\n",
    "        # Sample uniform random numbers\n",
    "        random_nums = torch.rand_like(acceptance_ratios)\n",
    "\n",
    "        # Find number of accepted tokens\n",
    "        # Accept if random number < min(1, target_prob / draft_prob)\n",
    "        accepts = random_nums < torch.minimum(\n",
    "            torch.ones_like(acceptance_ratios),\n",
    "            acceptance_ratios\n",
    "        )\n",
    "\n",
    "        # Find first rejection\n",
    "        try:\n",
    "            n_accepted = torch.where(~accepts)[0][0].item()\n",
    "        except:\n",
    "            n_accepted = len(accepts)\n",
    "\n",
    "        return n_accepted\n",
    "\n",
    "        # accepted_tokens = []\n",
    "        # for i in range(len(draft_tokens)):\n",
    "        #     if target_probs[i] / draft_probs[i] > torch.rand(1).item():\n",
    "        #         accepted_tokens.append(draft_tokens[i])\n",
    "        #     else:\n",
    "        #         break # Stop if token is not accepted\n",
    "\n",
    "        # return len(accepted_tokens)\n",
    "    \n",
    "    # TODO: verify this, might need to do some window size thing\n",
    "    def update_draft_model(self):\n",
    "        \"\"\"Update draft model with the replay buffer.\"\"\"\n",
    "        if len(self.replay_buffer) == 0:\n",
    "            return\n",
    "\n",
    "        # Get draft tokens, draft and target probabilities from the replay buffer\n",
    "        # draft_tokens = torch.tensor([x[0] for x in self.replay_buffer], device=self.device)\n",
    "        # print(self.replay_buffer[0][0])\n",
    "        # draft_probs = self.replay_buffer[:, 0]\n",
    "        # target_probs = self.replay_buffer[:, 1]\n",
    "        draft_probs = torch.stack([x[0][0] for x in self.replay_buffer], dim=0)\n",
    "        target_probs = torch.stack([x[1][0] for x in self.replay_buffer], dim=0)\n",
    "\n",
    "        self.draft_model.train()\n",
    "\n",
    "        # for param in self.draft_model.parameters():\n",
    "        #     print(param.requires_grad)\n",
    "\n",
    "\n",
    "        # criterion = torch.nn.CrossEntropyLoss()\n",
    "        # print(draft_probs.shape, target_probs.shape)\n",
    "        loss = self.soft_cross_entropy(draft_probs, target_probs)\n",
    "        print(\"Loss grad_fn:\", loss.grad_fn)\n",
    "        print(\"Draft probs grad_fn:\", draft_probs.grad_fn)\n",
    "        print(\"Target probs grad_fn:\", target_probs.grad_fn)\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        self.draft_model.eval()\n",
    "\n",
    "        # Clear the replay buffer\n",
    "        self.replay_buffer = []\n",
    "\n",
    "    def soft_cross_entropy(self, predicts, targets, padding_mask=None):\n",
    "        predict_log_prob = torch.nn.functional.log_softmax(predicts, dim=-1)\n",
    "        targets_prob = torch.nn.functional.softmax(targets, dim=-1)\n",
    "        entropy = -targets_prob * predict_log_prob\n",
    "        # expand_mask = padding_mask.unsqueeze(-1).expand_as(entropy)\n",
    "        # entropy.masked_fill_(expand_mask, 0)\n",
    "        # mean_entropy = entropy.sum() / (~padding_mask).sum()\n",
    "        return entropy\n",
    "\n",
    "    def translate(\n",
    "        self,\n",
    "        source_text: str,\n",
    "        max_length: int = 128\n",
    "    ) -> str:\n",
    "        \"\"\"Translate source text using speculative decoding.\"\"\"\n",
    "        # Encode source text\n",
    "        encoder_inputs = self.tokenizer(\n",
    "            f\"translate English to German: {source_text}\",\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True\n",
    "        ).to(self.device)\n",
    "\n",
    "        # Initialize with start token\n",
    "        decoder_input_ids = torch.tensor([[self.tokenizer.pad_token_id]], device=self.device)\n",
    "\n",
    "        self.iteration_count = 0\n",
    "        self.replay_buffer = []\n",
    "\n",
    "        while decoder_input_ids.shape[1] < max_length:\n",
    "            self.temp_buffer = []\n",
    "\n",
    "            while decoder_input_ids.shape[1] < max_length:\n",
    "                # Get draft tokens autoregressively\n",
    "                # print(\"Encoder Inputs\", encoder_inputs.input_ids.shape)\n",
    "                draft_tokens, draft_probs, proposed_decoder_ids, draft_logits = self.get_draft_logits(\n",
    "                    encoder_inputs.input_ids,\n",
    "                    encoder_inputs.attention_mask,\n",
    "                    decoder_input_ids,\n",
    "                    self.gamma\n",
    "                )\n",
    "\n",
    "                draft_tokens = torch.tensor(draft_tokens, device=self.device)\n",
    "                draft_probs = torch.tensor(draft_probs, device=self.device)\n",
    "\n",
    "                if len(draft_tokens) == 0:\n",
    "                    break\n",
    "\n",
    "                # Get target probabilities in parallel\n",
    "                target_probs, target_logits = self.get_target_probs(\n",
    "                    encoder_inputs.input_ids,\n",
    "                    encoder_inputs.attention_mask,\n",
    "                    decoder_input_ids,\n",
    "                    draft_tokens\n",
    "                )\n",
    "                # print(draft_logits.shape, target_logits.shape)\n",
    "\n",
    "                # Verify tokens\n",
    "                n_accepted = self.verify_tokens(target_probs, draft_tokens, draft_probs)\n",
    "\n",
    "                # Accept verified tokens\n",
    "                if n_accepted > 0:\n",
    "                    decoder_input_ids = torch.cat([\n",
    "                        decoder_input_ids,\n",
    "                        draft_tokens[:n_accepted].unsqueeze(0)\n",
    "                    ], dim=1)\n",
    "\n",
    "                # # If rejection or no acceptance, sample one token from target\n",
    "                # if n_accepted < len(draft_tokens):\n",
    "                #     with torch.no_grad():\n",
    "                #         outputs = self.target_model(\n",
    "                #             input_ids=encoder_inputs.input_ids,\n",
    "                #             attention_mask=encoder_inputs.attention_mask,\n",
    "                #             decoder_input_ids=decoder_input_ids,\n",
    "                #             return_dict=True\n",
    "                #         )\n",
    "                #         logits = outputs.logits[:, -1, :]\n",
    "                #         probs = F.softmax(logits, dim=-1)\n",
    "                #         token_id = torch.multinomial(probs, num_samples=1)\n",
    "                #         decoder_input_ids = torch.cat([decoder_input_ids, token_id], dim=1)\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    # print(target_logits.shape, draft_logits.shape)\n",
    "                    probs = target_logits[:, -1, :] #- draft_logits[:, -1, :]\n",
    "                    probs = F.softmax(probs, dim=-1)\n",
    "                    token_id = torch.multinomial(probs, num_samples=1)\n",
    "                    # print(probs.shape, token_id.shape)\n",
    "                    decoder_input_ids = torch.cat([decoder_input_ids, token_id], dim=1)\n",
    "\n",
    "                # Check for end of sequence\n",
    "                if decoder_input_ids[0][-1].item() == self.tokenizer.eos_token_id:\n",
    "                    break\n",
    "                \n",
    "                # TODO: Update buffer with draft and target logits of the first rejected token, verify implementation\n",
    "                # rejected_tokens = draft_tokens[n_accepted]\n",
    "                if n_accepted < len(draft_tokens):\n",
    "                    # rejected_prob_draft = draft_logits[:, n_accepted, :]\n",
    "                    # rejected_prob_target = target_logits[:, n_accepted, :]\n",
    "\n",
    "                    self.temp_buffer.append((draft_logits[:, -1, :], target_logits[:, -1, :]))\n",
    "\n",
    "                # Check for end of sequence\n",
    "                if decoder_input_ids[0][-1].item() == self.tokenizer.eos_token_id:\n",
    "                    break\n",
    "\n",
    "            self.replay_buffer.extend(self.temp_buffer)\n",
    "            self.iteration_count += 1\n",
    "\n",
    "            if self.iteration_count % self.update_interval == 0:\n",
    "                self.update_draft_model()\n",
    "                self.iteration_count = 0\n",
    "\n",
    "        # Decode translation\n",
    "        translation = self.tokenizer.decode(\n",
    "            decoder_input_ids[0],\n",
    "            skip_special_tokens=True,\n",
    "            clean_up_tokenization_spaces=True\n",
    "        )\n",
    "        return translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize decoder\n",
    "online_decoder = OnlineSpeculativeDecoder()\n",
    "\n",
    "# Example translation\n",
    "source_text = \"In a world where technology evolves at an unprecedented pace, individuals and organizations must adapt quickly to the rapid advancements in artificial intelligence, machine learning, and automation, ensuring that ethical considerations, environmental sustainability, and equitable access to resources are prioritized to create a future that benefits all of humanity.\"\n",
    "\n",
    "# Time the translation\n",
    "start_time = time.time()\n",
    "translation = online_decoder.translate(source_text)\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"Source: {source_text}\")\n",
    "print(f\"Translation: {translation}\\n\")\n",
    "print(f\"Time taken: {end_time - start_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Online V2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OnlineSpeculativeDecoder:\n",
    "    def __init__(\n",
    "        self,\n",
    "        target_model_name = \"google-t5/t5-large\",\n",
    "        draft_model_name = \"google-t5/t5-small\",\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "        gamma = 4,\n",
    "        update_interval=2,  # Update draft model after every `update_interval` iterations\n",
    "        buffer_size_threshold=2,  # Buffer size threshold for updates\n",
    "        time_threshold=2,  # Time threshold (in seconds) for updates\n",
    "    ):\n",
    "        self.device = device\n",
    "        self.gamma = gamma\n",
    "        self.update_interval = update_interval\n",
    "        self.buffer_size_threshold = buffer_size_threshold\n",
    "        self.time_threshold = time_threshold\n",
    "        self.last_update_time = time.time()  # Track last update time\n",
    "\n",
    "        self.tokenizer = T5Tokenizer.from_pretrained(target_model_name)\n",
    "        self.target_model = T5ForConditionalGeneration.from_pretrained(target_model_name).to(device)\n",
    "        self.draft_model = T5ForConditionalGeneration.from_pretrained(draft_model_name).to(device)\n",
    "\n",
    "        self.target_model.eval()\n",
    "        self.draft_model.eval()\n",
    "\n",
    "        # Buffers for storing token proposals and updates\n",
    "        self.replay_buffer = []\n",
    "        self.temp_buffer = []  # Temporary buffer for a single request\n",
    "\n",
    "        # Counter for iteration tracking\n",
    "        self.iteration_count = 0\n",
    "\n",
    "    def get_draft_logits(\n",
    "        self,\n",
    "        input_ids: torch.Tensor,\n",
    "        attention_mask: torch.Tensor,\n",
    "        decoder_input_ids: torch.Tensor,\n",
    "        gamma: int\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"Get draft logits for gamma tokens\"\"\"\n",
    "        draft_tokens = []\n",
    "        draft_probs = []\n",
    "        current_decoder_ids = decoder_input_ids\n",
    "\n",
    "        # Generate gamma tokens from the draft model\n",
    "        for _ in range(gamma):\n",
    "            with torch.no_grad():\n",
    "                outputs = self.draft_model(\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    decoder_input_ids=current_decoder_ids,\n",
    "                    return_dict=True\n",
    "                )\n",
    "                logits = outputs.logits[:, -1, :]  # Get logits for last position\n",
    "                probs = F.softmax(logits, dim=-1)\n",
    "\n",
    "                # Sample token\n",
    "                token_id = torch.multinomial(probs, num_samples=1).squeeze(-1)\n",
    "                prob = probs.gather(-1, token_id.unsqueeze(-1)).squeeze(-1)\n",
    "\n",
    "                draft_tokens.append(token_id.item())\n",
    "                draft_probs.append(prob.item())\n",
    "\n",
    "                # Update decoder inputs for next iteration\n",
    "                current_decoder_ids = torch.cat(\n",
    "                    [current_decoder_ids, token_id.view(1, 1)],\n",
    "                    dim=1\n",
    "                )\n",
    "\n",
    "                if token_id.item() == self.tokenizer.eos_token_id:\n",
    "                    break\n",
    "\n",
    "        return draft_tokens, draft_probs, current_decoder_ids, outputs.logits\n",
    "\n",
    "    def get_target_probs(\n",
    "        self,\n",
    "        input_ids: torch.Tensor,\n",
    "        attention_mask: torch.Tensor,\n",
    "        decoder_input_ids: torch.Tensor,\n",
    "        draft_tokens: torch.Tensor\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"Get target probabilities for the draft tokens in parallel.\"\"\"\n",
    "        with torch.no_grad():\n",
    "            # Add draft tokens to decoder input\n",
    "            full_decoder_ids = torch.cat([decoder_input_ids, draft_tokens.unsqueeze(0)], dim=1)\n",
    "\n",
    "            outputs = self.target_model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                decoder_input_ids=full_decoder_ids,\n",
    "                return_dict=True\n",
    "            )\n",
    "\n",
    "            # Get probabilities for positions before each draft token\n",
    "            logits = outputs.logits[:, -(len(draft_tokens) + 1):-1, :]\n",
    "            target_probs = F.softmax(logits, dim=-1)\n",
    "\n",
    "            return target_probs.squeeze(0), outputs.logits\n",
    "        \n",
    "    def get_logits(self, model, input_ids, attention_mask, decoder_input_ids):\n",
    "        return model(\n",
    "            input_ids=input_ids,\n",
    "            decoder_input_ids=decoder_input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "        ).logits\n",
    "\n",
    "    def verify_tokens(\n",
    "        self,\n",
    "        target_probs: torch.Tensor,\n",
    "        draft_tokens: torch.Tensor,\n",
    "        draft_probs: torch.Tensor,\n",
    "    ) -> int:\n",
    "        \"\"\"Determine number of accepted tokens\"\"\"\n",
    "        # Get target probabilities for the draft tokens\n",
    "        target_probs_draft_tokens = target_probs.gather(\n",
    "            -1,\n",
    "            draft_tokens.unsqueeze(-1)\n",
    "        ).squeeze(-1)\n",
    "\n",
    "        # Calculate acceptance ratios\n",
    "        acceptance_ratios = target_probs_draft_tokens / draft_probs\n",
    "\n",
    "        # Sample uniform random numbers\n",
    "        random_nums = torch.rand_like(acceptance_ratios)\n",
    "\n",
    "        # Find number of accepted tokens\n",
    "        # Accept if random number < min(1, target_prob / draft_prob)\n",
    "        accepts = random_nums < torch.minimum(\n",
    "            torch.ones_like(acceptance_ratios),\n",
    "            acceptance_ratios\n",
    "        )\n",
    "\n",
    "        # Find first rejection\n",
    "        try:\n",
    "            n_accepted = torch.where(~accepts)[0][0].item()\n",
    "        except:\n",
    "            n_accepted = len(accepts)\n",
    "\n",
    "        return n_accepted\n",
    "\n",
    "        # accepted_tokens = []\n",
    "        # for i in range(len(draft_tokens)):\n",
    "        #     if target_probs[i] / draft_probs[i] > torch.rand(1).item():\n",
    "        #         accepted_tokens.append(draft_tokens[i])\n",
    "        #     else:\n",
    "        #         break # Stop if token is not accepted\n",
    "\n",
    "        # return len(accepted_tokens)\n",
    "    \n",
    "    # TODO: verify this, might need to do some window size thing\n",
    "    # def update_draft_model(self):\n",
    "    #     \"\"\"Update draft model with the replay buffer.\"\"\"\n",
    "    #     if len(self.replay_buffer) == 0:\n",
    "    #         return\n",
    "\n",
    "    #     # Get draft tokens, draft and target probabilities from the replay buffer\n",
    "    #     draft_tokens = torch.tensor([x[0] for x in self.replay_buffer], device=self.device)\n",
    "    #     target_logits = torch.tensor([x[1] for x in self.replay_buffer], device=self.device)\n",
    "\n",
    "    #     encoder_inputs = s\n",
    "    #     output = self.draft_model(\n",
    "    #         input_ids=encoder_inputs.input_ids,\n",
    "    #         attention_mask=encoder_inputs.attention_mask,\n",
    "    #         decoder_input_ids=decoder_input_ids,\n",
    "    #         return_dict=True\n",
    "    #     )\n",
    "\n",
    "    def pad_to_2d(self, tensor_list, pad_token_id, max_len=None):\n",
    "        if not isinstance(tensor_list[0], torch.Tensor):\n",
    "            tensor_list = [torch.tensor(t).reshape(1, -1) for t in tensor_list]\n",
    "        if max_len is None:\n",
    "            max_len = max([t.shape[-1] for t in tensor_list])\n",
    "        assert max_len > 0\n",
    "\n",
    "        # Pad each tensor to the max length and stack them to form a 2D tensor\n",
    "        result = torch.cat(\n",
    "            [\n",
    "                torch.nn.functional.pad(\n",
    "                    tensor, (0, max_len - tensor.shape[-1]),\n",
    "                    value=pad_token_id\n",
    "                )\n",
    "                for tensor in tensor_list\n",
    "            ],\n",
    "            dim=0\n",
    "        )\n",
    "        return result\n",
    "        \n",
    "\n",
    "    def soft_cross_entropy(self, predicts, targets, padding_mask=None):\n",
    "        predict_log_prob = torch.nn.functional.log_softmax(predicts, dim=-1)\n",
    "        targets_prob = torch.nn.functional.softmax(targets, dim=-1)\n",
    "        entropy = -targets_prob * predict_log_prob\n",
    "        expand_mask = padding_mask.unsqueeze(-1).expand_as(entropy)\n",
    "        entropy.masked_fill_(expand_mask, 0)\n",
    "        mean_entropy = entropy.sum() / (~padding_mask).sum()\n",
    "        return mean_entropy\n",
    "\n",
    "    def translate_dataset(\n",
    "        self,\n",
    "        sentences: List[str],\n",
    "        max_length: int = 128\n",
    "    ) -> List[str]:\n",
    "        \"\"\"Translate dataset using online speculative decoding.\"\"\"\n",
    "\n",
    "        self.iteration_count = 0\n",
    "        self.replay_buffer = []\n",
    "\n",
    "        translated_data = []\n",
    "\n",
    "        for source_text in sentences:\n",
    "            # Encode source text\n",
    "            encoder_inputs = self.tokenizer(\n",
    "                f\"translate English to German: {source_text}\",\n",
    "                return_tensors=\"pt\",\n",
    "                padding=True\n",
    "            ).to(self.device)\n",
    "\n",
    "            # Initialize with start token\n",
    "            decoder_input_ids = torch.tensor([[self.tokenizer.pad_token_id]], device=self.device)\n",
    "            self.temp_buffer = []\n",
    "\n",
    "            while decoder_input_ids.shape[1] < max_length:\n",
    "\n",
    "                # Check for end of sequence\n",
    "                if decoder_input_ids[0][-1].item() == self.tokenizer.eos_token_id:\n",
    "                    break\n",
    "\n",
    "                # Get draft tokens autoregressively\n",
    "                # print(\"Encoder Inputs\", encoder_inputs.input_ids.shape)\n",
    "                draft_tokens, draft_probs, proposed_decoder_ids, draft_logits = self.get_draft_logits(\n",
    "                    encoder_inputs.input_ids,\n",
    "                    encoder_inputs.attention_mask,\n",
    "                    decoder_input_ids,\n",
    "                    self.gamma\n",
    "                )\n",
    "\n",
    "                draft_tokens = torch.tensor(draft_tokens, device=self.device)\n",
    "                draft_probs = torch.tensor(draft_probs, device=self.device)\n",
    "\n",
    "                if len(draft_tokens) == 0:\n",
    "                    break\n",
    "\n",
    "                # Get target probabilities in parallel\n",
    "                target_probs, target_logits = self.get_target_probs(\n",
    "                    encoder_inputs.input_ids,\n",
    "                    encoder_inputs.attention_mask,\n",
    "                    decoder_input_ids,\n",
    "                    draft_tokens\n",
    "                )\n",
    "                # print(draft_logits.shape, target_logits.shape)\n",
    "\n",
    "                # Verify tokens\n",
    "                n_accepted = self.verify_tokens(target_probs, draft_tokens, draft_probs)\n",
    "\n",
    "                # Accept verified tokens\n",
    "                if n_accepted > 0:\n",
    "                    decoder_input_ids = torch.cat([\n",
    "                        decoder_input_ids,\n",
    "                        draft_tokens[:n_accepted].unsqueeze(0)\n",
    "                    ], dim=1)\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    # print(target_logits.shape, draft_logits.shape)\n",
    "                    probs = target_logits[:, -1, :] #- draft_logits[:, -1, :]\n",
    "                    probs = F.softmax(probs, dim=-1)\n",
    "                    token_id = torch.multinomial(probs, num_samples=1)\n",
    "                    # print(probs.shape, token_id.shape)\n",
    "                    decoder_input_ids = torch.cat([decoder_input_ids, token_id], dim=1)\n",
    "                \n",
    "                # rejected_tokens = draft_tokens[n_accepted]\n",
    "                if n_accepted < len(draft_tokens):\n",
    "\n",
    "                    self.temp_buffer.append((encoder_inputs, decoder_input_ids, target_logits, n_accepted))\n",
    "\n",
    "            self.replay_buffer.extend(self.temp_buffer)\n",
    "            self.iteration_count += 1\n",
    "\n",
    "            if self.iteration_count % self.update_interval == 0:\n",
    "                # self.update_draft_model()\n",
    "                self.draft_model.train()\n",
    "                \n",
    "                # finetune over collected tokens and logits\n",
    "                encoder_input_ids = self.pad_to_2d([x[0].input_ids for x in self.replay_buffer], 0)\n",
    "                encoder_attention_mask = torch.stack([x[0].attention_mask[0] for x in self.replay_buffer], dim=0)\n",
    "                decoder_input_ids = self.pad_to_2d([x[1] for x in self.replay_buffer], 0, 512)\n",
    "\n",
    "                print(encoder_input_ids.shape, encoder_attention_mask.shape, decoder_input_ids.shape)\n",
    "\n",
    "                target_logits = [x[2] for x in self.replay_buffer]\n",
    "                for i in range(len(target_logits)):\n",
    "                    temp = torch.zeros(1, 32128, device=self.device).repeat(512 - target_logits[i].shape[1], 1).unsqueeze(0)\n",
    "                    target_logits[i] = torch.cat([target_logits[i], temp], dim=1)\n",
    "\n",
    "                n_accepted_tokens = [x[3] for x in self.replay_buffer]\n",
    "\n",
    "                # CUDA out of memory error\n",
    "                draft_logits = self.get_logits(self.draft_model, encoder_input_ids, encoder_attention_mask, decoder_input_ids).float()\n",
    "\n",
    "                # need to get loss only using the wrong tokens\n",
    "                # TODO: check if we need to ignore the pad tokens in the mask\n",
    "                mask = torch.ones_like(decoder_input_ids, dtype=torch.bool)\n",
    "                for i in range(len(n_accepted_tokens)):\n",
    "                    mask[i, n_accepted_tokens[i]:] = False\n",
    "                loss = self.soft_cross_entropy(draft_logits, target_logits, mask)\n",
    "                loss.backward()\n",
    "\n",
    "                self.draft_model.eval()\n",
    "                self.replay_buffer = []\n",
    "                self.iteration_count = 0\n",
    "\n",
    "            # Decode translation\n",
    "            translation = self.tokenizer.decode(\n",
    "                decoder_input_ids[0],\n",
    "                skip_special_tokens=True,\n",
    "                clean_up_tokenization_spaces=True\n",
    "            )\n",
    "            translated_data.append(translation)\n",
    "        return translated_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize decoder\n",
    "online_decoder = OnlineSpeculativeDecoder()\n",
    "\n",
    "# Example translation\n",
    "source_text = \"In a world where technology evolves at an unprecedented pace, individuals and organizations must adapt quickly to the rapid advancements in artificial intelligence, machine learning, and automation, ensuring that ethical considerations, environmental sustainability, and equitable access to resources are prioritized to create a future that benefits all of humanity.\"\n",
    "sents = [source_text] * 10\n",
    "# Time the translation\n",
    "start_time = time.time()\n",
    "translation = online_decoder.translate_dataset(sents)\n",
    "end_time = time.time()\n",
    "\n",
    "for i, sent in enumerate(sents):\n",
    "    print(f\"Source: {sent}\")\n",
    "    print(f\"Translation: {translation[i]}\\n\")\n",
    "print(f\"Time taken: {end_time - start_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
